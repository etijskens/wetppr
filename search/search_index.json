{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Material for the course 2000wetppr -- Parallel programming of the University of Antwerp, Faculty of Science, Academic year 2023-24, Table of Contents Overview Glossary Links Course text Exercises Evaluation VSC infrastructure About the author","title":"Home"},{"location":"#table-of-contents","text":"Overview Glossary Links Course text Exercises Evaluation VSC infrastructure About the author","title":"Table of Contents"},{"location":"about-the-author/","text":"[Engel]bert Tijskens Education Lic. Aard- en delfstofkunde, Master in Physics of Microelectronics and Material Sciences , Doctor in de Natuurwetenschappen - ik heb dus eigenlijk geen opleiding genoten in programmeren of wetenschappelijk rekenen ..., maar veel opleidingen bijgewoond en vooral veel gelezen tijdens mijn voortdurende zoektocht naar betere manieren om de opdrachten waar ik voor stond uit te voeren. Work ik werk sinds 2012 voor CalcUA , de UA kernfaciliteit voor supercomputing, en voor het VSC , het Vlaams Supercomputer Centrum . Ik verzorg er opleiding en ondersteuning van onderzoekers rond wetenschappelijk programmeren voor HPC-omgevingen en performantie-analyse. k ben gepassioneerd door Python , C++ , Fortran en libraries en frameworks waarmee hoog-performante en parallelle applicaties kunnen gebouwd worden. Teaching Sinds 2014 geef ik het vak \"Parallel programmeren\". Ik geef graag les en wil mijn ervaring van 30 jaar wetenschappelijk programmeren delen met jonge onderzoekers. Discrete Element Modelling Voor 2012 leidde ik de DEM Research Group aan de KU Leuven. DEM staat voor Discrete Element Modelling . Je kan het vergelijken met Molecular Dynamics , maar dan in de macroscopische wereld met atomen die een vorm hebben, korrels dus, of grains in het Engels. Daarom wordt het ook Granular Dynamics genoemd. Korrelstromen komen in heel wat industri\u00eble processen voor en het modelleren ervan is interessant, om inzicht te verwerven in korrelige processen en om er goede procesinstallaties voor te ontwerpen. Dit is erg uitdagend omdat de fysica van korrelige processen zo complex is. In tegenstelling tot MD zijn interacties tussen korrels dissipatief en worden de contactkrachten bepaald door materiaaleigenschappen en oppervlakte-eigenschappen en zijn er zowel normale als tangenti\u00eble contactkrachten (wrijving). Bovendien zijn de korrels - afhankelijk van het materiaal - soms vervormbaar, of zelfs breekbaar. Omdat korrelige processen vaak over heel veel deeltjes gaan, zijn performantie en parallellisatie essentieel. De simulatiesoftware waar we toen aan werkten, wordt nu gecommercialiseerd door Mpacts . CNH maaidorser - ontwikkeld met Mpacts Hier zijn verscheidene korrelige processen aan de orde: de strohalmen maaien en binnen trekken in de machine, de graantjes losmaken van de aar en ze scheiden van het stro, stro afvoeren naar achter, het kaf van het koren scheiden (kaf weg blazen), het graan transporteren naar de verzamelbak bovenaan de machine, het graan transporteren van de verzamelbak naar de aanhangwagen achter de tractor, Spherische korrels die op een trampoline vallen Mpacts: ijsbreker Mpacts simulatie van een ijsbreker die door een ijslaag breekt. Kijk op Mpacts case studies voor meer voorbeelden.","title":"About the author"},{"location":"about-the-author/#engelbert-tijskens","text":"","title":"[Engel]bert Tijskens"},{"location":"about-the-author/#education","text":"Lic. Aard- en delfstofkunde, Master in Physics of Microelectronics and Material Sciences , Doctor in de Natuurwetenschappen - ik heb dus eigenlijk geen opleiding genoten in programmeren of wetenschappelijk rekenen ..., maar veel opleidingen bijgewoond en vooral veel gelezen tijdens mijn voortdurende zoektocht naar betere manieren om de opdrachten waar ik voor stond uit te voeren.","title":"Education"},{"location":"about-the-author/#work","text":"ik werk sinds 2012 voor CalcUA , de UA kernfaciliteit voor supercomputing, en voor het VSC , het Vlaams Supercomputer Centrum . Ik verzorg er opleiding en ondersteuning van onderzoekers rond wetenschappelijk programmeren voor HPC-omgevingen en performantie-analyse. k ben gepassioneerd door Python , C++ , Fortran en libraries en frameworks waarmee hoog-performante en parallelle applicaties kunnen gebouwd worden.","title":"Work"},{"location":"about-the-author/#teaching","text":"Sinds 2014 geef ik het vak \"Parallel programmeren\". Ik geef graag les en wil mijn ervaring van 30 jaar wetenschappelijk programmeren delen met jonge onderzoekers.","title":"Teaching"},{"location":"about-the-author/#discrete-element-modelling","text":"Voor 2012 leidde ik de DEM Research Group aan de KU Leuven. DEM staat voor Discrete Element Modelling . Je kan het vergelijken met Molecular Dynamics , maar dan in de macroscopische wereld met atomen die een vorm hebben, korrels dus, of grains in het Engels. Daarom wordt het ook Granular Dynamics genoemd. Korrelstromen komen in heel wat industri\u00eble processen voor en het modelleren ervan is interessant, om inzicht te verwerven in korrelige processen en om er goede procesinstallaties voor te ontwerpen. Dit is erg uitdagend omdat de fysica van korrelige processen zo complex is. In tegenstelling tot MD zijn interacties tussen korrels dissipatief en worden de contactkrachten bepaald door materiaaleigenschappen en oppervlakte-eigenschappen en zijn er zowel normale als tangenti\u00eble contactkrachten (wrijving). Bovendien zijn de korrels - afhankelijk van het materiaal - soms vervormbaar, of zelfs breekbaar. Omdat korrelige processen vaak over heel veel deeltjes gaan, zijn performantie en parallellisatie essentieel. De simulatiesoftware waar we toen aan werkten, wordt nu gecommercialiseerd door Mpacts .","title":"Discrete Element Modelling"},{"location":"about-the-author/#cnh-maaidorser-ontwikkeld-met-mpacts","text":"Hier zijn verscheidene korrelige processen aan de orde: de strohalmen maaien en binnen trekken in de machine, de graantjes losmaken van de aar en ze scheiden van het stro, stro afvoeren naar achter, het kaf van het koren scheiden (kaf weg blazen), het graan transporteren naar de verzamelbak bovenaan de machine, het graan transporteren van de verzamelbak naar de aanhangwagen achter de tractor,","title":"CNH maaidorser - ontwikkeld met Mpacts"},{"location":"about-the-author/#spherische-korrels-die-op-een-trampoline-vallen","text":"","title":"Spherische korrels die op een trampoline vallen"},{"location":"about-the-author/#mpacts-ijsbreker","text":"Mpacts simulatie van een ijsbreker die door een ijslaag breekt. Kijk op Mpacts case studies voor meer voorbeelden.","title":"Mpacts: ijsbreker"},{"location":"assignment-2023-24/","text":"Assignment 2022-23 ?","title":"Assignment 2022-23"},{"location":"assignment-2023-24/#assignment-2022-23","text":"","title":"Assignment 2022-23"},{"location":"assignment-2023-24/#_1","text":"","title":"?"},{"location":"chapter-1/","text":"Chapter 1 - Introduction Material: this website, which is built with mkdocs from this GitHub repo . The repo contains also some example code and code for some of the case studies of chapter 4 in directories wetppr and tests/wetppr . some presentations found here . (Most of these need some reworking, especially those not in VSC format). Overview What is a parallel program? Possible reasons to parallelize a program. What is a parallel program? A parallel program is a program that distributes its work over different processing units such that parts of its work load can be computed simultaneously. At the end the program gathers the partial results from the processing units and combines them in a global result. If the tasks are independent of each other, the program is called embarrassingly parallel . In general, the individual tasks are not independent and need to exchange information. This is called communication . The opposite of a parallel program is a serial or sequential program, executing all its instructions one after the other. Possible reasons to parallelize a program 1. Reduce the time to solution The term time to solution in general means the time your machine needs to solve a computational problem. If the problem can be divided in smaller tasks that can be computed simultaneously, the time to solution decreases. If a company can solve a research or engineering question in a week or a day, that is an important difference. A processing unit has a maximum number of instructions it can execute per second, this is called its peak performance . Obviously, the peak performance is a machine limit puts a hard limit to what a processing unit can achieve in a given amount of time. But instructions operate on data, and moving data from main memory takes time as well. A program that must process lots of data but does little computation is limited by the speed at which the processing unit can fetch data from the main memory. This is called the memory bandwidth (usually in Mbits/s). Programs that do a lot of computation and does not move a lot of data in or out of main memory is called compute limited . A program that moves a lot of data and little computation is bandwidth limited . While in the past programs used to be compute bound, today, most programs are memory bound, because the speed of the processing units increased much faster than the speed of memory. As a consequence, efficient memory access patterns are crucial to the performance of a program. 2. Solve bigger problems in the same time There is a third machine limit that plays a role, namely the amount of main memory. This puts a limit on the size of the problem that can be treated, e.g. the number of volume elements in a CFD simulation or the number of atoms in a MD simulation. If the program can distribute the work over, say 10 machines, it has 10 times the amount of memory at its disposition and thus can solve a 10 times bigger problem. 3. Produce more accurate solutions More accuracy can come from more complex physical models, or from using more basis functions to expand the solution. This leads to more computation and perhaps a prohibitively long time to solution. Problems involving discretisation (the process of dividing the domain of a computational problem in small elements, as in computational fluid dynamics and finite element modelling) the accuracy typically improves when the elements get smaller, as in approximating the integral under a curve by rectangles. In both cases parallelization of the program may be necessary to obtain a solution. 4 Competition If a program that is in competition with other programs that solve the same problem, parallelization will allow it to reduce the time to solution, to compute bigger problems and achieve more accurate solution. This is, obviously, a competitive advantage. Can't I just by a faster and bigger computer? Nope, that fairy tale ended approximately at the beginning of this century with the advent of the multi-processor computer, also called multi-core computer. Increasing the peak performance by increasing the clock frequency was no longer possible, because the power consumption of a processor increases as the third power of the clock frequency. At a certain point it became impossible or too expensive to cool the processor. The only way to get a processor execute more instructions per second was to put more processing units on it (cores). At that point serial program became even slower on the new multi-processors because the clock frequency was reduced to remain inside the power envelope. Moore's law predicts that the number of transistors in a processor doubles every 18 months due to increasing miniaturization. With this the combined peak performance of the multi-processors increases as well, but the peak performance of the individual processing units no longer does. This makes it necessary to parallelize programs in order to keep up with Moore's law. It must be said that the increase of peak performance was not always in line with Moore's law. At some point the peak performance of processing units was increased by adding parallelization concept in single processing units like pipelining and SIMD vectorisation. We'll come to that later.","title":"Chapter 1 - Introduction"},{"location":"chapter-1/#chapter-1-introduction","text":"Material: this website, which is built with mkdocs from this GitHub repo . The repo contains also some example code and code for some of the case studies of chapter 4 in directories wetppr and tests/wetppr . some presentations found here . (Most of these need some reworking, especially those not in VSC format).","title":"Chapter 1 - Introduction"},{"location":"chapter-1/#overview","text":"What is a parallel program? Possible reasons to parallelize a program.","title":"Overview"},{"location":"chapter-1/#what-is-a-parallel-program","text":"A parallel program is a program that distributes its work over different processing units such that parts of its work load can be computed simultaneously. At the end the program gathers the partial results from the processing units and combines them in a global result. If the tasks are independent of each other, the program is called embarrassingly parallel . In general, the individual tasks are not independent and need to exchange information. This is called communication . The opposite of a parallel program is a serial or sequential program, executing all its instructions one after the other.","title":"What is a parallel program?"},{"location":"chapter-1/#possible-reasons-to-parallelize-a-program","text":"","title":"Possible reasons to parallelize a program"},{"location":"chapter-1/#1-reduce-the-time-to-solution","text":"The term time to solution in general means the time your machine needs to solve a computational problem. If the problem can be divided in smaller tasks that can be computed simultaneously, the time to solution decreases. If a company can solve a research or engineering question in a week or a day, that is an important difference. A processing unit has a maximum number of instructions it can execute per second, this is called its peak performance . Obviously, the peak performance is a machine limit puts a hard limit to what a processing unit can achieve in a given amount of time. But instructions operate on data, and moving data from main memory takes time as well. A program that must process lots of data but does little computation is limited by the speed at which the processing unit can fetch data from the main memory. This is called the memory bandwidth (usually in Mbits/s). Programs that do a lot of computation and does not move a lot of data in or out of main memory is called compute limited . A program that moves a lot of data and little computation is bandwidth limited . While in the past programs used to be compute bound, today, most programs are memory bound, because the speed of the processing units increased much faster than the speed of memory. As a consequence, efficient memory access patterns are crucial to the performance of a program.","title":"1. Reduce the time to solution"},{"location":"chapter-1/#2-solve-bigger-problems-in-the-same-time","text":"There is a third machine limit that plays a role, namely the amount of main memory. This puts a limit on the size of the problem that can be treated, e.g. the number of volume elements in a CFD simulation or the number of atoms in a MD simulation. If the program can distribute the work over, say 10 machines, it has 10 times the amount of memory at its disposition and thus can solve a 10 times bigger problem.","title":"2. Solve bigger problems in the same time"},{"location":"chapter-1/#3-produce-more-accurate-solutions","text":"More accuracy can come from more complex physical models, or from using more basis functions to expand the solution. This leads to more computation and perhaps a prohibitively long time to solution. Problems involving discretisation (the process of dividing the domain of a computational problem in small elements, as in computational fluid dynamics and finite element modelling) the accuracy typically improves when the elements get smaller, as in approximating the integral under a curve by rectangles. In both cases parallelization of the program may be necessary to obtain a solution.","title":"3. Produce more accurate solutions"},{"location":"chapter-1/#4-competition","text":"If a program that is in competition with other programs that solve the same problem, parallelization will allow it to reduce the time to solution, to compute bigger problems and achieve more accurate solution. This is, obviously, a competitive advantage.","title":"4 Competition"},{"location":"chapter-1/#cant-i-just-by-a-faster-and-bigger-computer","text":"Nope, that fairy tale ended approximately at the beginning of this century with the advent of the multi-processor computer, also called multi-core computer. Increasing the peak performance by increasing the clock frequency was no longer possible, because the power consumption of a processor increases as the third power of the clock frequency. At a certain point it became impossible or too expensive to cool the processor. The only way to get a processor execute more instructions per second was to put more processing units on it (cores). At that point serial program became even slower on the new multi-processors because the clock frequency was reduced to remain inside the power envelope. Moore's law predicts that the number of transistors in a processor doubles every 18 months due to increasing miniaturization. With this the combined peak performance of the multi-processors increases as well, but the peak performance of the individual processing units no longer does. This makes it necessary to parallelize programs in order to keep up with Moore's law. It must be said that the increase of peak performance was not always in line with Moore's law. At some point the peak performance of processing units was increased by adding parallelization concept in single processing units like pipelining and SIMD vectorisation. We'll come to that later.","title":"Can't I just by a faster and bigger computer?"},{"location":"chapter-2/","text":"Chapter 2 - Aspects of modern CPU architecture You do not have to be a CPU architecture specialist in order to be able to write efficient code. However, there are a few aspects of CPU architecture that you should understand. The hierarchical structure of CPU Memory CPU memory of modern CPUs is hierarchically organised. The memory levels close to the processor need to be fast, to serve it with data so that it can continue its work. As fast memory is expensive, the levels close to the processor are also smaller. The farther away from the processor the bigger they are, but also the slower. Each processing unit (or core) has a number of registers (~1 kB) and vector registers on which instructions can immediately operate (latency = 0 cycles). The registers are connected to a dedicated L1 cache (~32 kB per core), with a latency of ~ 1 cycle. This is in turn connected to: a dedicated L2 cache , (~256 kB per core), with a latency of ~10 cycles. This is in turn connected to: the L3 cache , which is shared among a group of cores, (~2 MB per core), with a latency of ~50 cycles. This is connected to: the main memory , which is shared by all cores,(256 GB - 2 TB), with a latency of ~200 cycles. The cores and the caches are on the same chip. For this reason they are considerably faster than the main memory. Faster memory is more expensive and therefor smaller. The figure below illustrates the layout. The I/O hub connects the cpu to the outside world, hard disk, network, ... When an instruction needs a data item in a register, the CPU looks first in the L1 cache, if it is there it will it to the register that was requested. Otherwise, the CPU looks in L2. If it is there, it is copied to L1 and the register. Otherwise, the CPU looks in L3. If it is there, it is copied to L2, L1 and the register. Otherwise, the CPU looks copies the cache line surrounding the data item to L3, L2, L1 and the data item itself to the register. A cache line is typically 64 bytes long and thus can contain 4 double precision floating point numbers or 8 single precision numbers. The main consequence of this strategy is that if the data item is part of an array, the next elements of that array will also be copied to L1 so that when processing the array the latency associated with main memory is amortized over 4 or 8 iterations. In addition, the CPU will notice when it is processing an array and prefetch the next cache line of the array in order to avoid that the processor has to wait for the data again. This strategy for loading data leads to two important best practices for making optimal use of the cache. Exploit Spatial locality : Organize your data layout in main memory in a way that data in a cache line are mostly needed together. Exploit Temporal locality : Organize your computations in a way that once a cache line is in L1 cache, as much as possible computations on that data are carried out. This favors a high computational intensity (see below). Common techniques for this are loop fusion and tiling . Loop fusion Here are two loops over an array x : for xi in x: do_something_with(xi) for xi in x: do_something_else_with(xi) If the array x is big, too big to fit in the cache, the above code would start loading x elements into the cache, cach line by cache line. Since x is to large to fit in the cache, at some point, when the cache is full, the CPU will start to evict the cache lines that were loaded long time a go ane are no more used to replace them with new cache lines. By the time the first loop finishes, the entire beginning of the x array has been evicted and the scond loop can start to transfer x again from the main memory to the registers, cache line by cache lina. this violiate the temporal locality principle. So, it incurs twice the data traffic. Loop fusion fuses the two loops into one and does all computations needed on xi when it is in the cache. for xi in x: do_something_with(xi) do_something_else_with(xi) The disadvantage of loop fusion is that the body of the loop may become too large and require more vector registers than are available. At that point some computations may be done sequentially and performance may suffer. Tiling Tiling is does the opposite. Ik keeps the loops separate but restricts them to chunks of x which fit in L1 cache. for chunk in x: # chunk is a slice of x that fits in L1 for xi in chunk: do_something_with(xi) for xi in chunk: do_something_else_with(xi) Again all computations that need to be done to xi are done when it is in L1 cache. Again the entire x array is transferred only once to the cache. A disadvantage of tiling is that the chunk size needs to be tuned to the size of L1, which may differ on different machines. Thus, this approach is not cache-oblivious . Loop fusion, on the other hand, is cache-oblivious. A good understanding of the workings of the hierarchical structure of processor memory is required to write efficient programs. Although, at first sight, it may seem an overly complex solution for a simple problem, but it is a good compromise to the many faces of a truly complex problem.\\ Tip An absolute must-see for this course is the excellent presentation on this matter by Scott Meyers: CPU Caches and Why You Care . (We can also recommend all his books on C++). Intra-core parallellisation features Modern CPUs are designed to (among other things) process loops as efficiently as possible, as loops typically account for a large part of the work load of a program. To make that possible CPUs use two important concepts: instruction pipelining (ILP) and SIMD vectorisation . Instruction pipelining Instruction pipelining is very well explained here . Basically, instructions are composed of micro-instructions (typically 5: instruction Fetch (IF), instruction decode (ID), execute (EX), memory access (MEM), write back (WB), details here ), each of which are executed in separate hardware units of the CPU. By executing the instructions sequentially, only one of those units would be active at a time: namely, the unit responsible for the current micro-instruction. By adding extra instruction registers, all micro-instruction hardware units can work simultaneously, but on micro-instructions pertaining to different but consecutive instructions. In this way, on average 5 (typically) instructions are being executed in parallel. This is very useful for loops. Executing micro-instructions serially, without pipelining: Pipelined execution of micro-instructions, in the middle part 5 micro-instructions are executed simultaneously, which means that on average 5 instructions are executed simultaneously: There are a couple of problems that may lead to pipeline stalls , situations where the pipeline comes to halt. A data element is requested that is not in the L1 cache. It must be fetched from deeper cache levels or even from main memory. This is called a cache miss . A L1 cache miss means that the data is not found in L1, but is found in L2. In a L2 cache miss it is not found in L2 but it is in L3, and a L3 cache miss, or a cache miss tout court* de data is not found in L3 and has to be fetched from main memory. The pipeline stops executing for a number of cycles corresponding to the latency of that cache miss. Data cache misses are the most important cause of pipeline stalls and as the latency can be really high (~100 cycles). A instruction is needed that is not in the L1 instruction cache. This may sometimes happen when a (large) function is called that is not inlined. Just as for a data cache miss, the pipeline stalls for a number of cycles corresponding to the latency of the cache miss, just as for a data cache miss. You might wonder how a pipeline proceeds when confronted with a branching instruction, a condition that has to be tested, and must start executing different streams of instructions depending on the outcome (typically if-then-else constructs). Here's the thing: it guesses the outcome of the test and starts executing the corresponding branch. As soon as it notices that it guessed wrong, which is necessarily after the condition has been tested, it stops, steps back and restarts at the correct branch. Obviously, the performance depends on how well it guesses. The guesses are generally rather smart. It is able to recognize temporal patterns, and if it doesn't find one, falls back on statistics. Random outcomes of the condition are thus detrimental to performance as its guess will be wrong at least half the time. SIMD vectorisation Scalar arithemetic, e.g. the addition, in CPUs operates as follows: the two operands are loaded in two (scalar) registers, the add instruction will add them and put the result in a third register. In modern CPUs the registers have been widened to contain more than one operand and the corresponding vector addition can compute and store the result in the same number of cycles. Typically, a vector register is now 512 bits wide, or 64 bytes, the same as the lenght of a cache line. SIMD vectorisation can in principle speed up loops by a factor of 2, 4, 8, 16, depending on the number of bytes the data elements use. Howecer, when the data being processed is not in the cache it does not help. Tip If your code does not vectorize, first find out if the data is in the cache, If not is does not help. The cost of floating point instructions Note All animals are equal, but some animals are more equal than others. Animal farm, George Orwell. Not all mathematical operations are equally fast. Here's a table listing their relative cost: cost operations cheap addition, subtraction, multipication rather expeesive division expensive square root very expensive trigonometric, exponential, logarithmic functions As an example, let's write a functon for the Lennard-Jones potential: Here's a first C++ translation of the mathematical expression of the Lennard-jones potential: double VLJ0( double r ) { return 1./pow(r,12) - 1./pow(r,6); } We measured the cost of VLJ0 by timing its application to a long array and express it relative to the best implementation we could come up with. The cost of VLJ0 is 18.0, so it is a really expensive implemenation. In view of the table above, that should come to no surprise: it has two divisions and two pow calls which raise a real number to a real power. pow is implemented using an exponential and a logarithm. Let's try to improve that. We can get rid of the divisions using 1/r = r^{-1} : function double VLJ1( double r ) { return std::pow(r,-12) - std::pow(r,-6); } This scores a bit better: 14.9, but the two pow calls remain expensive. The expression for the Lennard-Jones potential can be rewritten as V(r)=r^{-6}(r^{-6}-1) . Using a temporary to store r^{-6} we are left with only one pow call: double VLJ2( double r ) { double tmp = std::pow(r,-6); return tmp*(tmp-1.0); } This has a performance score of 7.8, still far away from 1. Realizing that we don't need to use pow because the expression has in fact integer powers, double VLJ3( double r ) { double tmp = 1.0/(r*r*r*r*r*r); return tmp*(tmp-1.0); } This has one division, 6 multiplications and subtraction. We can still reduce the number of multiplications a bit: double VLJ( Real_t r ) { double rr = 1./r; rr *= rr; double rr6 = rr*rr*rr; return rr6*(rr6-1); } Both these implementation have a performance score of 1. The optimum has been reached. The effect of two multiplications less in the last implementation doesn't show, because in fact the compiler optimizes them away anyway. Note Compilers are smart, but it will not do the math for you. There is yet a common sense optimisation that can be applied. The standard formulation of the Lennard-Jones potential is expressed as a function of r . Since it has only even powers of we can as well express it as a function of s=r^2 : V_2(s) = 1/s^3(1/s^3 - 1) At first sight, this may not immediately seem an optimisation, but in Molecular Dynamics the Lennard-Jones potential is embedded in a loop over all interacting pairs for which distance between the interacting atoms is computed: double interaction_energy = 0; for(int i=0; i<n_atosm; ++i) std::vector<int>& verlet_list_i = get_verlet_list(i); for(int j : verlet_list_i) { r_ij = std::sqrt( (x[j] - x[i])^2 + (y[j] - y[i])^2 + (z[j] - z[i])^2 ) if( r_ij < r_cutoff) interaction_energy += VLJ(r_ij); } Using V_2 this loop can be implemented as: double interaction_energy = 0; double r2_cutoff = r_cutoff^2; for(int i=0; i<n_atosm; ++i) std::vector<int>& verlet_list_i = get_verlet_list(i); for(int j : verlet_list_i) { s_ij = (x[j] - x[i])^2 + (y[j] - y[i])^2 + (z[j] - z[i])^2 if( s_ij < r2_cutoff) interaction_energy += V_2(s_ij); } This avoid the evaluation of a sqrt for every interacting pair of atoms. Homework Write a program in C++ or Fortran to time the above implementations of the Lennard-Jones potential. Since timers are not accurate enough to measure a single call, apply it to an array and divide the time for processing the array by the number of array elements. Think about the length of the array in relation to the size of the cache (L1/L2/L3). Think about vectorisation. Consequences of computer architecture for performance Recommendations for array processing The hierarchical organisation of computer memory has also important consequences for the layout of data arrays and for loops over arrays in terms of performance (see below). Loops should be long . Typically, at the begin and end of the loop thee pipeline is not full. When the loop is long, these sections can be amortized with respect to the inner section, where the pipeline is full. Branches in loops should be predictable . The outcome of unpredictable branches will be guessed wrongly, causing pipeline stalls. Sometimes it may be worthwile to sort the array according to the probability of the outcome if this work can be amortized over many loops. Loops should access data contiguously and with unit stride . This assures that at the next iteration of the loop the data element needed is already in the L1 Cache and can be accessed without delay, vector registers can be filled efficiently because they need contiguous elements from the input array. Loops should have high computatonal intensity . The computational intensity I_c is defined as I_c = \\frac {n_{cc}}{n_{rw}} , with n_{cc} the number of compute cycles and n_{rw} the total number of bytes read and written. A high computational intensity means many compute cycles and little data traffic to/from memory and thus implies that there will be no pipeline due to waiting for data to arrive. This is a compute bound loop. Low computational intensity, on the other hand, will cause many pipeline stalls by waiting for data. This is a memory bound loop. Here, it is the bandwidth (the speed at which data can be transported from main memory to the registers) that is the culprit, rather than the latency. Recommendations for data structures The unit stride for loops recommendation translates into a recommendation for data structures. Let's take Molecular Dynamics as an example. Object Oriented Programming (OOP) would propose a Atom class with properties for mass m , position \\textbf{r} , velocity \\textbf{v} , acceleration \\textbf{a} , and possibly others as well, but let's ignore those for the time being. Next, the object oriented programmer would create an array of Atoms. This approach is called an array of structures (AoS). The AoS approach leads to a data layout in memory like | m_0 , r_{x0} , r_{y0} , r_{z0} , v_{x0} , v_{y0} , v_{z0} , a_{x0} , | a_{y0} , a_{z0} , m_1 , r_{x1} , r_{y1} , r_{z1} , v_{x1} , v_{y1} , | v_{z1} , a_{x1} , a_{y1} , a_{z1} , m_2 , r_{x2} , r_{y2} , r_{z2} , | v_{x2} , v_{y2} , v_{z2} , a_{x2} , a_{y2} , a_{z2} , ... Assume we store the properties as single precision floating point numbers, hence a cache line spans 8 values. We marked the cache line boundaries in the list above with vertical bars. Suppose for some reason we need to find all atoms j for which r_{xj} is between x_{lwr} and x_{upr} . A loop over all atoms j would test r_{xj} and remember the j for which the test holds. Note that every cache line contains at most one single data item that we need in this algorithm. some cache lines will even contain no data items that we need. For every data iten we need, a new cache line must be loaded. This is terribly inefficient. There is a lot of data traffic, only 1/8 of which is useful and the bandwidth will saturate quickly. Vectorisation would be completely useless. To fill the vector register we would need 8 cache lines, most of which would correspond to cache misses and cost hundreds of cycles, before we can do 8 comparisons at once. The AoS, while intuitively very attractive, is clearly a disaster as it comes to performance. The - much better - alternative data structure is the SoA, structure of Arrays . This creates an AtomContainer class (to stay in the terminology of Object Oriented progrmming) containing an array of length n_{atoms} for each property. In this case there would be arrays for m , r_x , r_y , r_z , v_x , v_y , v_z , a_x , a_y , a_z . Now all r_x are stored contiguously in memory and every item in a cache would be used. Only one cache line would be needed to fill a vector register. Prefetching would do a perfect job. The SoA data structure is much more efficient, and once you get used to it, almost equally intuitive from an OOP viewpoint. Sometimes there is discussion about storing the coordinates of a vector, e.g. \\textbf{r} as per-coordinate arrays, as above, or as an array of vectors. The latter makes is more practical to define vector functions like magnitude, distance, dot and vector products, ... but they make it harder to SIMD vectorise those functions efficiently, because contiguous data items need to be moved into different vector registers. Selecting algorithms based on computational complexity The computational complexity of an algorithm is an indication of how the number of instructions in an algorithms scales with the problem size N . E.g. the work of an O(N^2) algorithm scales quadratically with its problem size. As an example consider brute force neighbour detection (Verlet list construction) of N interacting atoms in Molecular Dynamics: // C++ for (int i=0; i<N; ++i) for (int j=i+1; j<N; ++j) { r2ij = squared_distance(i,j); if (r2ij<r2cutoff) add_to_Verlet_list(i,j); } Note Note that we have avoided the computation of the square root by using the squared distance rather than the distance. The body of the inner for loop is executed N*(N-1)/2 = N^2/2 -N/2 times. Hence, it is O(N^2) . Cell-based Verlet list construction restricts the inner loop to the surrounding cells of atom i and is therefor O(N) . The computational complexity of an algorithm used to be a good criterion for algorithm selection: less work means faster, not? Due to the workings of the hierarchical memory of modern computers the answer is not so clear-cut. Consider two search algorithms for finding an element in a sorted array, linear search and binary search bisecting. Linear search simply loops over all elements until the element is found (or a larger element is found), and is thus O(N) . Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array. The complexity of this algorithm is O(log{N}) . Clearly, binary search finds the answer by visiting far fewer elements in the array as indicated by its lower complexity. However, contrary to linear search it visits the elements in the array non-contiguously, and it is very well possible that there will be a cache miss on every access. Linear search, on the other hand, will have no cache misses: it loads a cache line, visits all the elements in it and in the mean time the prefetching machinery takes care of loading the next cache line. It is only limited by the bandwidth. For small arrays linear search will be faster than binary search. For large arrays the situation is reversed. A clever approach would be to combine both methods: start with binary search and switch to linear search as soon as the part of the array to search is small enough. This needs some tuning to find the N at which both algorithms perform equally well. The combined algorithm is thus not cache-oblivious. Tip There is no silver bullet. All approaches have advantages and disadvantages, some may appear in this situation and others in another situation. The only valid reasoning is: numbers tell the tale ( meten is weten ): measure the performance of your code. Measure it twice, then measure again. Supercomputer architecture Note For a gentle but more detailed introduction about supercomputer architecture check out [this VSC course] (https://calcua. uantwerpen.be/courses/supercomputers-for-starters/Hardware-20221013-handouts.pdf). An updated version will appear soon here (look for 'Supercomputers for starters'). We haven't talked about supercomputer architecture so far. In fact, supercomputers are not so very different from ordinary computers. The basic building block of a supercomputer is a compute node , or a node tout court . It can be seen as an ordinary computer but without peripheral devices (no screen, no keyboard, no mouse, ...). A supercomputer consists of 100s to 1 000s of nodes (totalling up to 100 000s of cores), mutually connected to an ultra-fast network, the interconnect. The interconnect allows the nodes to exchange information so that they can work together on the same computational problem. It is the number of nodes and cores that makes a supercomputer a supercomputer, not (!) the performance of the individual cores. Motherboards for supercomputer nodes typically have 2 sockets, each of which holds a CPU. Technically speaking they behave as a single CPU double the size, and double the memory. Performance-wise, however, the latency across the two CPUs is typically a factor 2 larger. This goes by the name ccNUMA , or cache coherent non-uniform memory architecture . Cache coherence means that if caches of different copies hold copies of the same cache line, and one of them is modified, all copies are updated. NUMA means that there are different domains in the global address space of the node with different latency and/or bandwidth. CPU0 can access data in DRAM1, but this is significantly slower (typically 2x).","title":"Chapter 2 - Aspects of modern CPU architecture"},{"location":"chapter-2/#chapter-2-aspects-of-modern-cpu-architecture","text":"You do not have to be a CPU architecture specialist in order to be able to write efficient code. However, there are a few aspects of CPU architecture that you should understand.","title":"Chapter 2 - Aspects of modern CPU architecture"},{"location":"chapter-2/#the-hierarchical-structure-of-cpu-memory","text":"CPU memory of modern CPUs is hierarchically organised. The memory levels close to the processor need to be fast, to serve it with data so that it can continue its work. As fast memory is expensive, the levels close to the processor are also smaller. The farther away from the processor the bigger they are, but also the slower. Each processing unit (or core) has a number of registers (~1 kB) and vector registers on which instructions can immediately operate (latency = 0 cycles). The registers are connected to a dedicated L1 cache (~32 kB per core), with a latency of ~ 1 cycle. This is in turn connected to: a dedicated L2 cache , (~256 kB per core), with a latency of ~10 cycles. This is in turn connected to: the L3 cache , which is shared among a group of cores, (~2 MB per core), with a latency of ~50 cycles. This is connected to: the main memory , which is shared by all cores,(256 GB - 2 TB), with a latency of ~200 cycles. The cores and the caches are on the same chip. For this reason they are considerably faster than the main memory. Faster memory is more expensive and therefor smaller. The figure below illustrates the layout. The I/O hub connects the cpu to the outside world, hard disk, network, ... When an instruction needs a data item in a register, the CPU looks first in the L1 cache, if it is there it will it to the register that was requested. Otherwise, the CPU looks in L2. If it is there, it is copied to L1 and the register. Otherwise, the CPU looks in L3. If it is there, it is copied to L2, L1 and the register. Otherwise, the CPU looks copies the cache line surrounding the data item to L3, L2, L1 and the data item itself to the register. A cache line is typically 64 bytes long and thus can contain 4 double precision floating point numbers or 8 single precision numbers. The main consequence of this strategy is that if the data item is part of an array, the next elements of that array will also be copied to L1 so that when processing the array the latency associated with main memory is amortized over 4 or 8 iterations. In addition, the CPU will notice when it is processing an array and prefetch the next cache line of the array in order to avoid that the processor has to wait for the data again. This strategy for loading data leads to two important best practices for making optimal use of the cache. Exploit Spatial locality : Organize your data layout in main memory in a way that data in a cache line are mostly needed together. Exploit Temporal locality : Organize your computations in a way that once a cache line is in L1 cache, as much as possible computations on that data are carried out. This favors a high computational intensity (see below). Common techniques for this are loop fusion and tiling .","title":"The hierarchical structure of CPU Memory"},{"location":"chapter-2/#loop-fusion","text":"Here are two loops over an array x : for xi in x: do_something_with(xi) for xi in x: do_something_else_with(xi) If the array x is big, too big to fit in the cache, the above code would start loading x elements into the cache, cach line by cache line. Since x is to large to fit in the cache, at some point, when the cache is full, the CPU will start to evict the cache lines that were loaded long time a go ane are no more used to replace them with new cache lines. By the time the first loop finishes, the entire beginning of the x array has been evicted and the scond loop can start to transfer x again from the main memory to the registers, cache line by cache lina. this violiate the temporal locality principle. So, it incurs twice the data traffic. Loop fusion fuses the two loops into one and does all computations needed on xi when it is in the cache. for xi in x: do_something_with(xi) do_something_else_with(xi) The disadvantage of loop fusion is that the body of the loop may become too large and require more vector registers than are available. At that point some computations may be done sequentially and performance may suffer.","title":"Loop fusion"},{"location":"chapter-2/#tiling","text":"Tiling is does the opposite. Ik keeps the loops separate but restricts them to chunks of x which fit in L1 cache. for chunk in x: # chunk is a slice of x that fits in L1 for xi in chunk: do_something_with(xi) for xi in chunk: do_something_else_with(xi) Again all computations that need to be done to xi are done when it is in L1 cache. Again the entire x array is transferred only once to the cache. A disadvantage of tiling is that the chunk size needs to be tuned to the size of L1, which may differ on different machines. Thus, this approach is not cache-oblivious . Loop fusion, on the other hand, is cache-oblivious. A good understanding of the workings of the hierarchical structure of processor memory is required to write efficient programs. Although, at first sight, it may seem an overly complex solution for a simple problem, but it is a good compromise to the many faces of a truly complex problem.\\ Tip An absolute must-see for this course is the excellent presentation on this matter by Scott Meyers: CPU Caches and Why You Care . (We can also recommend all his books on C++).","title":"Tiling"},{"location":"chapter-2/#intra-core-parallellisation-features","text":"Modern CPUs are designed to (among other things) process loops as efficiently as possible, as loops typically account for a large part of the work load of a program. To make that possible CPUs use two important concepts: instruction pipelining (ILP) and SIMD vectorisation .","title":"Intra-core parallellisation features"},{"location":"chapter-2/#instruction-pipelining","text":"Instruction pipelining is very well explained here . Basically, instructions are composed of micro-instructions (typically 5: instruction Fetch (IF), instruction decode (ID), execute (EX), memory access (MEM), write back (WB), details here ), each of which are executed in separate hardware units of the CPU. By executing the instructions sequentially, only one of those units would be active at a time: namely, the unit responsible for the current micro-instruction. By adding extra instruction registers, all micro-instruction hardware units can work simultaneously, but on micro-instructions pertaining to different but consecutive instructions. In this way, on average 5 (typically) instructions are being executed in parallel. This is very useful for loops. Executing micro-instructions serially, without pipelining: Pipelined execution of micro-instructions, in the middle part 5 micro-instructions are executed simultaneously, which means that on average 5 instructions are executed simultaneously: There are a couple of problems that may lead to pipeline stalls , situations where the pipeline comes to halt. A data element is requested that is not in the L1 cache. It must be fetched from deeper cache levels or even from main memory. This is called a cache miss . A L1 cache miss means that the data is not found in L1, but is found in L2. In a L2 cache miss it is not found in L2 but it is in L3, and a L3 cache miss, or a cache miss tout court* de data is not found in L3 and has to be fetched from main memory. The pipeline stops executing for a number of cycles corresponding to the latency of that cache miss. Data cache misses are the most important cause of pipeline stalls and as the latency can be really high (~100 cycles). A instruction is needed that is not in the L1 instruction cache. This may sometimes happen when a (large) function is called that is not inlined. Just as for a data cache miss, the pipeline stalls for a number of cycles corresponding to the latency of the cache miss, just as for a data cache miss. You might wonder how a pipeline proceeds when confronted with a branching instruction, a condition that has to be tested, and must start executing different streams of instructions depending on the outcome (typically if-then-else constructs). Here's the thing: it guesses the outcome of the test and starts executing the corresponding branch. As soon as it notices that it guessed wrong, which is necessarily after the condition has been tested, it stops, steps back and restarts at the correct branch. Obviously, the performance depends on how well it guesses. The guesses are generally rather smart. It is able to recognize temporal patterns, and if it doesn't find one, falls back on statistics. Random outcomes of the condition are thus detrimental to performance as its guess will be wrong at least half the time.","title":"Instruction pipelining"},{"location":"chapter-2/#simd-vectorisation","text":"Scalar arithemetic, e.g. the addition, in CPUs operates as follows: the two operands are loaded in two (scalar) registers, the add instruction will add them and put the result in a third register. In modern CPUs the registers have been widened to contain more than one operand and the corresponding vector addition can compute and store the result in the same number of cycles. Typically, a vector register is now 512 bits wide, or 64 bytes, the same as the lenght of a cache line. SIMD vectorisation can in principle speed up loops by a factor of 2, 4, 8, 16, depending on the number of bytes the data elements use. Howecer, when the data being processed is not in the cache it does not help. Tip If your code does not vectorize, first find out if the data is in the cache, If not is does not help.","title":"SIMD vectorisation"},{"location":"chapter-2/#the-cost-of-floating-point-instructions","text":"Note All animals are equal, but some animals are more equal than others. Animal farm, George Orwell. Not all mathematical operations are equally fast. Here's a table listing their relative cost: cost operations cheap addition, subtraction, multipication rather expeesive division expensive square root very expensive trigonometric, exponential, logarithmic functions As an example, let's write a functon for the Lennard-Jones potential: Here's a first C++ translation of the mathematical expression of the Lennard-jones potential: double VLJ0( double r ) { return 1./pow(r,12) - 1./pow(r,6); } We measured the cost of VLJ0 by timing its application to a long array and express it relative to the best implementation we could come up with. The cost of VLJ0 is 18.0, so it is a really expensive implemenation. In view of the table above, that should come to no surprise: it has two divisions and two pow calls which raise a real number to a real power. pow is implemented using an exponential and a logarithm. Let's try to improve that. We can get rid of the divisions using 1/r = r^{-1} : function double VLJ1( double r ) { return std::pow(r,-12) - std::pow(r,-6); } This scores a bit better: 14.9, but the two pow calls remain expensive. The expression for the Lennard-Jones potential can be rewritten as V(r)=r^{-6}(r^{-6}-1) . Using a temporary to store r^{-6} we are left with only one pow call: double VLJ2( double r ) { double tmp = std::pow(r,-6); return tmp*(tmp-1.0); } This has a performance score of 7.8, still far away from 1. Realizing that we don't need to use pow because the expression has in fact integer powers, double VLJ3( double r ) { double tmp = 1.0/(r*r*r*r*r*r); return tmp*(tmp-1.0); } This has one division, 6 multiplications and subtraction. We can still reduce the number of multiplications a bit: double VLJ( Real_t r ) { double rr = 1./r; rr *= rr; double rr6 = rr*rr*rr; return rr6*(rr6-1); } Both these implementation have a performance score of 1. The optimum has been reached. The effect of two multiplications less in the last implementation doesn't show, because in fact the compiler optimizes them away anyway. Note Compilers are smart, but it will not do the math for you. There is yet a common sense optimisation that can be applied. The standard formulation of the Lennard-Jones potential is expressed as a function of r . Since it has only even powers of we can as well express it as a function of s=r^2 : V_2(s) = 1/s^3(1/s^3 - 1) At first sight, this may not immediately seem an optimisation, but in Molecular Dynamics the Lennard-Jones potential is embedded in a loop over all interacting pairs for which distance between the interacting atoms is computed: double interaction_energy = 0; for(int i=0; i<n_atosm; ++i) std::vector<int>& verlet_list_i = get_verlet_list(i); for(int j : verlet_list_i) { r_ij = std::sqrt( (x[j] - x[i])^2 + (y[j] - y[i])^2 + (z[j] - z[i])^2 ) if( r_ij < r_cutoff) interaction_energy += VLJ(r_ij); } Using V_2 this loop can be implemented as: double interaction_energy = 0; double r2_cutoff = r_cutoff^2; for(int i=0; i<n_atosm; ++i) std::vector<int>& verlet_list_i = get_verlet_list(i); for(int j : verlet_list_i) { s_ij = (x[j] - x[i])^2 + (y[j] - y[i])^2 + (z[j] - z[i])^2 if( s_ij < r2_cutoff) interaction_energy += V_2(s_ij); } This avoid the evaluation of a sqrt for every interacting pair of atoms. Homework Write a program in C++ or Fortran to time the above implementations of the Lennard-Jones potential. Since timers are not accurate enough to measure a single call, apply it to an array and divide the time for processing the array by the number of array elements. Think about the length of the array in relation to the size of the cache (L1/L2/L3). Think about vectorisation.","title":"The cost of floating point instructions"},{"location":"chapter-2/#consequences-of-computer-architecture-for-performance","text":"","title":"Consequences of computer architecture for performance"},{"location":"chapter-2/#recommendations-for-array-processing","text":"The hierarchical organisation of computer memory has also important consequences for the layout of data arrays and for loops over arrays in terms of performance (see below). Loops should be long . Typically, at the begin and end of the loop thee pipeline is not full. When the loop is long, these sections can be amortized with respect to the inner section, where the pipeline is full. Branches in loops should be predictable . The outcome of unpredictable branches will be guessed wrongly, causing pipeline stalls. Sometimes it may be worthwile to sort the array according to the probability of the outcome if this work can be amortized over many loops. Loops should access data contiguously and with unit stride . This assures that at the next iteration of the loop the data element needed is already in the L1 Cache and can be accessed without delay, vector registers can be filled efficiently because they need contiguous elements from the input array. Loops should have high computatonal intensity . The computational intensity I_c is defined as I_c = \\frac {n_{cc}}{n_{rw}} , with n_{cc} the number of compute cycles and n_{rw} the total number of bytes read and written. A high computational intensity means many compute cycles and little data traffic to/from memory and thus implies that there will be no pipeline due to waiting for data to arrive. This is a compute bound loop. Low computational intensity, on the other hand, will cause many pipeline stalls by waiting for data. This is a memory bound loop. Here, it is the bandwidth (the speed at which data can be transported from main memory to the registers) that is the culprit, rather than the latency.","title":"Recommendations for array processing"},{"location":"chapter-2/#recommendations-for-data-structures","text":"The unit stride for loops recommendation translates into a recommendation for data structures. Let's take Molecular Dynamics as an example. Object Oriented Programming (OOP) would propose a Atom class with properties for mass m , position \\textbf{r} , velocity \\textbf{v} , acceleration \\textbf{a} , and possibly others as well, but let's ignore those for the time being. Next, the object oriented programmer would create an array of Atoms. This approach is called an array of structures (AoS). The AoS approach leads to a data layout in memory like | m_0 , r_{x0} , r_{y0} , r_{z0} , v_{x0} , v_{y0} , v_{z0} , a_{x0} , | a_{y0} , a_{z0} , m_1 , r_{x1} , r_{y1} , r_{z1} , v_{x1} , v_{y1} , | v_{z1} , a_{x1} , a_{y1} , a_{z1} , m_2 , r_{x2} , r_{y2} , r_{z2} , | v_{x2} , v_{y2} , v_{z2} , a_{x2} , a_{y2} , a_{z2} , ... Assume we store the properties as single precision floating point numbers, hence a cache line spans 8 values. We marked the cache line boundaries in the list above with vertical bars. Suppose for some reason we need to find all atoms j for which r_{xj} is between x_{lwr} and x_{upr} . A loop over all atoms j would test r_{xj} and remember the j for which the test holds. Note that every cache line contains at most one single data item that we need in this algorithm. some cache lines will even contain no data items that we need. For every data iten we need, a new cache line must be loaded. This is terribly inefficient. There is a lot of data traffic, only 1/8 of which is useful and the bandwidth will saturate quickly. Vectorisation would be completely useless. To fill the vector register we would need 8 cache lines, most of which would correspond to cache misses and cost hundreds of cycles, before we can do 8 comparisons at once. The AoS, while intuitively very attractive, is clearly a disaster as it comes to performance. The - much better - alternative data structure is the SoA, structure of Arrays . This creates an AtomContainer class (to stay in the terminology of Object Oriented progrmming) containing an array of length n_{atoms} for each property. In this case there would be arrays for m , r_x , r_y , r_z , v_x , v_y , v_z , a_x , a_y , a_z . Now all r_x are stored contiguously in memory and every item in a cache would be used. Only one cache line would be needed to fill a vector register. Prefetching would do a perfect job. The SoA data structure is much more efficient, and once you get used to it, almost equally intuitive from an OOP viewpoint. Sometimes there is discussion about storing the coordinates of a vector, e.g. \\textbf{r} as per-coordinate arrays, as above, or as an array of vectors. The latter makes is more practical to define vector functions like magnitude, distance, dot and vector products, ... but they make it harder to SIMD vectorise those functions efficiently, because contiguous data items need to be moved into different vector registers.","title":"Recommendations for data structures"},{"location":"chapter-2/#selecting-algorithms-based-on-computational-complexity","text":"The computational complexity of an algorithm is an indication of how the number of instructions in an algorithms scales with the problem size N . E.g. the work of an O(N^2) algorithm scales quadratically with its problem size. As an example consider brute force neighbour detection (Verlet list construction) of N interacting atoms in Molecular Dynamics: // C++ for (int i=0; i<N; ++i) for (int j=i+1; j<N; ++j) { r2ij = squared_distance(i,j); if (r2ij<r2cutoff) add_to_Verlet_list(i,j); } Note Note that we have avoided the computation of the square root by using the squared distance rather than the distance. The body of the inner for loop is executed N*(N-1)/2 = N^2/2 -N/2 times. Hence, it is O(N^2) . Cell-based Verlet list construction restricts the inner loop to the surrounding cells of atom i and is therefor O(N) . The computational complexity of an algorithm used to be a good criterion for algorithm selection: less work means faster, not? Due to the workings of the hierarchical memory of modern computers the answer is not so clear-cut. Consider two search algorithms for finding an element in a sorted array, linear search and binary search bisecting. Linear search simply loops over all elements until the element is found (or a larger element is found), and is thus O(N) . Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array. The complexity of this algorithm is O(log{N}) . Clearly, binary search finds the answer by visiting far fewer elements in the array as indicated by its lower complexity. However, contrary to linear search it visits the elements in the array non-contiguously, and it is very well possible that there will be a cache miss on every access. Linear search, on the other hand, will have no cache misses: it loads a cache line, visits all the elements in it and in the mean time the prefetching machinery takes care of loading the next cache line. It is only limited by the bandwidth. For small arrays linear search will be faster than binary search. For large arrays the situation is reversed. A clever approach would be to combine both methods: start with binary search and switch to linear search as soon as the part of the array to search is small enough. This needs some tuning to find the N at which both algorithms perform equally well. The combined algorithm is thus not cache-oblivious. Tip There is no silver bullet. All approaches have advantages and disadvantages, some may appear in this situation and others in another situation. The only valid reasoning is: numbers tell the tale ( meten is weten ): measure the performance of your code. Measure it twice, then measure again.","title":"Selecting algorithms based on computational complexity"},{"location":"chapter-2/#supercomputer-architecture","text":"Note For a gentle but more detailed introduction about supercomputer architecture check out [this VSC course] (https://calcua. uantwerpen.be/courses/supercomputers-for-starters/Hardware-20221013-handouts.pdf). An updated version will appear soon here (look for 'Supercomputers for starters'). We haven't talked about supercomputer architecture so far. In fact, supercomputers are not so very different from ordinary computers. The basic building block of a supercomputer is a compute node , or a node tout court . It can be seen as an ordinary computer but without peripheral devices (no screen, no keyboard, no mouse, ...). A supercomputer consists of 100s to 1 000s of nodes (totalling up to 100 000s of cores), mutually connected to an ultra-fast network, the interconnect. The interconnect allows the nodes to exchange information so that they can work together on the same computational problem. It is the number of nodes and cores that makes a supercomputer a supercomputer, not (!) the performance of the individual cores. Motherboards for supercomputer nodes typically have 2 sockets, each of which holds a CPU. Technically speaking they behave as a single CPU double the size, and double the memory. Performance-wise, however, the latency across the two CPUs is typically a factor 2 larger. This goes by the name ccNUMA , or cache coherent non-uniform memory architecture . Cache coherence means that if caches of different copies hold copies of the same cache line, and one of them is modified, all copies are updated. NUMA means that there are different domains in the global address space of the node with different latency and/or bandwidth. CPU0 can access data in DRAM1, but this is significantly slower (typically 2x).","title":"Supercomputer architecture"},{"location":"chapter-3/","text":"Chapter 3 - Optimise first, then parallelize When to parallelize, and what to do first... When your program takes too long, the memory of your machine is too small for your problem or the accuracy you need cannot be met, you're hitting the wall. Parallelization seems necessary, and you feel in need of a supercomputer. However, supercomputers are expensive machines and resources are limited. It should come to no surprise that it is expected that programs are allowed to run on supercomputers only if they make efficient use of their resources. Often, serial programs provide possibilities to improve the performance. These come in two categories: common sense optimisations (often completely overlooked by researchers) which rely on a good understanding of the mathematical formulation of the problem and the algorithm, and code optimisations which rely on understanding processor architecture and compilers. Lets first look at common sense optimisations. Common sense optimisations Common sense optimizations come from a good understanding of the mathematical formulation of the problem and seeing opportunities to reduce the amount of work. We give three examples. 1. Magnetization of bulk ferromagnets I was asked to speed up a program for computing the magnetisation m(T) of bulk ferromagnets as a function of temperature T . This is given by a self-consistent solution of the equations: m = \\frac{1}{2 + 4\\Phi(m)} \\Phi(m) = \\frac{1}{N} \\sum_{\\textbf{k}} \\frac{1}{e^{\\beta\\eta(\\textbf{k})m} - 1} with \\beta = 1/{k_B T} . At T=0 we have m(0) = m_0 = 0.5 , and at high T , m(T) approaches zero. The solution is a curve like this: The program compute this as follows: For any temperature T , set m = m_0 as an inital guess. Then iterate m_ {i+1} = 1/(2 + 4\\Phi(m_i)) until \\Delta m = m_{i+1} - m_i is small. Here, \\Phi(m) = \\sum_{n=1}^\\infty \\frac{a}{\\pi} \\left(\\int_0^{\\pi/a} dq e^{-nm\\beta\\eta_1(q)}\\right)^3 and the integral is computed using Gauss-Legendre integration on 64 points. The choice of m=0.5 as initial guess is obviously a good one close to T=0 . However, looking at the graph above, it becomes clear that as T increases the solution moves further and further away from 0.5 . Furthermore, if we compute tempurature points at equidistant tempurature points, T_j = \\delta j for some \\delta and j=0,1,2, . .. , it is also clear that the solution of the previous temperature point, i.e. m_{j-1} , is a far better initial initial guess guess than m_0 . This turns out to be 1.4x faster. Not a tremendous improvement, but as the graph above seems continuous w e can take this idea a step further: using interpolation from solutions a lower temperature points to predict the next solution and use that as an initial guess. Linear interpolation of m_j from m_{j-1} and m_{j-2} gives a speedup of 1.94x and quadratic interpolation from m_{j-1} , m_{j-2} and m_{j-3} a factor of 2.4x. That is a substantial speedup achieved without acttually modifying the code. This optimisation comes entirely from understanding what your algorithm actually does. Investigation of the code itself demonstrated that it made suffered from a lot of dynamic memory management and that it did not vectorize. After fixing these issues, the code ran an additional 13.6x faster. In total the code was sped up by an impressive 32.6x. 2. Transforming the problem domain At another occasion I had to investigate a code for calculating a complicated sum of integrals in real space. After fixing some bugs and some optimisation to improve the efficiency, it was still rather slow because the formula converged slowly As the code was running almost at peak performance, so there was little room for improvement. However, at some point we tried to apply the Fourier transform to get an expression in frequency space. This expression turned out to converge much faster and consequently far less terms had to be computed, yielding a speedup of almost 2 orders of magnitude and was much more accurate. This is another example of common sense optimisation originating in a good mathematical background. The natural formulation of a problem is not necessarily the best to use for computation. 3. Transforming data to reduce their memory footprint I recently reviewed a Python code by the Vlaamse Milieumaatschappij for modelling the migration of invertebrate aquatic species in response to improving (or deteriorating) water quality. The program read a lot data from .csv files. For a project it was necessary to run a parameter optimisation. That is a procedure where model parameters are varied until the outcome is satisfactory. If the number of model parameters is large the number of program runs required can easily reach in the 100 000s. The program was parallellized on a single node. However, the program was using that many data that 18 cores of the 128 cores available on a node already consumed all the available memory. By replacing the data types of the columns of the datasets with datatypes with a smaller footprint, such as replacing categorical data with integer IDs, replacing 32-bit integers with 16-bit or even 8-bit integers, float64 real numbers with float32 or float16 numbers reduced the amount of data used by a factor 8. All of a sudden much more cores could be engaged in the computation and the simulation sped up considerably. Some of these \"common sense optimisations\" may seem obvious. Yet, of all the codes I reviewed during my career, few of them were immune to common sense optimisation. Perhaps, developing (scientific) software takes a special mindset: Tip The scientific software developer mindset : Constantly ask yourself 'How can I improve this? How can I make it faster, leaner, more readable, more flexible, more reusable, ... ?' Common sense optimisations are optimisations that in general don't require complex code analysis, require very little code changes and thus little effort to implement them. Yet they can make a significant contribution. Code optimisations Code optimisations are optimisations aiming at making your solution method run as efficient as possible on the machine(s) that you have at your disposal. This is sometimes referred as code modernisation , because code that was optimised for the CPUs of two years a go may well need some revision for the latest CPU technology. These optimisations must, necessarily, take in account the specific processor architecture of your machine(s). Important topics are: Avoid pipeline stalls (due to impredictable branches, e.g. ) Ensure SIMD vectorisation. On modern processors vector registers can contain 4 double precision floating point numbers or 8 single precision numbers and vector instructions operate on these in the same number of cycles as scalar instructions. Failing to vectorise can reduce the speed of your program by a factor up to 8x! Smart data access patterns are indispensable for programs with a memory footprint that exceeds the size of the cache. Transferring data from main memory (DRAM) to the processor's registers is slow: typical latencies are in the order of 100 cycles (which potentially wastes ~800 single precision vectorised operations). Vector instructions are of no help if the processing unit must wait for the data. This is clearly much more technical and complicated (in the sense that it requires knowledge from outside the scientific domain of the problem you are trying to solve). Especially fixing memory access patterns can be difficult and a lot ofwork, as you may have to change the data structures used by your program, which usually also means rewriting a lot of code accessing the data. Such code optimisations can contribute significantly to the performance of a program, typically around 5-10x, but possibly more. As supercomputers are expensive research infrastructure in high demand, we cannot effort to waste resources. That being said, the lifetime of your program is also of importance. If you are developing a code that will be run a few times during your Master project or PhD, using only a hundred of node days, and to be forgotten afterwards, it is perhaps not worth to spend 3 months optimising it. Often, however, there is a way around these technicalities. If the scientific problem you are trying to solve, can be expressed in the formalism of common mathematical domains, e.g. linear algebra, Fourier analysis, ..., there is a good chance that there are good software libraries, designed with HPC in mind, that solved these problems for you. In most cases there are even bindings available for your favorite progamming language (C/C++, Fortran, Python, ...). All you have to do is translate the mathematical formulation of your problem into library calls. Tip Use HPC libraries as much as possible . There is little chance that you will outperform them. Quite to the contrary: your own code will probably do significantly worse. By using HPC libraries you gain three times: you gain performance, you gain development time as you will need a lot less code to solve your problem, less debugging, simpler maintenance, ... your learn how to use the library which will get you at speed readily when you take on your next scientific problem. Tip Don't reinvent the wheel . The wheel was invented ~8000 years ago. Many very clever people have put effort in it and is pretty perfect by now. Reinventing it will unlikely result in an improvement. By extension: if you need some code, spend some time google-ing around to learn what is already available and how other researchers attack the problem. It can save you weeks of programming and debugging. Adapting someone else's code to your needs will learn you more than coding it from scratch. You'll discover other approaches to coding problems than yours, other language constructs, idioms, dependencies to build on, learn to read someone else's code, learn to integrate pieces. When is code optimized enough? Tip Premature optimization is the root of all evil Donald Knuth . This quote by a famous computer scientist in 1974 is often used to argue that you should only optimize if there is a real need. If code is too slow, measurements ( profiling ) should tell in which part of the code most time is spent. That part needs optimision. Iterate this a few times. Blind optimisation leads to useless and developer time wasting micro-optimisations rendering the code hard to read and maintain. On the other hand, if we are writing code for a supercomputer, it better be super-efficient. But even then, depending on the lifetime of the program we are writing, there is a point at which the efforts spent optimising are outweighed by having to wait for the program going in production. How can one judge wether a code needs further optimization or not? Obviously, there are no tricks for exposing opportunities for common sense optimisations, nor for knowing wether better algorithms exist. That is domain knowledgs, it comes with experience, and requires a lot of background. But for a given code and given input, can we know wether improvements are possible? In Chapter 1 we mentioned the existence of machine limits, the peak performance, P_p , the maximum number of floating point operstions that can be executed per second, and the bandwidth, B , the maximum number of bytes that can be moved between main memory and the CPU's registers per second. It is instructive to study how these machine limits govern the maximum performance P_{max} as a function of the computational intensity I_c . If the CPU must not wait for data, P_{max} = P_p . In the situation where the bandwidth is limiting the computation P_{max} = BI_c . This leads to the formula: P_{max} = min(P_p,BI_c) This is called the roofling model, since its graph looks like a roofliine. We can measure the actual performance and computational intensity of the program and plot it on the graph. The point must necessarily be under the roofline. For a micro-benchmark, such as a loop with a simple body, we can compute I_c by hand, count the Flops and time the benchmark to obtain the computational intensity. For an entire program a performance analysis tool can construct the graph and measure where the program is in the graph. Let's discuss 4 different cases, corresponding to the four numbered spots in the graph above. Point 1 lies in the bandwidth limited region, but well below the roofline. Something prevents the program to go at the maximum performance. There can be many causes: bad memory access causing cache misses, the code may fail to vectorize, pipeline stalls, ... for a micro-benchmark you can perhaps spot the cause without help. For a larger program a performance analyzer will highlight the problems. Point 2 lies in the peak performance limited region, also well below the roofline. Hence, cache misses are unlikely the cause. Point 3 lies close to the roofline and the boundary between the bandwidth limited region and the peak performance limited region. This the sweet spot. Both peak performance and bandwith are fully used. Point 4 lies close to the roofline in the peak performance limited region. This is an energy-efficient computation at peak performance. It moves little data (high I_c ). Moving data is by far the most energy consuming part in a computation. Common approaches towards parallelization Before we discuss common parallelization approaches, we need to explain some concepts: process (wikipedia) : \"In computing, a process is the instance of a computer program that is being executed by one or more threads.\" A process has its own address space , the region of main memory that can be addressed by the process. Normally, a process cannot go outside its address space, nor can any other process go inside the process's own address space. An exception is when both processes agree to communicate, which is the basis of distributed memory parallelism (see below). In general, a process is restricted to a single node, and the maximum number of parallel threads is equal to the number of cores on that node. thread (wikipedia) : \"In computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed ...\". The instructions in a thread are thus by definition sequential. In the context of parallel computing, parallel threads are managed by the parent process to run different tasks in parallel. Obviously, parallel threads need to run on distinct cores to be truely concurrent. As threads belong to a process, they can in principle have access to the entire address space of the process. Sometimes a distinction is made between hardware threads and software threads. A software thread is a set of sequential instructions for a computational task that is scheduled by the program to execute. When its execution starts, it is assigned to a core. software threads can be interrupted, in order to let the core do other, more urgent work, e.g. , and restarted. There can be many more software threads in a program than it has cores available, but, obviously, they cannot all run in parallel. Software threads arre very useful in personal computers with many interactive applications opened simultaneously, where are many non-urgent tasks. For HPC applications they are a bit heavy weight. A hardware thread is a lightweight software thread that is exclusively tied to a core. When given work, it can start immediately and runs to completion without interruption. That is certainly useful in HPC where not loosing compute cycles is more important than flexibility. Now that we understand the concepts of processes and threads, we can explain three different types of parallelization: Shared memory parallelization In shared memory parallelization there is one process managing a number of threads to do work in parallel. As all the threads belong to the same process, they have access to the entire memory address space, that is, they share memory. To avoid problems, as well as for performance reasons, the variables inside a thread are private by default, i.e. they can only be accessed by the thread itself, and must be declare shared if other threads should have access too. Cooperating threads must exchange information by reading and writing to shared variables. The fact that all the threads belong to the same process, implies that shared memory programs are limited to a single node, because a process cannot span several nodes. However, there exist shared memory machines larger than a typical supercomputer node, e.g. SuperDome at KU leuven. Such systems allow to run a shared memory program with much more threads and much more memory. This approach can be useful when distributed memory parallelization is not feasible for some reason. The most common framework for shared memory parallelization is OpenMP . OpenMP parallelization of a sequential program is relatively simple, requiring little changes to the source code in the form of directives. A good starting point for OpenMP parallelization is this video . An important limitation of OpenMP is that it is only available in C/C++/Fortran, and not Python. Fortunately, Python has other options, e.g. multiprocessing , concurrent.futures and dask . In addition it is possible to build your own Python modules from C++ or Fortran code, in which OpenMP is used to parallelize some tasks. The popular numpy is a good example. PGAS PGAS , or Partitioned Global Address Space , is a parallel programming paradigm that provide communication operations involving a global memory address space on top of an otherwise distributed memory system. Distributed memory parallelization Distributed memory parallelization is the opposite of shared memory parallelization. There are many process, each with only a single-thread. Every process has its own memory address space. These address spaces are not shared, they are distributed. Therefor, explicity communication is necessary to exchange information. For processes on the same machine (=node) this communication is intra-node, but for processes on distinct machines messages are sent over the interconnect. Distributed memory programs are considerably more complex to write, as the communication must be explicitly handled by the programmer, but may use as many processes as you want. Transformation of a sequential program into a distributed memory program is often a big programming effort. The most common framework is MPI . MPI is available in C/C++/Fortran and also in Python by the mpi4py module. Hybrid memory parallelization Hybrid memory parallelization combines both approaches. It has an unlimited number of processes, and a number of threads per process, which run in parallel in a shared memory approach (OpenMP). The process communicate with each other using MPI. Typically, the computation is organised as one proces per NUMA domain and one thread per core in dat NUMA domain. This approach uses shared memory parallelization where it is useful (on a NUMA domain), but removes the limitation to a single machine. It has less processes, and thus less overhead in terms of memory footprint, and communication overhead. It is also a bit more complex that pure distributed memory parallelization, and much more complex than shared memory parallelization. Hybrid memoryy parallelization is usually implemented with OpenMP at the shared memory level and MPI at the distributed level.","title":"Chapter 3 - Optimise first, then parallelize"},{"location":"chapter-3/#chapter-3-optimise-first-then-parallelize","text":"","title":"Chapter 3 - Optimise first, then parallelize"},{"location":"chapter-3/#when-to-parallelize-and-what-to-do-first","text":"When your program takes too long, the memory of your machine is too small for your problem or the accuracy you need cannot be met, you're hitting the wall. Parallelization seems necessary, and you feel in need of a supercomputer. However, supercomputers are expensive machines and resources are limited. It should come to no surprise that it is expected that programs are allowed to run on supercomputers only if they make efficient use of their resources. Often, serial programs provide possibilities to improve the performance. These come in two categories: common sense optimisations (often completely overlooked by researchers) which rely on a good understanding of the mathematical formulation of the problem and the algorithm, and code optimisations which rely on understanding processor architecture and compilers. Lets first look at common sense optimisations.","title":"When to parallelize, and what to do first..."},{"location":"chapter-3/#common-sense-optimisations","text":"Common sense optimizations come from a good understanding of the mathematical formulation of the problem and seeing opportunities to reduce the amount of work. We give three examples.","title":"Common sense optimisations"},{"location":"chapter-3/#1-magnetization-of-bulk-ferromagnets","text":"I was asked to speed up a program for computing the magnetisation m(T) of bulk ferromagnets as a function of temperature T . This is given by a self-consistent solution of the equations: m = \\frac{1}{2 + 4\\Phi(m)} \\Phi(m) = \\frac{1}{N} \\sum_{\\textbf{k}} \\frac{1}{e^{\\beta\\eta(\\textbf{k})m} - 1} with \\beta = 1/{k_B T} . At T=0 we have m(0) = m_0 = 0.5 , and at high T , m(T) approaches zero. The solution is a curve like this: The program compute this as follows: For any temperature T , set m = m_0 as an inital guess. Then iterate m_ {i+1} = 1/(2 + 4\\Phi(m_i)) until \\Delta m = m_{i+1} - m_i is small. Here, \\Phi(m) = \\sum_{n=1}^\\infty \\frac{a}{\\pi} \\left(\\int_0^{\\pi/a} dq e^{-nm\\beta\\eta_1(q)}\\right)^3 and the integral is computed using Gauss-Legendre integration on 64 points. The choice of m=0.5 as initial guess is obviously a good one close to T=0 . However, looking at the graph above, it becomes clear that as T increases the solution moves further and further away from 0.5 . Furthermore, if we compute tempurature points at equidistant tempurature points, T_j = \\delta j for some \\delta and j=0,1,2, . .. , it is also clear that the solution of the previous temperature point, i.e. m_{j-1} , is a far better initial initial guess guess than m_0 . This turns out to be 1.4x faster. Not a tremendous improvement, but as the graph above seems continuous w e can take this idea a step further: using interpolation from solutions a lower temperature points to predict the next solution and use that as an initial guess. Linear interpolation of m_j from m_{j-1} and m_{j-2} gives a speedup of 1.94x and quadratic interpolation from m_{j-1} , m_{j-2} and m_{j-3} a factor of 2.4x. That is a substantial speedup achieved without acttually modifying the code. This optimisation comes entirely from understanding what your algorithm actually does. Investigation of the code itself demonstrated that it made suffered from a lot of dynamic memory management and that it did not vectorize. After fixing these issues, the code ran an additional 13.6x faster. In total the code was sped up by an impressive 32.6x.","title":"1. Magnetization of bulk ferromagnets"},{"location":"chapter-3/#2-transforming-the-problem-domain","text":"At another occasion I had to investigate a code for calculating a complicated sum of integrals in real space. After fixing some bugs and some optimisation to improve the efficiency, it was still rather slow because the formula converged slowly As the code was running almost at peak performance, so there was little room for improvement. However, at some point we tried to apply the Fourier transform to get an expression in frequency space. This expression turned out to converge much faster and consequently far less terms had to be computed, yielding a speedup of almost 2 orders of magnitude and was much more accurate. This is another example of common sense optimisation originating in a good mathematical background. The natural formulation of a problem is not necessarily the best to use for computation.","title":"2. Transforming the problem domain"},{"location":"chapter-3/#3-transforming-data-to-reduce-their-memory-footprint","text":"I recently reviewed a Python code by the Vlaamse Milieumaatschappij for modelling the migration of invertebrate aquatic species in response to improving (or deteriorating) water quality. The program read a lot data from .csv files. For a project it was necessary to run a parameter optimisation. That is a procedure where model parameters are varied until the outcome is satisfactory. If the number of model parameters is large the number of program runs required can easily reach in the 100 000s. The program was parallellized on a single node. However, the program was using that many data that 18 cores of the 128 cores available on a node already consumed all the available memory. By replacing the data types of the columns of the datasets with datatypes with a smaller footprint, such as replacing categorical data with integer IDs, replacing 32-bit integers with 16-bit or even 8-bit integers, float64 real numbers with float32 or float16 numbers reduced the amount of data used by a factor 8. All of a sudden much more cores could be engaged in the computation and the simulation sped up considerably. Some of these \"common sense optimisations\" may seem obvious. Yet, of all the codes I reviewed during my career, few of them were immune to common sense optimisation. Perhaps, developing (scientific) software takes a special mindset: Tip The scientific software developer mindset : Constantly ask yourself 'How can I improve this? How can I make it faster, leaner, more readable, more flexible, more reusable, ... ?' Common sense optimisations are optimisations that in general don't require complex code analysis, require very little code changes and thus little effort to implement them. Yet they can make a significant contribution.","title":"3. Transforming data to reduce their memory footprint"},{"location":"chapter-3/#code-optimisations","text":"Code optimisations are optimisations aiming at making your solution method run as efficient as possible on the machine(s) that you have at your disposal. This is sometimes referred as code modernisation , because code that was optimised for the CPUs of two years a go may well need some revision for the latest CPU technology. These optimisations must, necessarily, take in account the specific processor architecture of your machine(s). Important topics are: Avoid pipeline stalls (due to impredictable branches, e.g. ) Ensure SIMD vectorisation. On modern processors vector registers can contain 4 double precision floating point numbers or 8 single precision numbers and vector instructions operate on these in the same number of cycles as scalar instructions. Failing to vectorise can reduce the speed of your program by a factor up to 8x! Smart data access patterns are indispensable for programs with a memory footprint that exceeds the size of the cache. Transferring data from main memory (DRAM) to the processor's registers is slow: typical latencies are in the order of 100 cycles (which potentially wastes ~800 single precision vectorised operations). Vector instructions are of no help if the processing unit must wait for the data. This is clearly much more technical and complicated (in the sense that it requires knowledge from outside the scientific domain of the problem you are trying to solve). Especially fixing memory access patterns can be difficult and a lot ofwork, as you may have to change the data structures used by your program, which usually also means rewriting a lot of code accessing the data. Such code optimisations can contribute significantly to the performance of a program, typically around 5-10x, but possibly more. As supercomputers are expensive research infrastructure in high demand, we cannot effort to waste resources. That being said, the lifetime of your program is also of importance. If you are developing a code that will be run a few times during your Master project or PhD, using only a hundred of node days, and to be forgotten afterwards, it is perhaps not worth to spend 3 months optimising it. Often, however, there is a way around these technicalities. If the scientific problem you are trying to solve, can be expressed in the formalism of common mathematical domains, e.g. linear algebra, Fourier analysis, ..., there is a good chance that there are good software libraries, designed with HPC in mind, that solved these problems for you. In most cases there are even bindings available for your favorite progamming language (C/C++, Fortran, Python, ...). All you have to do is translate the mathematical formulation of your problem into library calls. Tip Use HPC libraries as much as possible . There is little chance that you will outperform them. Quite to the contrary: your own code will probably do significantly worse. By using HPC libraries you gain three times: you gain performance, you gain development time as you will need a lot less code to solve your problem, less debugging, simpler maintenance, ... your learn how to use the library which will get you at speed readily when you take on your next scientific problem. Tip Don't reinvent the wheel . The wheel was invented ~8000 years ago. Many very clever people have put effort in it and is pretty perfect by now. Reinventing it will unlikely result in an improvement. By extension: if you need some code, spend some time google-ing around to learn what is already available and how other researchers attack the problem. It can save you weeks of programming and debugging. Adapting someone else's code to your needs will learn you more than coding it from scratch. You'll discover other approaches to coding problems than yours, other language constructs, idioms, dependencies to build on, learn to read someone else's code, learn to integrate pieces.","title":"Code optimisations"},{"location":"chapter-3/#when-is-code-optimized-enough","text":"Tip Premature optimization is the root of all evil Donald Knuth . This quote by a famous computer scientist in 1974 is often used to argue that you should only optimize if there is a real need. If code is too slow, measurements ( profiling ) should tell in which part of the code most time is spent. That part needs optimision. Iterate this a few times. Blind optimisation leads to useless and developer time wasting micro-optimisations rendering the code hard to read and maintain. On the other hand, if we are writing code for a supercomputer, it better be super-efficient. But even then, depending on the lifetime of the program we are writing, there is a point at which the efforts spent optimising are outweighed by having to wait for the program going in production. How can one judge wether a code needs further optimization or not? Obviously, there are no tricks for exposing opportunities for common sense optimisations, nor for knowing wether better algorithms exist. That is domain knowledgs, it comes with experience, and requires a lot of background. But for a given code and given input, can we know wether improvements are possible? In Chapter 1 we mentioned the existence of machine limits, the peak performance, P_p , the maximum number of floating point operstions that can be executed per second, and the bandwidth, B , the maximum number of bytes that can be moved between main memory and the CPU's registers per second. It is instructive to study how these machine limits govern the maximum performance P_{max} as a function of the computational intensity I_c . If the CPU must not wait for data, P_{max} = P_p . In the situation where the bandwidth is limiting the computation P_{max} = BI_c . This leads to the formula: P_{max} = min(P_p,BI_c) This is called the roofling model, since its graph looks like a roofliine. We can measure the actual performance and computational intensity of the program and plot it on the graph. The point must necessarily be under the roofline. For a micro-benchmark, such as a loop with a simple body, we can compute I_c by hand, count the Flops and time the benchmark to obtain the computational intensity. For an entire program a performance analysis tool can construct the graph and measure where the program is in the graph. Let's discuss 4 different cases, corresponding to the four numbered spots in the graph above. Point 1 lies in the bandwidth limited region, but well below the roofline. Something prevents the program to go at the maximum performance. There can be many causes: bad memory access causing cache misses, the code may fail to vectorize, pipeline stalls, ... for a micro-benchmark you can perhaps spot the cause without help. For a larger program a performance analyzer will highlight the problems. Point 2 lies in the peak performance limited region, also well below the roofline. Hence, cache misses are unlikely the cause. Point 3 lies close to the roofline and the boundary between the bandwidth limited region and the peak performance limited region. This the sweet spot. Both peak performance and bandwith are fully used. Point 4 lies close to the roofline in the peak performance limited region. This is an energy-efficient computation at peak performance. It moves little data (high I_c ). Moving data is by far the most energy consuming part in a computation.","title":"When is code optimized enough?"},{"location":"chapter-3/#common-approaches-towards-parallelization","text":"Before we discuss common parallelization approaches, we need to explain some concepts: process (wikipedia) : \"In computing, a process is the instance of a computer program that is being executed by one or more threads.\" A process has its own address space , the region of main memory that can be addressed by the process. Normally, a process cannot go outside its address space, nor can any other process go inside the process's own address space. An exception is when both processes agree to communicate, which is the basis of distributed memory parallelism (see below). In general, a process is restricted to a single node, and the maximum number of parallel threads is equal to the number of cores on that node. thread (wikipedia) : \"In computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed ...\". The instructions in a thread are thus by definition sequential. In the context of parallel computing, parallel threads are managed by the parent process to run different tasks in parallel. Obviously, parallel threads need to run on distinct cores to be truely concurrent. As threads belong to a process, they can in principle have access to the entire address space of the process. Sometimes a distinction is made between hardware threads and software threads. A software thread is a set of sequential instructions for a computational task that is scheduled by the program to execute. When its execution starts, it is assigned to a core. software threads can be interrupted, in order to let the core do other, more urgent work, e.g. , and restarted. There can be many more software threads in a program than it has cores available, but, obviously, they cannot all run in parallel. Software threads arre very useful in personal computers with many interactive applications opened simultaneously, where are many non-urgent tasks. For HPC applications they are a bit heavy weight. A hardware thread is a lightweight software thread that is exclusively tied to a core. When given work, it can start immediately and runs to completion without interruption. That is certainly useful in HPC where not loosing compute cycles is more important than flexibility. Now that we understand the concepts of processes and threads, we can explain three different types of parallelization:","title":"Common approaches towards parallelization"},{"location":"chapter-3/#shared-memory-parallelization","text":"In shared memory parallelization there is one process managing a number of threads to do work in parallel. As all the threads belong to the same process, they have access to the entire memory address space, that is, they share memory. To avoid problems, as well as for performance reasons, the variables inside a thread are private by default, i.e. they can only be accessed by the thread itself, and must be declare shared if other threads should have access too. Cooperating threads must exchange information by reading and writing to shared variables. The fact that all the threads belong to the same process, implies that shared memory programs are limited to a single node, because a process cannot span several nodes. However, there exist shared memory machines larger than a typical supercomputer node, e.g. SuperDome at KU leuven. Such systems allow to run a shared memory program with much more threads and much more memory. This approach can be useful when distributed memory parallelization is not feasible for some reason. The most common framework for shared memory parallelization is OpenMP . OpenMP parallelization of a sequential program is relatively simple, requiring little changes to the source code in the form of directives. A good starting point for OpenMP parallelization is this video . An important limitation of OpenMP is that it is only available in C/C++/Fortran, and not Python. Fortunately, Python has other options, e.g. multiprocessing , concurrent.futures and dask . In addition it is possible to build your own Python modules from C++ or Fortran code, in which OpenMP is used to parallelize some tasks. The popular numpy is a good example.","title":"Shared memory parallelization"},{"location":"chapter-3/#pgas","text":"PGAS , or Partitioned Global Address Space , is a parallel programming paradigm that provide communication operations involving a global memory address space on top of an otherwise distributed memory system.","title":"PGAS"},{"location":"chapter-3/#distributed-memory-parallelization","text":"Distributed memory parallelization is the opposite of shared memory parallelization. There are many process, each with only a single-thread. Every process has its own memory address space. These address spaces are not shared, they are distributed. Therefor, explicity communication is necessary to exchange information. For processes on the same machine (=node) this communication is intra-node, but for processes on distinct machines messages are sent over the interconnect. Distributed memory programs are considerably more complex to write, as the communication must be explicitly handled by the programmer, but may use as many processes as you want. Transformation of a sequential program into a distributed memory program is often a big programming effort. The most common framework is MPI . MPI is available in C/C++/Fortran and also in Python by the mpi4py module.","title":"Distributed memory parallelization"},{"location":"chapter-3/#hybrid-memory-parallelization","text":"Hybrid memory parallelization combines both approaches. It has an unlimited number of processes, and a number of threads per process, which run in parallel in a shared memory approach (OpenMP). The process communicate with each other using MPI. Typically, the computation is organised as one proces per NUMA domain and one thread per core in dat NUMA domain. This approach uses shared memory parallelization where it is useful (on a NUMA domain), but removes the limitation to a single machine. It has less processes, and thus less overhead in terms of memory footprint, and communication overhead. It is also a bit more complex that pure distributed memory parallelization, and much more complex than shared memory parallelization. Hybrid memoryy parallelization is usually implemented with OpenMP at the shared memory level and MPI at the distributed level.","title":"Hybrid memory parallelization"},{"location":"chapter-4/","text":"chapter 4 - Case studies Monte Carlo ground state energy calculation of a small atom cluster Introduction The code for this benchmark was Kindly provided by Jesus Eduardo Galvan Moya, former PhD student of the Physics Department, Condensed Matter Theory. It is a small molecular dynamics code which happens to serve many didactical purposes. It is simple code, not too big. Full of issues you should learn to pay attention to ;-) The goal of the program is to calculate the ground state energy of a small atomistic system of 10-150 atoms. The system is at 0K, so there are no velocities, and the total energy of the system consist of the interaction energy only. Interactions are described by a pair-wise interaction potential, without cutoff radius (brute force). A Monte Carlo approach is used to find the configuration with the lowest energy, 1000 separate runs with different initial configuration are run. Each run comprises 200 000 random atom moves. Finally, the run with the lowest energy is kept and subjected to Quasi-Newton iteration in order to find a local energy minimum. Implementation Here is how this algorithm goes (C++ pseudo code): n_atoms = 50; // (for example) std::vector<double> x, y, z, xmin, ymin, zmin; double Emin = std::numeric_limits<double>::max(); for(int ic=0; ic<1000; ++ic) {// loop over initial configurations // generate initial configuration initialize(x,y,z); for(int ip=0; ip<200000; ++ip) {// loop over random perturbations // perturb the current configuration x += small_perturbation(); y += small_perturbation(); z += small_perturbation(); E = 0; // double loop over all interactions for(int i=0; i<n_atoms; ++i) for(int j=0; j<i; ++j) { double rij = std::sqrt((x[j]-x[j])^2 + (y[j]-y[j])^2 + (z[j]-z[j])^2); E += V(rij); } } } if( E < Emin ) {// remember the current (perturbed) configuration xmin = x; ymin = y; zmin = z; Emin = E } } // Perform a Newton-Raphsom iteration on E(x,y,z) with x0 = xmin, y0 = ymin, z = zmin. ... The memory footprint of this problem is ( n_atoms x 3) doubles x 8 bytes/ double . For n_atoms = 150 , that is 3600 bytes, which is far less than the size of L1 cache (32KB). Hence, the problem fits easily in the L1 cache. As soonas the entire problem is loaded in the cache, the code will run without needing to wait for data. Furthermore, the interaction potential V(r) = A \\frac{exp{({\\alpha}r)}}{r^n} - B \\frac{exp{(-\\beta(r-c_{att}))}} {(r-c_{att})^{n_{att}} + d_{att}} - \\frac{C}{r} is rather compute intensive, as it uses several expensive operations: two exponentials and two divisions, plus the square root for the distance which here cannot be avoided: r = r_{ij}(r_i,r_j) = \\sqrt{(x_j-x_i)^2 + (y_j-y_i)^2 + (z_j-z_i)^2 } Consequently, the code is certainly compute bound. Optimisation Most of the work is carried out in the inner double loop over the interactions. Let's see if we can optimise this. Initially, both expressions for the interatomic distance r_{ij}(r_i,r_j) and the interaction potential V(r) were implemented as functions called in the double loop. The first timing for the double loop with 50 atoms is 144 \\mu s. By checking the vectorisation report of the compiler, we learned that the two function calls prohibited vectorisation. After inlining the functions, the timing was reduced to 93 \\mu s. The inner loop contains a lot of short loops. This is bad for pipelining and vectorisation (many loops end with incompletely filled vector registers.) If we split the loop in a double loop for calculating the interatomic distances and storing them in a long array, and a long loop over that array to compute the interactions, the situation might improve. E = 0; int n_interactions = n_atoms*(n_atoms-1)/2; std::vector<double> rij(n_interactions); // (in C++ std::vector is actually a contiguous array) // double loop over all interactions for(int i=0; i<n_atoms; ++i) for(int j=0; j<i; ++j) rij = std::sqrt((x[j] - x[j])^2 + (y[j] - y[j])^2 + (z[j] - z[j])^2); } for(int ij=0; ij<n_interactions; ++ij) E += V(rij[ij]); This reduces the time from 93 to 86 \\mu s. Not much, but since we must runs this loop 1 000 x 200 000 times it nevertheless represents a substantial gain. Note We implemented this both in C++ and Fortran. The results were almost identical. You sometimes hear that C++ is an inefficient programming language and that the opposite holds for Fortran. This is not true. Both C++ and Fortran compilers are capable to build optimally performant progams for the CPU at hand. We'll come to this subject later . At this point, we seem to be done optimising the inner loops. Maybe there is something we can do to the surrounding loops? The perturbation loop adds a small perturbation to every coordinate of every atom in the list to see if the perturbation results in a lower energy. The perturbation involves 3n_{atoms} random numbers and generation random numbers is also rather expensive. We might wonder if it is really necessary to perturb all atoms. What if we perturbed only one atom? That reduces the number of random number generations by a factor n_{atoms} . In addition, most of the interactions remain the same, only the n_{atoms}-1 interactions with the perturbed atom change. Hence our program now has a complexity O(N) . In the original formulation the number of interaction to be computed was n_{atoms}(n_{atoms}-1)/2 = O(N^2) . As the program is compute bound changing the computational complexity from O(N^2) to O(N) will have a big impact. This optimisation falls under the common sense optimisations . It is important to realize that this optimisation changes the nature of the algorithm. It remains to be seen whether 200 000 configurations is still sufficient to find the minimum. We might need more, or maybe less. This up to the researcher to investigate. Let's see how we can implement this modification and how that effects the performance. We start with depicting the relation between $r_{ij}) as a (lower triangular) matrix and as the linear rij array in the split loop above. The linear array stores the rows of the lower triangular matrix: [r_{10} , r_{20} , r_{21} , r_{30} , r_{31} , r_{32}, r_{40} , r_{41} , r_{42} , r_{43} , ... ] . The matrix elements show the value or the index into the linear array. Let's do something similar for the interaction energy: We have added a column to compute the row sums and the total sum of the interaction energies E_{ij} . Let's now visualize the changes when an atom, say atom 4, is perturbed. The items changing due to perturbing r_4 are marked in orange. The row sum for row 4 has to be computed from scratch and in row 5 and 6 the elements corresponding to column 4 change as well. The next figure shows how the perturbed result can be computed from the previous result by first subtracting the previous result and then adding the new result. Here is a comparison of the timings: N O(N^2) O(N) speedup 50 86 \\mu s 5.7 15.1 150 (x3) 747 \\mu s (x9) 17.3 \\mu s (x3) 43.3 500 (x10) 8616 \\mu s (x100) 57.0 \\mu s (x10) 115.2 Clearly, the timings for the O(N^2) algorithm increase quadratically, while those for the O(N) algorithm increase only linearly and the speedups are substantial. The O(N) algorithm for 500 atoms - a number that our researcher considered unattainable because it would take too long to compute - is still faster than the O(N) algorithm. Tip Look for algorithms of low computational complexity. However, The best algorithme may also depend on the problem as we saw in Selecting algorithms based on computational complexity . Despite the considerable performance improvement, there are a few disadvantages to it too. The O(N) algorithm has more code, is more difficult to understand and thus harder to maintain. Morover, its loops are more complex, making it harder for the compiler to optimize. Autovectorisation doesn't work. If it needs further optimization, it is certainly no low-hanging fruit. Parallelization If the time of solution for this sofar sequential program is still too large, we might opt for parallelization. The interaction loop is now doing relatively little work, and hard to parallelize. On the other hand the perturbation loop can be easily distributed over more threads as this loop is embarrassingly parallel . As long as every thread generates a different series of random numbers they can run their share of the perturbation iterations completely independent. This is very easy to achieve with OpenMP. In the end every thread would have its own minimum energy configuration, and the overall minimum energy configuration is simply found as the minimum of per thread minima. Since every core has its own L1 cache, the problem for each thread also fits in L1. Project mcgse The wetppr/mcgse folder repeats this case study for the Morse potential (I lost the original code :-( ) V(r) = D_e(1 - e^{-\\alpha(r-r_e)})^2 We will assume that all parameters are unity. V(r) = (1 - e^{1-r)})^2 Here is its graph: Using our research software devolopment strategy , we start in Python, implement both algorithms and test. A good test is the case of a cluster of 4 atoms. Energy minimum then consists of a tetrahedron with unit sides. Every pair is then at equilibrium distance and E_{min}=0 . The vertices of the tetrahedron are on a sphere of radius \\sqrt{3/8} . Let us randomly distribute 4 points on a sphere of radius \\sqrt{3/8} and see how well close we get to E_{min}=0 . import numpy as np import mcgse # our module for this project: wetppr/mcgse sample = mcgse.sample_unit_sphere(4) * np.sqrt(3/8) config = (sample[0], sample[1], sample[2]) # initial coordinates of the atoms (x,y,z) dist = mcgse.LogNormal(mean=-5, sigma=.4) # distribution to draw the length of the displacements from # the distribution and its parameters were selected using quite some trial and error to obtain # useful results... Emin_ON2, *config_min_ON2 = mcgse.execute_perturbation_loop(config=config, n_iterations=20000, dist=dist, algo='ON2', verbosity=1) Emin_ON , *config_min_ON = mcgse.execute_perturbation_loop(config=config, n_iterations=20000, dist=dist, algo='ON' , verbosity=1) Here are the results for 5 runs: ON2 iteration 0: Emin=1.8642580817361518 ON2 iteration 200000: Emin=0.343375960680797, last improvement: iteration = 2044 ON iteration 0: Emin=1.8642580817361518 ON iteration 200000: Emin=0.1318184548419835, last improvement: iteration = 30162 ON2 iteration 0: Emin=1.0114013021541974 ON2 iteration 200000: Emin=0.368488427516059, last improvement: iteration = 32701 ON iteration 0: Emin=1.0114013021541974 ON iteration 200000: Emin=0.058861153165589014, last improvement: iteration = 5168 ON2 iteration 0: Emin=3.69912617914294 ON2 iteration 200000: Emin=0.3819530373342961, last improvement: iteration = 4580 ON iteration 0: Emin=3.69912617914294 ON iteration 200000: Emin=0.3297933435887894, last improvement: iteration = 65216 ON2 iteration 0: Emin=3.299140128625619 ON2 iteration 200000: Emin=0.5323556068840862, last improvement: iteration = 12505 ON iteration 0: Emin=3.299140128625619 ON iteration 200000: Emin=0.5270227273967558, last improvement: iteration = 16929 ON2 iteration 0: Emin=1.2894488159651718 ON2 iteration 200000: Emin=0.40188231571036437, last improvement: iteration = 2621 ON iteration 0: Emin=1.2894488159651718 ON iteration 200000: Emin=0.07936811573814093, last improvement: iteration = 25806 We can draw some interesting observations from these runs: Neither of the algorithms seem to get close to the minimum, In terms of closeness to the minimum there no clear winner, although ON got rather close twice, The higher the initial energy, the worse the solution, which is acceptable, as the average displacement magnitude is fixed. None of the algorithms seems to converge. In the first and the last run ON2 found its best guess at 2044 and 2621 iterations. None of the approximately 198_000 later attempts could reduce the energy. This seems to be the case for ON as well, although the numbers are a bit higher. Despite being far from the minimum, improvements seem to involve progressively more work. Especially the last conclusion is rather worrying. Our algorithms don't seem to sample the configuration space very efficiently. Perhaps, rather than displacing the atoms randomly, it might be more efficient to move them in the direction of the steepest descent of the energy surface. Since we have an analytical expression, we can compute it. The interaction V(r_{ij}) exerts a force {\\mathbf{F}}_k = -\\nabla_{\\mathbf{r}_k}E = -\\nabla_{\\mathbf{r}_k} \\sum_{i<j}V(r_{ij}) = -\\sum_{i<j} \\nabla_{\\mathbf{r}_k}V(r_{ij}) = -\\sum_{i<j} \\frac{d}{d_{r_{ij}}}V(r_{ij})\\nabla_{\\mathbf{r}_k}r_{ij} = -\\sum_{i<j} V'(r_{ij})\\nabla_{\\mathbf{r}_k}r_{ij} Here, \\nabla_{\\mathbf{r}_k}r_{ij} = 0 \\text{ if } k \\ne i,j and \\nabla_{\\mathbf{r}_k}r_{kj} = -\\frac{\\mathbf{r}_{kj}}{r_{kj}} = -{\\hat{\\mathbf{r}}}_{kj} \\nabla_{\\mathbf{r}_k}r_{jk} = \\frac{\\mathbf{r}_{jk}}{r_{jk}} = {\\hat{\\mathbf{r}}}_{jk} Thus, Hence: \\mathbf{F}_k = \\sum_{j\\ne{k}} V'(r_{kj}){\\hat{\\mathbf{r}}}_{kj} = -\\sum_{j<k} V'(r_{jk}){\\hat{\\mathbf{r}}}_{jk} + \\sum_{k<j} V'(r_{kj}){\\hat{\\mathbf{r}}}_{kj} Finally (setting all parameters to unity), V'(r) = -2(1-e^{1-r})e^{1-r} Now that we have the forces on the atoms in the current configuration, we should be able to move the atoms in the directon of the force, rather than in a random direction, as before. In fact we have a true minimisation problem now. to be continued... Study of data access patterns in a large Lennard-Jones systems Introduction In this case study we consider a large system of atoms whose interaction is described by a Lennard-Jones potential. By large we mean a system that does not fit in the cache. Consequently, the effect of caches will be noticabel in the results. We will consider two different settings. A Monte Carlo setting, as above, in which the interaction energy is computed as a sum of pairwise interactions. It is of little physical significance, but is useful to demonstrate the effect of the caches on the computations. The second setting is a true molecular dynamics setting in which the time evolution of a collection of atoms is computed by time integration of the interaction forces which are computed as the gradient of the interaction potential. This gives rise to time dependent accelerations, velocities and positions of the atoms. Monte Carlo setting The interaction energy is given by: E=\\sum_{i<j}V(r_{ij}) Since our sytem is large, say billions of atoms, computing this sum considering all pairs, is computationally unfeasible because it has O(N^2) computational complexity. We will discuss approaches to reduce the computational complexity to O(N) . To study the effect of the cache we will compute the partial sum E_i=\\sum_{j\\ne{i}}V(r_{ij}) for i=0 , that is E_0=\\sum_{j=1}^{N}V(r_{0j}) Because our system is translationally invariant, we can put atom 0 at the origin, in which case r_{0j}=r_j . Thus, we end up with: E_0=\\sum_{j=1}^{N}V(r_{j}) We will use the best implementation for the Lennard-Jones potential that we discussed in The cost of floating point instructions , expressed as a function of r^2 , as to avoid the square root needed to compute r . We consider three different cases: A contiguous loop over arrays x[1:N] , y[1:N] , z[1:N] . This is a structure of arrays (SoA) approach. A contiguous loop over a single array xyz[1:3N , in which the x , y and z coordinates of the i -th atom come after each other followed by the x , y and z coordinates of the i+1 -th atom. This is a array of structures approach (AoS). A contiguous loop over arrays x[1:N] , y[1:N] , z[1:N] in which the atoms are picked by random permutation of 1..N . So, all atoms are visited, but in a random order. For each case E_0=\\sum_{j=1}^{N}V(r_{j}) is computed for N \\in \\{2^9,2^10,2^11,...,2^{29}\\} repeating the loop over j 2^{29}/N times. In this way the amount of interaction potential evaluations is exactly 2^{29} irrespective of the length of the array, and the timings can be compared. The smallest arrays fit in L1, while the longest arrays ( 2^29\\approx0.5\\times10^9 ) do not even fit in L3. Here are the timings: It is clearly visible that the behaviour of the random case above is very different from the two contiguous cases. For the longest arrays, the performance is a whopping 15x worse on the random loop, yet every case performs exactly the same work. There burning question is of course: \"what is causing the performance breakdown of the randomized loop\"? The second question, certainly less burning, but nevertheless important, is: \"is the lowest curve (the AoS case) the best we can get?\". If you are really curious, you might wonder about the small difference between the AoS case and the SoA case at larger N . To help your understanding of the problem, here is a different representation of the same graph, this time the number of bytes used by the arrays on the x-axis instead of the array size N . With this x-axis it is easy to draw the boundaries of the L1, L2 and L3 caches. Surprisingly enough, the changes in the curves coincide with the cache boundaries. As soon as the problem is too large for a cache, cache misses cause pipeline stalls, and the CPU has to wait for the data needed. The latency increases at every cache boundary and the slowdown becomes more pronounced each time. This also explains the slight advantage for the AoS case over the SoA case for problems not fitting in L3. As x, y, and z follow contiguously in memory in the AoS case, when it needs new data from memory, it has to wait for only a single cache line, while the SoA needs three. If you have difficulties to grasp, revisit the talk by Scott Meyers CPU Caches and Why You Care . The second question is a bit harder to answer. Let us analyze the performance of the (Fortran) loop: ! Contiguous access, SoA: p=[xxx\u2026yyy\u2026zzz\u2026] do ik=1,k do im=1,m ! FLOPS r2 = (p(im)-x0)**2 ! +(p(m+im)-y0)**2 ! +(p(2*m+im)-z0)**2 ! 3-, 2+, 3* ! r = lj_pot2(r) ! r2i = 1.0d0/r2 ! 1/ rr6i = r2i*r2i*r2i; ! 2* V0j = 4.0d0*rr6*(rr6-1.0d0); ! 2*, 1- enddo !------------ enddo ! 14 flops The loop has 14 floating point operations. It is executed 2^29 times in 1.2s. That makes 6.26\\times 10^9 flops/s. The peak performance of the machine is 1 core x 1 instruction per cycle x 4 SIMD registers per instruction x 2.8 GHz = 11.2 Gcycles/s = 11.2 Gflops/s. Consequently, we are running at 56% of the peak performance. So it looks as if we could still do better. Let us analyze the data traffic of the same loop: ! Contiguous access, SoA: p=[xxx\u2026yyy\u2026zzz\u2026] do ik=1,k do im=1,m ! FLOPS ! DATA r2 = (p(im)-x0)**2 ! ! +(p(m+im)-y0)**2 ! ! +(p(2*m+im)-z0)**2 ! 3-, 2+, 3* ! 3DP ! r = lj_pot2(r) ! ! r2i = 1.0d0/r2 ! 1/ ! rr6i = r2i*r2i*r2i; ! 2* ! V0j = 4.0d0*rr6*(rr6-1.0d0); ! 2*, 1- ! enddo !---------------!----- enddo ! 14 flops ! 24B The loop reads 24 bytes x 2^29 iterations in 1.2 s. That makes 10.7 GB/s. The bandwidth of the machine is 109 GB/s for 10 cores, that is 10.9 GB for 1 core. Our loop runs at the maximum bandwidth. It is bandwith saturated . This is a machine limit. It can simply not feed the CPU with data faster than this. It is instructive to draw a roofline model for this. The above loop, that is the contiguous cases, plot on the bandwidth part of the roofline indicating that the machine limit (bandwidth) is reached, the random case sits close to the bottom far away from all machine limits. The conclusion is that the loop as it is runs at its maximum speed, being bandwidth limited. However, 44% of the time the CPU is not doing useful work, because it is waiting for data. That means that if we replaced the Lennard-Jones potential with another one that is about twice as compute intensive, and for that reason more accurate, we would still finish the computation in 1.2s and have a more accurate solution, because we are using the cycles that the CPU was waiting for data to do the extra computations. Molecular Dynamics setting We consider the same system, a large collection of atoms interacting through a Lennard-Jones potential. In a Molecular Dynamics setting the time evolution of th system is computed by time integration of the classical equation of motion: \\dot{\\mathbf{r}} = \\mathbf{v} \\dot{\\mathbf{v}} = \\mathbf{a} \\mathbf{a} = \\mathbf{F} The forces are computed as the gradient of the interaction energy: \\mathbf{F}_i = \\nabla_{\\mathbf{r}_i}{E} = \\nabla_{\\mathbf{r}_i} \\sum_{j\\ne{i}}^{N}V(r_{ij}) We assume a system size of N=10^9 atoms. The number of terms in the sum above is then 10^9(10^9-1)/2\\approx{10^ {18}} . That will keep us busy, won't it... However, when you start evaluating all these contributions, you very soon realize that most of them are really small, so small that they don't actually contribute to the result. They are short-ranged . Mathematically, a force is short-ranged if it decays faster than r^{-2} . This is because the area of a sphere with radius r is 4\\pi r^2 and hence the number of particles at distance grows as r^2 . Consequently, in order for the force exerted by those particle to be negligible it has to decay faster than r^{-2} . The derivative of the Lennard-Jones potential is: V'(r) = ({-6}/{r}) r^{-6}(2r^{-6}-1) Hence, \\mathbf{F}_i = \\sum_{j\\ne{i}}^{N}\\nabla_{\\mathbf{r}_i}V(r_{ij}) = \\sum_{j\\ne{i}}^{N}V'(r_{ij})\\nabla_{\\mathbf{r} _i}r_{ij} = \\sum_{j\\ne{i}}^{N}V'(r_{ij}) \\hat{\\mathbf{r}}_{ij} = \\sum_{j\\ne{i}}^{N} ({-6}/{r_{ij}}) r_{ij}^{-6}(2r_{ij}^{-6}-1) \\frac{\\mathbf{r}_{ij}}{r_{ij}} = \\sum_{j\\ne{i}}^ {N} -6 r_{ij}^{-8}(2r_{ij}^{-6}-1) \\mathbf{r}_{ij} Note that the force factor f , that is the factor in front of \\mathbf{r}_ij , can also be expressed in terms of s=r^2=\\delta{x}^2+\\delta{y}^2+\\delta{x}^2 : f(s) = -6 s^{-4}(2s^{-3}-1) \\mathbf{F}_i = \\sum_{j\\ne{i}}^{N} f(s_{ij}) \\mathbf{r}_{ij} So, we can avoid the square root in computing r_ij . Clearly, we can compute the interaction energy and the interaction force in one go with little extra effort: V(s) = s^{-3}(s^{-3}-1) E = \\sum_{i<j} V(s_{ij}) The fact that the interaction force is short-ranged, allows us to neglect the interaction forces beyond a cutoff distance r_c , thus offering a possibility to avoid the cost of an O(N^2) algorithm. Implementing cutoff As a first step we we can avoid the computation of the interaction energy and the interaction force if r_{ij}>r_c , or s_{ij}>s_c : # (python psseudo-code) for i in range(N): for j in range(i): x_ij = x[j]-x[i] y_ij = y[j]-y[i] z_ij = z[j]-z[i] s_ij = x_ij*x_ij + y_ij*y_ij + z_ij*z_ij if s_ij <= s_c: t = 1/s_ij t3 = t*t*t E += t3*(t3-1) f = -6*t*t3*(2*t3-1) Fx[i] += f*x_ij Fy[i] += f*y_ij Fz[i] += f*z_ij Fx[j] -= f*x_ij Fy[j] -= f*y_ij Fz[j] -= f*z_ij Although this loop only computes the interactions wheen s_{ij}\\le{s_c} , it still visits every pair to compute s_{ij} . The corresponding amount of work is still O(N^2) . Some improvement is possible by using Verlet lists. The Verlet list of an atom i is the set of atoms j for which r_{ij}<r_v , where r_v is typically a bit larger than r_c . The loop is now witten as: # (python psseudo-code) for i in range(N): for j in verlet_list(i): # as above The loop over j is now much shorter, its length is bounded, typically in the range 10..100 . Hence, the double loop is effectively O(N) . The construction of the Verlet list, however, is still O(N^2) , but the cost of it is amortised over a number of timesteps. Because atoms move only a little bit over a time step and r_v>r_c , the Verlet list can indeed be reused a number of timesteps, before it needs to be updated. Algorithms for constructing the Verlet list with O(N) complexity do exist. Here's a 2-D version of cell-based Verlet list construction . It can be easily extended to 3-D, but that is harder to visualise. In the left figure below, atom i (the orange dot) is surrounded by a blue circle of radius r_v . Atoms inside the blue circle are in the Verlet list of atom i . We now overlay the domain with a square grid, of grid size r_v (middle figure). Atom pairs in the same cell or in nearest neighbour cells are Verlet list candidates, but not pairs in second nearest neighbours or further. To construct the Verlet list of atom i , we only have to test atoms in the same cell, or in its 8 nearest neighbours, all coloured light-blue. By iterating over all cells and over the atoms it contains, the Verlet lists of all atoms can be constructed with O(N) complexity. In fact, by looking for pairs in all rearest neighbours, all candidate pairs are visited twice ( ij and ji ). Hence, only half of the nearest neighbours needs to be visited (right figure). The algorithm requires that the grid implements: a cell list: a list of all the atoms that are in the cell, in order to iterate over all atoms in a cell. The cell lists can be constructed with O(N) complexity, and a method to find the neighbour cells of a cell. This is a good example for demonstrating the effectiveness of our strategy for research software development . Here are the steps you should take Start out in Python. Take a small system, e.g. N=5 , use Numpy arrays for the positions, velocities, ... Implement brute force computation of interactions and interaction forces ( O(N^2) ). Implement brute force computation of interactions and interaction forces with cutoff ( O(N^2) ). Implement brute force construction of Verlet lists ( O(N^2) ). (You might need a larger system for testing this). Implement Verlet list computation of interactions and interaction forces ( O(N) ). Implement cell-based Verlet list construction ( O(N) ). (You might need a larger system for testing this). Optimise, try using Numba, or by taking the compute intensive parts to C++. Of course test and validate every step, e.g. by comparing to previous steps. Tip Remember that, for performance , you should avoid using loops in Python . When I implemented the cell-based Verlet list construction in Python, it turned out to be terribly slow, mainly because of 5 levels of nesting Python loops. The C++ version turned out to be 1200x faster (twelve hundred indeed, no typo!). Moving atoms The initalization of a physically consistent system of atoms is a non-trivial task in itself. Because molecular motion conserves energy, random positions and velocities at time t=0 may pos a lot of trouble for time integration. When two atoms happen to be very close they experience very high repulsive force and thus are accelerated vigorously. This can easily make the simulation explode. A practical way is to put atoms on a lattice with interatomic distances close to the equilibrium distance of the Lennard-Jones potential, e.g. primitive cubic, body-centred cubic (BCC), face-centred cubic (FCC), hexagonal closest packing (HCP). then slowly increase random velocities to increase the kinetice energy and hence the temperature. When initializing the system on a lattice, often the performance is rather good because the regular arrangement allows for a good data access pattern. However, as (simulation) time proceeds the atoms move and diffusion kicks in. Every timestep, some atoms will move in and out of some other atom's Verlet sphere. Gradually, the atoms will move further and further from their original positions, but their location in memory does not change, and, consequentially, the data access pattern appproaches the random array access we discussed above, leading to considerable performance degradation.","title":"chapter 4 - Case studies"},{"location":"chapter-4/#chapter-4-case-studies","text":"","title":"chapter 4 - Case studies"},{"location":"chapter-4/#monte-carlo-ground-state-energy-calculation-of-a-small-atom-cluster","text":"","title":"Monte Carlo ground state energy calculation of a small atom cluster"},{"location":"chapter-4/#introduction","text":"The code for this benchmark was Kindly provided by Jesus Eduardo Galvan Moya, former PhD student of the Physics Department, Condensed Matter Theory. It is a small molecular dynamics code which happens to serve many didactical purposes. It is simple code, not too big. Full of issues you should learn to pay attention to ;-) The goal of the program is to calculate the ground state energy of a small atomistic system of 10-150 atoms. The system is at 0K, so there are no velocities, and the total energy of the system consist of the interaction energy only. Interactions are described by a pair-wise interaction potential, without cutoff radius (brute force). A Monte Carlo approach is used to find the configuration with the lowest energy, 1000 separate runs with different initial configuration are run. Each run comprises 200 000 random atom moves. Finally, the run with the lowest energy is kept and subjected to Quasi-Newton iteration in order to find a local energy minimum.","title":"Introduction"},{"location":"chapter-4/#implementation","text":"Here is how this algorithm goes (C++ pseudo code): n_atoms = 50; // (for example) std::vector<double> x, y, z, xmin, ymin, zmin; double Emin = std::numeric_limits<double>::max(); for(int ic=0; ic<1000; ++ic) {// loop over initial configurations // generate initial configuration initialize(x,y,z); for(int ip=0; ip<200000; ++ip) {// loop over random perturbations // perturb the current configuration x += small_perturbation(); y += small_perturbation(); z += small_perturbation(); E = 0; // double loop over all interactions for(int i=0; i<n_atoms; ++i) for(int j=0; j<i; ++j) { double rij = std::sqrt((x[j]-x[j])^2 + (y[j]-y[j])^2 + (z[j]-z[j])^2); E += V(rij); } } } if( E < Emin ) {// remember the current (perturbed) configuration xmin = x; ymin = y; zmin = z; Emin = E } } // Perform a Newton-Raphsom iteration on E(x,y,z) with x0 = xmin, y0 = ymin, z = zmin. ... The memory footprint of this problem is ( n_atoms x 3) doubles x 8 bytes/ double . For n_atoms = 150 , that is 3600 bytes, which is far less than the size of L1 cache (32KB). Hence, the problem fits easily in the L1 cache. As soonas the entire problem is loaded in the cache, the code will run without needing to wait for data. Furthermore, the interaction potential V(r) = A \\frac{exp{({\\alpha}r)}}{r^n} - B \\frac{exp{(-\\beta(r-c_{att}))}} {(r-c_{att})^{n_{att}} + d_{att}} - \\frac{C}{r} is rather compute intensive, as it uses several expensive operations: two exponentials and two divisions, plus the square root for the distance which here cannot be avoided: r = r_{ij}(r_i,r_j) = \\sqrt{(x_j-x_i)^2 + (y_j-y_i)^2 + (z_j-z_i)^2 } Consequently, the code is certainly compute bound.","title":"Implementation"},{"location":"chapter-4/#optimisation","text":"Most of the work is carried out in the inner double loop over the interactions. Let's see if we can optimise this. Initially, both expressions for the interatomic distance r_{ij}(r_i,r_j) and the interaction potential V(r) were implemented as functions called in the double loop. The first timing for the double loop with 50 atoms is 144 \\mu s. By checking the vectorisation report of the compiler, we learned that the two function calls prohibited vectorisation. After inlining the functions, the timing was reduced to 93 \\mu s. The inner loop contains a lot of short loops. This is bad for pipelining and vectorisation (many loops end with incompletely filled vector registers.) If we split the loop in a double loop for calculating the interatomic distances and storing them in a long array, and a long loop over that array to compute the interactions, the situation might improve. E = 0; int n_interactions = n_atoms*(n_atoms-1)/2; std::vector<double> rij(n_interactions); // (in C++ std::vector is actually a contiguous array) // double loop over all interactions for(int i=0; i<n_atoms; ++i) for(int j=0; j<i; ++j) rij = std::sqrt((x[j] - x[j])^2 + (y[j] - y[j])^2 + (z[j] - z[j])^2); } for(int ij=0; ij<n_interactions; ++ij) E += V(rij[ij]); This reduces the time from 93 to 86 \\mu s. Not much, but since we must runs this loop 1 000 x 200 000 times it nevertheless represents a substantial gain. Note We implemented this both in C++ and Fortran. The results were almost identical. You sometimes hear that C++ is an inefficient programming language and that the opposite holds for Fortran. This is not true. Both C++ and Fortran compilers are capable to build optimally performant progams for the CPU at hand. We'll come to this subject later . At this point, we seem to be done optimising the inner loops. Maybe there is something we can do to the surrounding loops? The perturbation loop adds a small perturbation to every coordinate of every atom in the list to see if the perturbation results in a lower energy. The perturbation involves 3n_{atoms} random numbers and generation random numbers is also rather expensive. We might wonder if it is really necessary to perturb all atoms. What if we perturbed only one atom? That reduces the number of random number generations by a factor n_{atoms} . In addition, most of the interactions remain the same, only the n_{atoms}-1 interactions with the perturbed atom change. Hence our program now has a complexity O(N) . In the original formulation the number of interaction to be computed was n_{atoms}(n_{atoms}-1)/2 = O(N^2) . As the program is compute bound changing the computational complexity from O(N^2) to O(N) will have a big impact. This optimisation falls under the common sense optimisations . It is important to realize that this optimisation changes the nature of the algorithm. It remains to be seen whether 200 000 configurations is still sufficient to find the minimum. We might need more, or maybe less. This up to the researcher to investigate. Let's see how we can implement this modification and how that effects the performance. We start with depicting the relation between $r_{ij}) as a (lower triangular) matrix and as the linear rij array in the split loop above. The linear array stores the rows of the lower triangular matrix: [r_{10} , r_{20} , r_{21} , r_{30} , r_{31} , r_{32}, r_{40} , r_{41} , r_{42} , r_{43} , ... ] . The matrix elements show the value or the index into the linear array. Let's do something similar for the interaction energy: We have added a column to compute the row sums and the total sum of the interaction energies E_{ij} . Let's now visualize the changes when an atom, say atom 4, is perturbed. The items changing due to perturbing r_4 are marked in orange. The row sum for row 4 has to be computed from scratch and in row 5 and 6 the elements corresponding to column 4 change as well. The next figure shows how the perturbed result can be computed from the previous result by first subtracting the previous result and then adding the new result. Here is a comparison of the timings: N O(N^2) O(N) speedup 50 86 \\mu s 5.7 15.1 150 (x3) 747 \\mu s (x9) 17.3 \\mu s (x3) 43.3 500 (x10) 8616 \\mu s (x100) 57.0 \\mu s (x10) 115.2 Clearly, the timings for the O(N^2) algorithm increase quadratically, while those for the O(N) algorithm increase only linearly and the speedups are substantial. The O(N) algorithm for 500 atoms - a number that our researcher considered unattainable because it would take too long to compute - is still faster than the O(N) algorithm. Tip Look for algorithms of low computational complexity. However, The best algorithme may also depend on the problem as we saw in Selecting algorithms based on computational complexity . Despite the considerable performance improvement, there are a few disadvantages to it too. The O(N) algorithm has more code, is more difficult to understand and thus harder to maintain. Morover, its loops are more complex, making it harder for the compiler to optimize. Autovectorisation doesn't work. If it needs further optimization, it is certainly no low-hanging fruit.","title":"Optimisation"},{"location":"chapter-4/#parallelization","text":"If the time of solution for this sofar sequential program is still too large, we might opt for parallelization. The interaction loop is now doing relatively little work, and hard to parallelize. On the other hand the perturbation loop can be easily distributed over more threads as this loop is embarrassingly parallel . As long as every thread generates a different series of random numbers they can run their share of the perturbation iterations completely independent. This is very easy to achieve with OpenMP. In the end every thread would have its own minimum energy configuration, and the overall minimum energy configuration is simply found as the minimum of per thread minima. Since every core has its own L1 cache, the problem for each thread also fits in L1.","title":"Parallelization"},{"location":"chapter-4/#project-mcgse","text":"The wetppr/mcgse folder repeats this case study for the Morse potential (I lost the original code :-( ) V(r) = D_e(1 - e^{-\\alpha(r-r_e)})^2 We will assume that all parameters are unity. V(r) = (1 - e^{1-r)})^2 Here is its graph: Using our research software devolopment strategy , we start in Python, implement both algorithms and test. A good test is the case of a cluster of 4 atoms. Energy minimum then consists of a tetrahedron with unit sides. Every pair is then at equilibrium distance and E_{min}=0 . The vertices of the tetrahedron are on a sphere of radius \\sqrt{3/8} . Let us randomly distribute 4 points on a sphere of radius \\sqrt{3/8} and see how well close we get to E_{min}=0 . import numpy as np import mcgse # our module for this project: wetppr/mcgse sample = mcgse.sample_unit_sphere(4) * np.sqrt(3/8) config = (sample[0], sample[1], sample[2]) # initial coordinates of the atoms (x,y,z) dist = mcgse.LogNormal(mean=-5, sigma=.4) # distribution to draw the length of the displacements from # the distribution and its parameters were selected using quite some trial and error to obtain # useful results... Emin_ON2, *config_min_ON2 = mcgse.execute_perturbation_loop(config=config, n_iterations=20000, dist=dist, algo='ON2', verbosity=1) Emin_ON , *config_min_ON = mcgse.execute_perturbation_loop(config=config, n_iterations=20000, dist=dist, algo='ON' , verbosity=1) Here are the results for 5 runs: ON2 iteration 0: Emin=1.8642580817361518 ON2 iteration 200000: Emin=0.343375960680797, last improvement: iteration = 2044 ON iteration 0: Emin=1.8642580817361518 ON iteration 200000: Emin=0.1318184548419835, last improvement: iteration = 30162 ON2 iteration 0: Emin=1.0114013021541974 ON2 iteration 200000: Emin=0.368488427516059, last improvement: iteration = 32701 ON iteration 0: Emin=1.0114013021541974 ON iteration 200000: Emin=0.058861153165589014, last improvement: iteration = 5168 ON2 iteration 0: Emin=3.69912617914294 ON2 iteration 200000: Emin=0.3819530373342961, last improvement: iteration = 4580 ON iteration 0: Emin=3.69912617914294 ON iteration 200000: Emin=0.3297933435887894, last improvement: iteration = 65216 ON2 iteration 0: Emin=3.299140128625619 ON2 iteration 200000: Emin=0.5323556068840862, last improvement: iteration = 12505 ON iteration 0: Emin=3.299140128625619 ON iteration 200000: Emin=0.5270227273967558, last improvement: iteration = 16929 ON2 iteration 0: Emin=1.2894488159651718 ON2 iteration 200000: Emin=0.40188231571036437, last improvement: iteration = 2621 ON iteration 0: Emin=1.2894488159651718 ON iteration 200000: Emin=0.07936811573814093, last improvement: iteration = 25806 We can draw some interesting observations from these runs: Neither of the algorithms seem to get close to the minimum, In terms of closeness to the minimum there no clear winner, although ON got rather close twice, The higher the initial energy, the worse the solution, which is acceptable, as the average displacement magnitude is fixed. None of the algorithms seems to converge. In the first and the last run ON2 found its best guess at 2044 and 2621 iterations. None of the approximately 198_000 later attempts could reduce the energy. This seems to be the case for ON as well, although the numbers are a bit higher. Despite being far from the minimum, improvements seem to involve progressively more work. Especially the last conclusion is rather worrying. Our algorithms don't seem to sample the configuration space very efficiently. Perhaps, rather than displacing the atoms randomly, it might be more efficient to move them in the direction of the steepest descent of the energy surface. Since we have an analytical expression, we can compute it. The interaction V(r_{ij}) exerts a force {\\mathbf{F}}_k = -\\nabla_{\\mathbf{r}_k}E = -\\nabla_{\\mathbf{r}_k} \\sum_{i<j}V(r_{ij}) = -\\sum_{i<j} \\nabla_{\\mathbf{r}_k}V(r_{ij}) = -\\sum_{i<j} \\frac{d}{d_{r_{ij}}}V(r_{ij})\\nabla_{\\mathbf{r}_k}r_{ij} = -\\sum_{i<j} V'(r_{ij})\\nabla_{\\mathbf{r}_k}r_{ij} Here, \\nabla_{\\mathbf{r}_k}r_{ij} = 0 \\text{ if } k \\ne i,j and \\nabla_{\\mathbf{r}_k}r_{kj} = -\\frac{\\mathbf{r}_{kj}}{r_{kj}} = -{\\hat{\\mathbf{r}}}_{kj} \\nabla_{\\mathbf{r}_k}r_{jk} = \\frac{\\mathbf{r}_{jk}}{r_{jk}} = {\\hat{\\mathbf{r}}}_{jk} Thus, Hence: \\mathbf{F}_k = \\sum_{j\\ne{k}} V'(r_{kj}){\\hat{\\mathbf{r}}}_{kj} = -\\sum_{j<k} V'(r_{jk}){\\hat{\\mathbf{r}}}_{jk} + \\sum_{k<j} V'(r_{kj}){\\hat{\\mathbf{r}}}_{kj} Finally (setting all parameters to unity), V'(r) = -2(1-e^{1-r})e^{1-r} Now that we have the forces on the atoms in the current configuration, we should be able to move the atoms in the directon of the force, rather than in a random direction, as before. In fact we have a true minimisation problem now. to be continued...","title":"Project mcgse"},{"location":"chapter-4/#study-of-data-access-patterns-in-a-large-lennard-jones-systems","text":"","title":"Study of data access patterns in a large Lennard-Jones systems"},{"location":"chapter-4/#introduction_1","text":"In this case study we consider a large system of atoms whose interaction is described by a Lennard-Jones potential. By large we mean a system that does not fit in the cache. Consequently, the effect of caches will be noticabel in the results. We will consider two different settings. A Monte Carlo setting, as above, in which the interaction energy is computed as a sum of pairwise interactions. It is of little physical significance, but is useful to demonstrate the effect of the caches on the computations. The second setting is a true molecular dynamics setting in which the time evolution of a collection of atoms is computed by time integration of the interaction forces which are computed as the gradient of the interaction potential. This gives rise to time dependent accelerations, velocities and positions of the atoms.","title":"Introduction"},{"location":"chapter-4/#monte-carlo-setting","text":"The interaction energy is given by: E=\\sum_{i<j}V(r_{ij}) Since our sytem is large, say billions of atoms, computing this sum considering all pairs, is computationally unfeasible because it has O(N^2) computational complexity. We will discuss approaches to reduce the computational complexity to O(N) . To study the effect of the cache we will compute the partial sum E_i=\\sum_{j\\ne{i}}V(r_{ij}) for i=0 , that is E_0=\\sum_{j=1}^{N}V(r_{0j}) Because our system is translationally invariant, we can put atom 0 at the origin, in which case r_{0j}=r_j . Thus, we end up with: E_0=\\sum_{j=1}^{N}V(r_{j}) We will use the best implementation for the Lennard-Jones potential that we discussed in The cost of floating point instructions , expressed as a function of r^2 , as to avoid the square root needed to compute r . We consider three different cases: A contiguous loop over arrays x[1:N] , y[1:N] , z[1:N] . This is a structure of arrays (SoA) approach. A contiguous loop over a single array xyz[1:3N , in which the x , y and z coordinates of the i -th atom come after each other followed by the x , y and z coordinates of the i+1 -th atom. This is a array of structures approach (AoS). A contiguous loop over arrays x[1:N] , y[1:N] , z[1:N] in which the atoms are picked by random permutation of 1..N . So, all atoms are visited, but in a random order. For each case E_0=\\sum_{j=1}^{N}V(r_{j}) is computed for N \\in \\{2^9,2^10,2^11,...,2^{29}\\} repeating the loop over j 2^{29}/N times. In this way the amount of interaction potential evaluations is exactly 2^{29} irrespective of the length of the array, and the timings can be compared. The smallest arrays fit in L1, while the longest arrays ( 2^29\\approx0.5\\times10^9 ) do not even fit in L3. Here are the timings: It is clearly visible that the behaviour of the random case above is very different from the two contiguous cases. For the longest arrays, the performance is a whopping 15x worse on the random loop, yet every case performs exactly the same work. There burning question is of course: \"what is causing the performance breakdown of the randomized loop\"? The second question, certainly less burning, but nevertheless important, is: \"is the lowest curve (the AoS case) the best we can get?\". If you are really curious, you might wonder about the small difference between the AoS case and the SoA case at larger N . To help your understanding of the problem, here is a different representation of the same graph, this time the number of bytes used by the arrays on the x-axis instead of the array size N . With this x-axis it is easy to draw the boundaries of the L1, L2 and L3 caches. Surprisingly enough, the changes in the curves coincide with the cache boundaries. As soon as the problem is too large for a cache, cache misses cause pipeline stalls, and the CPU has to wait for the data needed. The latency increases at every cache boundary and the slowdown becomes more pronounced each time. This also explains the slight advantage for the AoS case over the SoA case for problems not fitting in L3. As x, y, and z follow contiguously in memory in the AoS case, when it needs new data from memory, it has to wait for only a single cache line, while the SoA needs three. If you have difficulties to grasp, revisit the talk by Scott Meyers CPU Caches and Why You Care . The second question is a bit harder to answer. Let us analyze the performance of the (Fortran) loop: ! Contiguous access, SoA: p=[xxx\u2026yyy\u2026zzz\u2026] do ik=1,k do im=1,m ! FLOPS r2 = (p(im)-x0)**2 ! +(p(m+im)-y0)**2 ! +(p(2*m+im)-z0)**2 ! 3-, 2+, 3* ! r = lj_pot2(r) ! r2i = 1.0d0/r2 ! 1/ rr6i = r2i*r2i*r2i; ! 2* V0j = 4.0d0*rr6*(rr6-1.0d0); ! 2*, 1- enddo !------------ enddo ! 14 flops The loop has 14 floating point operations. It is executed 2^29 times in 1.2s. That makes 6.26\\times 10^9 flops/s. The peak performance of the machine is 1 core x 1 instruction per cycle x 4 SIMD registers per instruction x 2.8 GHz = 11.2 Gcycles/s = 11.2 Gflops/s. Consequently, we are running at 56% of the peak performance. So it looks as if we could still do better. Let us analyze the data traffic of the same loop: ! Contiguous access, SoA: p=[xxx\u2026yyy\u2026zzz\u2026] do ik=1,k do im=1,m ! FLOPS ! DATA r2 = (p(im)-x0)**2 ! ! +(p(m+im)-y0)**2 ! ! +(p(2*m+im)-z0)**2 ! 3-, 2+, 3* ! 3DP ! r = lj_pot2(r) ! ! r2i = 1.0d0/r2 ! 1/ ! rr6i = r2i*r2i*r2i; ! 2* ! V0j = 4.0d0*rr6*(rr6-1.0d0); ! 2*, 1- ! enddo !---------------!----- enddo ! 14 flops ! 24B The loop reads 24 bytes x 2^29 iterations in 1.2 s. That makes 10.7 GB/s. The bandwidth of the machine is 109 GB/s for 10 cores, that is 10.9 GB for 1 core. Our loop runs at the maximum bandwidth. It is bandwith saturated . This is a machine limit. It can simply not feed the CPU with data faster than this. It is instructive to draw a roofline model for this. The above loop, that is the contiguous cases, plot on the bandwidth part of the roofline indicating that the machine limit (bandwidth) is reached, the random case sits close to the bottom far away from all machine limits. The conclusion is that the loop as it is runs at its maximum speed, being bandwidth limited. However, 44% of the time the CPU is not doing useful work, because it is waiting for data. That means that if we replaced the Lennard-Jones potential with another one that is about twice as compute intensive, and for that reason more accurate, we would still finish the computation in 1.2s and have a more accurate solution, because we are using the cycles that the CPU was waiting for data to do the extra computations.","title":"Monte Carlo setting"},{"location":"chapter-4/#molecular-dynamics-setting","text":"We consider the same system, a large collection of atoms interacting through a Lennard-Jones potential. In a Molecular Dynamics setting the time evolution of th system is computed by time integration of the classical equation of motion: \\dot{\\mathbf{r}} = \\mathbf{v} \\dot{\\mathbf{v}} = \\mathbf{a} \\mathbf{a} = \\mathbf{F} The forces are computed as the gradient of the interaction energy: \\mathbf{F}_i = \\nabla_{\\mathbf{r}_i}{E} = \\nabla_{\\mathbf{r}_i} \\sum_{j\\ne{i}}^{N}V(r_{ij}) We assume a system size of N=10^9 atoms. The number of terms in the sum above is then 10^9(10^9-1)/2\\approx{10^ {18}} . That will keep us busy, won't it... However, when you start evaluating all these contributions, you very soon realize that most of them are really small, so small that they don't actually contribute to the result. They are short-ranged . Mathematically, a force is short-ranged if it decays faster than r^{-2} . This is because the area of a sphere with radius r is 4\\pi r^2 and hence the number of particles at distance grows as r^2 . Consequently, in order for the force exerted by those particle to be negligible it has to decay faster than r^{-2} . The derivative of the Lennard-Jones potential is: V'(r) = ({-6}/{r}) r^{-6}(2r^{-6}-1) Hence, \\mathbf{F}_i = \\sum_{j\\ne{i}}^{N}\\nabla_{\\mathbf{r}_i}V(r_{ij}) = \\sum_{j\\ne{i}}^{N}V'(r_{ij})\\nabla_{\\mathbf{r} _i}r_{ij} = \\sum_{j\\ne{i}}^{N}V'(r_{ij}) \\hat{\\mathbf{r}}_{ij} = \\sum_{j\\ne{i}}^{N} ({-6}/{r_{ij}}) r_{ij}^{-6}(2r_{ij}^{-6}-1) \\frac{\\mathbf{r}_{ij}}{r_{ij}} = \\sum_{j\\ne{i}}^ {N} -6 r_{ij}^{-8}(2r_{ij}^{-6}-1) \\mathbf{r}_{ij} Note that the force factor f , that is the factor in front of \\mathbf{r}_ij , can also be expressed in terms of s=r^2=\\delta{x}^2+\\delta{y}^2+\\delta{x}^2 : f(s) = -6 s^{-4}(2s^{-3}-1) \\mathbf{F}_i = \\sum_{j\\ne{i}}^{N} f(s_{ij}) \\mathbf{r}_{ij} So, we can avoid the square root in computing r_ij . Clearly, we can compute the interaction energy and the interaction force in one go with little extra effort: V(s) = s^{-3}(s^{-3}-1) E = \\sum_{i<j} V(s_{ij}) The fact that the interaction force is short-ranged, allows us to neglect the interaction forces beyond a cutoff distance r_c , thus offering a possibility to avoid the cost of an O(N^2) algorithm.","title":"Molecular Dynamics setting"},{"location":"chapter-4/#implementing-cutoff","text":"As a first step we we can avoid the computation of the interaction energy and the interaction force if r_{ij}>r_c , or s_{ij}>s_c : # (python psseudo-code) for i in range(N): for j in range(i): x_ij = x[j]-x[i] y_ij = y[j]-y[i] z_ij = z[j]-z[i] s_ij = x_ij*x_ij + y_ij*y_ij + z_ij*z_ij if s_ij <= s_c: t = 1/s_ij t3 = t*t*t E += t3*(t3-1) f = -6*t*t3*(2*t3-1) Fx[i] += f*x_ij Fy[i] += f*y_ij Fz[i] += f*z_ij Fx[j] -= f*x_ij Fy[j] -= f*y_ij Fz[j] -= f*z_ij Although this loop only computes the interactions wheen s_{ij}\\le{s_c} , it still visits every pair to compute s_{ij} . The corresponding amount of work is still O(N^2) . Some improvement is possible by using Verlet lists. The Verlet list of an atom i is the set of atoms j for which r_{ij}<r_v , where r_v is typically a bit larger than r_c . The loop is now witten as: # (python psseudo-code) for i in range(N): for j in verlet_list(i): # as above The loop over j is now much shorter, its length is bounded, typically in the range 10..100 . Hence, the double loop is effectively O(N) . The construction of the Verlet list, however, is still O(N^2) , but the cost of it is amortised over a number of timesteps. Because atoms move only a little bit over a time step and r_v>r_c , the Verlet list can indeed be reused a number of timesteps, before it needs to be updated. Algorithms for constructing the Verlet list with O(N) complexity do exist. Here's a 2-D version of cell-based Verlet list construction . It can be easily extended to 3-D, but that is harder to visualise. In the left figure below, atom i (the orange dot) is surrounded by a blue circle of radius r_v . Atoms inside the blue circle are in the Verlet list of atom i . We now overlay the domain with a square grid, of grid size r_v (middle figure). Atom pairs in the same cell or in nearest neighbour cells are Verlet list candidates, but not pairs in second nearest neighbours or further. To construct the Verlet list of atom i , we only have to test atoms in the same cell, or in its 8 nearest neighbours, all coloured light-blue. By iterating over all cells and over the atoms it contains, the Verlet lists of all atoms can be constructed with O(N) complexity. In fact, by looking for pairs in all rearest neighbours, all candidate pairs are visited twice ( ij and ji ). Hence, only half of the nearest neighbours needs to be visited (right figure). The algorithm requires that the grid implements: a cell list: a list of all the atoms that are in the cell, in order to iterate over all atoms in a cell. The cell lists can be constructed with O(N) complexity, and a method to find the neighbour cells of a cell. This is a good example for demonstrating the effectiveness of our strategy for research software development . Here are the steps you should take Start out in Python. Take a small system, e.g. N=5 , use Numpy arrays for the positions, velocities, ... Implement brute force computation of interactions and interaction forces ( O(N^2) ). Implement brute force computation of interactions and interaction forces with cutoff ( O(N^2) ). Implement brute force construction of Verlet lists ( O(N^2) ). (You might need a larger system for testing this). Implement Verlet list computation of interactions and interaction forces ( O(N) ). Implement cell-based Verlet list construction ( O(N) ). (You might need a larger system for testing this). Optimise, try using Numba, or by taking the compute intensive parts to C++. Of course test and validate every step, e.g. by comparing to previous steps. Tip Remember that, for performance , you should avoid using loops in Python . When I implemented the cell-based Verlet list construction in Python, it turned out to be terribly slow, mainly because of 5 levels of nesting Python loops. The C++ version turned out to be 1200x faster (twelve hundred indeed, no typo!).","title":"Implementing cutoff"},{"location":"chapter-4/#moving-atoms","text":"The initalization of a physically consistent system of atoms is a non-trivial task in itself. Because molecular motion conserves energy, random positions and velocities at time t=0 may pos a lot of trouble for time integration. When two atoms happen to be very close they experience very high repulsive force and thus are accelerated vigorously. This can easily make the simulation explode. A practical way is to put atoms on a lattice with interatomic distances close to the equilibrium distance of the Lennard-Jones potential, e.g. primitive cubic, body-centred cubic (BCC), face-centred cubic (FCC), hexagonal closest packing (HCP). then slowly increase random velocities to increase the kinetice energy and hence the temperature. When initializing the system on a lattice, often the performance is rather good because the regular arrangement allows for a good data access pattern. However, as (simulation) time proceeds the atoms move and diffusion kicks in. Every timestep, some atoms will move in and out of some other atom's Verlet sphere. Gradually, the atoms will move further and further from their original positions, but their location in memory does not change, and, consequentially, the data access pattern appproaches the random array access we discussed above, leading to considerable performance degradation.","title":"Moving atoms"},{"location":"chapter-5/","text":"Chapter 5 - Developing Research Software A strategy for the development research software You are facing a new research question, to be solved computationally and on your shelf of computational tools nothing useful is found. You start with an empty sheet of paper on you desk. or rather with an empty screen on your laptop. How you take on such a challenge? This chapter is about a strategy for (research) code development that minimizes coding efforts, allows for high performance, provides flexible and reusable software components. Coding efforts is more than just the time to type your program in an editor or IDE. It is also the time you spend making sure that your code is correct, and stays correct while you are working on it, restructuring its components, improving performance, parallelising it, and ensuring that it solves the problem you need to solve. High performance is essential when we run our problem on a supercomputer, but perhaps that is not necessary. We want to postpone performance optimisation until it is really needed. This principle was set in stone in a quote by Donald Knuth in 1974 already: \"Premature optimisation is the root of all evil\". Spending time on optimisation before it is needed is wasting time, and stopping progress. Initially, we want to focus on simplicity and correctness, and on understanding the characteristics of the problem at hand. If the algorithm we choose to solve is inadequate, we want to know that as soon as possible. On the other hand, when it is needed, we want our project to be in a state that facilitates performance optimisation where it is needed. Finally, we want an approach that builds experience. Flexible and reusable software components are materializing the experience that we build up. They enable us to proceed faster when pieces of a last year's problem may be useful today. Two important aspects of reusable code are writing simple functions and classes with a single functionality, and documentation. Well documenting your code increases the chance that when you check out your code three months later, you will not be staring baffled at your screen wondering what it was all about. It happens to me, it can certainly happen to you. None of these features come without effort. It is also not a bucket list of checkboxes to make sure that you did not overlook something. They comprise a learning process, require attention, disipline and research . Here is a story that demonstrates an anti-pattern for research code development. Note The term pattern refers to a set problems with common features that can be efficiently solved with the same approach, by applying the pattern. The term comes from a very useful book \"Design Patterns - Elements of reusable object-oriented software\", by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, 1995. Several \"pattern\" books later transferred the approach to other domains. An anti-pattern is a pattern that perhaps solves the problem, or not, but if it does, in pessimal way. It is a pattern NOT to follow. A PhD student once asked me for support. He had written a 10 000 line Fortran program. When he ran it, the results were not what he expected, and he suspected that there was a bug 'somewhere'. He asked if I could help him find the bug and - no kidding - by the end of next week, because the program had to go into production by then. I had to disappoint him and told him that he needed a 'true' magician, which I, unfortunately, was not. Obviously, the program was flawed in some sense, but other than a bug it might just as well be one or more of the situations below: The program may contain many bugs, which is very well possible in view of its size. On average a programmer introduces about 1 bug in every 10 lines of code! Check this link for some amazing facts The algorithm for solving the problem is inappropriate. There is an accuracy problem, related e.g. discretisation of time, space, or a insufficient basis function for expanding the solution, or related to the finite precision of floating point numbers. (Floating point numbers are a processor's approximation of the real numbers, but they do have different mathematical properties. E.g. floating point addition is not commutative.) The mathematical formulation itself could be flawed or misunderstood. The program is correct, but the researchers expectations are wrong. ... It could easily take weeks, if not months to write tests for the individual components to learn about the behaviour of the program and narrow down to the source of the error. This anti-pattern is a disaster waiting to happy. For this reason a sound strategy that makes sure that the code you write is correct, that does what you want, and builds understanding of the problem at hand as you proceed, is indispensable. The strategy proposed below has been shaped by a lifetime of research software developement. I use it in nearly every software project I take on, whether it is small or big, simple or complex, starting from scratch or from someone else code. The proposed strategy builds on five principles: Start out in a high level programming language. Python is an excellent choice. Start out simple, as simple as possible, in order to build up understanding of your problem and how it should be solved fast. Test and validate your code, continuously, and for small code fragments, to discover bug and mistakes as soon as possible. Improve your code, adding better algorithms (which usually are more complex), pay attention to data structure facilitating good data access patterns, think of common sense optimisations , gradually increase the complexity of the problem. Keep principle 3. in mind and continue testing and validating the improvements. If neccessary, optimise your code. Necessary is when the time to solution is too long. If necessary, parallelise your code. Necessary is when the time to solution is still too long after paying attention to principle 5., or if the problem does not fit in the memory of a single machine. In that case it it advisable to optimise anyway to not waste cycles on an expensive supercomputer. Tip for researchers of the University of Antwerp and institutes affiliated with the VSC Researcher of the University of Antwerp and institutes affiliated with the VSC are wellcome to contact me for support when developing research software. The best time to do that would be before having written any line of code at all for the problem, in order to follow all principles. The ordering of the list is important and reflects the successive steps in working on a project. Below, we explain these principles in depth. Much of the wisdom of this strategy is integrated into a Python application micc2 . Principle 1 Start out in a high level language Python is an excellent choice: Python is a high-level general-purpose programming language that can be applied to many different classes of problems. Python is easy to learn. It is an interpreted and interactive language and programming in Python is intuitive, producing very readable code, and typically significantly more productive than in low-level languages as C/C++/Fortran. Scripting provides a very flexible approach to formulating a research problem, as compared to an input file of a low-level language program. It comes with a large standard Library There is wide variety of third-party extensions, the Python Package Index(PyPI) . Many packages are built with HPC in mind, on top of high quality HPC libraries. The functionality of standard library and extension packages is enabled easily as import module_name , and installing packages is as easy as: pip install numpy . The use of modules is so practical and natural to Python that researchers do not so often feel the need to reinvent wheels. Availability of High quality Python distributions ( Intel , Anaconda ), cross-platform Windows/Linux/MACOS Python is open source. In itself that is not necessarily an advantage, but its large community guarantees an very good documentation, and for many problems high-quality solutions are found readily on user forums. Python is used in probably any scientific domain, and may have many third party extension freely available in that domain. It is available with a lot scientific Python packages on all VSC -clusters. Several high-quality Integrated Development Environments (IDEs) are freely available for Python: e.g. PyCharm , VS Code , which at the same time provide support for C/C++/Fortran. Although Python in itself is a rather slow language, as we will see, there are many ways to cope with performance bottlenecks in Python. The quality of Python is embodied it its design principles \"The Zen of Python\" which are printed when you import this . Many of these also apply to our strategy for developing research software. > python >>> import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! In many ways, Python gently pushes you in the right direction, providing a pleasant programming experience. As an alternative on could consider Julia . Following a different programming paradigm, called multiple dispatch , its learning curve is probably a bit steeper and it has a smaller community. However, it is more performant than Python. On the other hand its programming productivity might not be that of Python. An other alternative could be Matlab . Although it has a long history of scientific computing, from as a language is it not as well designed as Python, and it is not fit for HPC. Moreover, it is a commercial product, and the community using it is not as willing to share its solutions as for Python, and there are much less extensions available. Often, you will need to solve the problem yourself, and, as a consequence, proceed at a slower pace. Starting out in a low-level programming language (C/C++/Fortran) is certainly a bad idea. Even if you are an experienced programmer you will proceed slower. The productivity of Python is easily 10 times larger than for C/C++/Fortran because: the advantages coming with an interpreted language, vs. a compiled language, the fact that Python is a very well designed, expressive and readable language with a rather flat learning curve, the availability of a large standard library and a wide range of domain-specific third party extensions, as well as the ease with which these are enabled in your code. Hence, this course sticks to Python. Principle 2 Start out as simple as possible Take a small toy problem, the smallest you can think of that still represents the problem that you want to solve. There is a famous quote, attributed to Einstein ( although it seems he formulated it differently ) \" Make everything as simple as possible, but not simpler \". That applies very well here. The simpler the problem you start with, the faster you will build experience and understanding of the problem. Ideally, take a problem with a known analytical solution. Look for something you can easily visualise. Four atoms are easier to visualise than a hundred. Visualisation is a perfect way for obtaining insight (pun intended!) in your problem. Choose the simplest algorithm that will do the trick, don't bother about performance. Once it works correctly, use it as a reference case for validating improvements. Here are some interesting Python modules for visualisation: matplotlib bokeh plotly seaborn Principle 3 Test and validate your code (changes) In view of these amazing facts on the abundance of bugs in code there seems to be little chance that a programmer writes 100 lines of code without bugs. On average 7 bugs are to be expected in every 100 lines of code. Probably not all these bugs affect the outcome of the program, but in research code the outcome is of course crucial. How can we ensure that our code is correct, and remains so as we continue to work on it? The answer is unit-tests. Unit-testing are pieces of test-code together with verified outcomes. In view of the abundancy of bus it is best to test small pieces of code, in the order of 10 lines. Test code is also code and thus can contain bugs as well. The amount of test code for a system can be large. An example is probaly the best way to demonstrate the concept. In chapter 4 we discussed the case study Monte Carlo ground state energy calculation of a small atom cluster and a small project wetppr/mcgse where the original study is repeated with a Morse potential, described by the formula: V(r) = D_e(1 - e^{-\\alpha(r-r_e)})^2 TODO: ## fix the link The code for this function is found in wetppr/mcgse/ init .py . Note that we provided default unit values for all the parameters: import numpy as np def morse_potential(r: float, D_e: float = 1, alpha: float = 1, r_e: float = 1) -> float: \"\"\"Compute the Morse potential for interatomic distance r. This is better than it looks, we can pass a numpy array for r, and it will use numpy array arithmetic to evaluate the expression for the array. Args: r: interatomic distance D_e: depth of the potential well, default = 1 alpha: width of the potential well, default = 1 r_e: location of the potential well, default = 1 \"\"\" return D_e * (1 - np.exp(-alpha*(r - r_e)))**2 The implementation of the function comprises only one line. How can we test its correctness? One approach would be to list a few r-values for which we know the outcome. E.g. V(r_e) = 0 . Here is a test funtion for it. Note that from math import isclose def test_morse_potential_at_r_e(): # the value at r_e=1 is 0 r = 1 Vr = mcgse.morse_potential(x) Vr_expected = 0 assert isclose(Vr, Vr_expected, rel_tol=1e-15) Because the function is using floating point arithmetic, the outcome could be subject to roundoff error. We account for a relative error of 1e-15 . When running the test, an AssertionError will be raised whenever the relative error is larger than 1e-15 . When it comes to testing functions it is practical to focus on mathematical properties of the function (fixing parameters to unity). E.g. 0 \\le V(r) for r \\in ]0,1] , 0 \\le V(r) < 1 for r \\in [1,+\\infty[ , V(r) is monotonously decreasing on ]0,1] , V(r) is monotonously increasing on [1,+\\infty[ , V''(r) is positive on ]0,1] , V''(r) is positive on ]1,r_i] , r_i=1 - \\log(1/2) being the inflection point, V''(r) is negative on ]r_i, +\\infty[ , ... See tests/wetppr/mcgse/test_mcgse.py for details. The file contains many more tests for other functions in the file wetppr/mcgse/__init__.py . Automating tests As soon as you have a few tests, running them manually after every code change, becomes impractical. We need to automate running the tests. Several automated test runners are available. Pytest is a good choice. When given a directory as a parameter, it will import all test_*.py files under it, look for methods starting with test , execute them and produce a concise report about which tests pass and which tests fail. Details about test discovery are found here . > cd path/to/wetppr > pytest tests ==================================== test session starts ===================================== platform darwin -- Python 3.9.5, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 rootdir: /Users/etijskens/software/dev/workspace/wetppr plugins: typeguard-2.13.3, mpi-0.5, anyio-3.6.2 collected 6 items tests/wetppr/mcgse/test_mcgse.py ..... [100%] ===================================== 5 passed in 1.65s ====================================== As is clear from the output above it found 5 tests in tests/wetppr/mcgse/test_mcgse.py , all executed successfully. In the presence of errors it is instructive to run pytest with -v -s options. For every new method added to your code, add some tests and run pytest . For every code change run all the tests again. Make sure they pass before you continue to improve or extend the code. Debugging a failing test Debugging is the process of stepping through a program, executing it line by line, and examining the results in order to find the source of the error. IDEs like PyCharm and VS Code provide a most friendly debugging experience, but you can also use the Python debugging module pdb : Debugging a failing test is often the best way to investigate the source of the error. Python has an interesting idiom that allows a module to be run as a script, i.e. execute a module . To that end you put this code snippet at the end of a module: # # module code here # if __name__ == \"__main__\": # # script code here. # print(\"-*# finished #*-\") # if this line doesn't show up, something went wrong. The idiom has no common name :-(, we will refer to it as the if __name__ == \"__main__\": idiom. If a module file is imported as in import module_name , only the module code is executed, and the body of the if statement if __name__ == \"__main__\": is ignored, because the import statement sets the value of the __name__ variable to the module name. Thus, the condition evaluates to False . Most of the module code will consist of def and class statements, defining methods (the Python term for a function) and classes. When Python executes a def or a class statement, it interprets the code of the method or class and registers them under their respective names, so that they can be called by the module or script that imported the module. If the module file is executed on the command line, as in the command python module_name.py or in an IDE, the python executable sets the __name__ variable to \"__main__\" . Thus, the condition evaluates to True and its body is executed. This Python idiom provides us with a practical approach for executing a failing test. Assume that by running pytest tests we find out that there is an error in the test function test_morse_potential_mathematical_properties : > pytest tests ==================================== test session starts ===================================== platform darwin -- Python 3.9.5, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 rootdir: /Users/etijskens/software/dev/workspace/wetppr plugins: typeguard-2.13.3, mpi-0.5, anyio-3.6.2 collected 6 items tests/wetppr/test_wetppr.py . [ 16%] tests/wetppr/mcgse/test_mcgse.py .F... [100%] ========================================== FAILURES ========================================== ________________________ test_morse_potential_mathematical_properties ________________________ ... ================================== short test summary info =================================== FAILED tests/wetppr/mcgse/test_mcgse.py::test_morse_potential_mathematical_properties - ass... ================================ 1 failed, 5 passed in 4.27s ================================= At the end of the test module tests/wetppr/mcgse/test_mcgse.py , which contains the failing test test_morse_potential_mathematical_properties you will see this code snippet: # # test functions here # if __name__ == \"__main__\": the_test_you_want_to_debug = test_energy print(\"__main__ running\", the_test_you_want_to_debug) the_test_you_want_to_debug() print(\"-*# finished #*-\") When we run pytest tests , pytest imports the test module, setting __name__ to \"test_mcgse\" . Hence the condition __name__ == \"__main__\" evaluates to False and its body is not executed. Only when the module is executed the body will be executed too. Since we want to run the test_morse_potential_mathematical_properties function, change the first line in the body to: the_test_you_want_to_debug = test_morse_potential_mathematical_properties The variable the_test_you_want_to_debug now is an alias for the test function that we want to debug, test_morse_potential_mathematical_properties . The next statement is a print statement producing something like: __main__ running <function test_morse_potential_mathematical_properties at 0x11098c310> which ensures us that we called the right test function. The test function is then called in the next statement through its alias the_test_you_want_to_debug : the_test_you_want_to_debug() To debug the test using pdb execute: > python -m pdb tests/wetppr/mcgse/test_mcgse.py Quick and dirty testing The if __name__ == \"__main__\": idiom has another interesting application. When working on small projects, with only a few functions, sometimes we don't want to set up a tests directory with a test_small_module.py file. Instead we can use the if __name__ == \"__main__\": idiom to write a quick and dirty test inside the module, or just call the function to check by running or debugging the module that it does the right thing. In this way your work is restricted to a single file, and there is no need to switch between the test file and the module file. Principle 4 Improve and extend the code Once the code for the simple problem you started with (see Principle 2 ) is tested, validated, and understood, you can gradually add complexity, approaching the true problem you need to solve. This may require improvements to the code because the time to solution will probably increase. Possible improvements are: better algorithms with lower computational complexity, better data structures, facilitating good data access patterns, common sense optimisations , increasing the flexibility of the code, restructuring the code, gradually increase the complexity of the problem. Obviously, continue to test and validate all code changes and extensions. Principle 5 Optimise if necessary If neccessary, optimise your code. Necessary is when the time to solution is too long. At this point you will probably already have a considerable Python code base. Bearing in mind that premature optimisation is the root of all evil , the important question is what needs optimisation and what not. The answer is provided by using a profiler. Profiler are tools that tell you how much time your program spends in the different parts of it and how many times tha part is executed. Some profilers provide information on a per function basis, and tell how many times that function was called and the average or cumulative time spent in it. Other profilers provide information on a per line basis, an tell how many times that line was executed and the average or cumulative time spent on it. This article provides the necessary details. You should start profiling on a per function basis and then profile the function(s) consuming most time with a line-by-line profiler. If it turns out that only a few lines of the function are responsible for the runtime consumption, split them off in a new function and try optimising the new function using one of the techniques discussed below. If not, try optimising the whole function with the very same techniques. Tip After optimising a (split-off) function, run the profiler again to check the performance. Techniques for optimising performance Below a series of optimisation techniques is presented, ordered by development effort needed. Use Python modules built for HPC There exist excellent Python modules that provide very performant high level data structures and operations. Using them can make a dramatic difference. E.g. the difference in performance between Python lists and Numpy arrays can easily be a factor 100. By using them you do not only gain performance. Once you have learned how to use a Python module, such as Numpy, development time is also reduced as you no longer have to code the low-level logic yourself. Here are a few interesting scientific Python modules: Numpy : n-dimensional arrays, mathematical functions, linear algebra, ... SciPy : fundamental algorithms for scientific computing... sympy : symbolic computation pandas : data analysis Numba Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python. It takes your Python code, transforms it into C code, compiles it (on the fly) and calls the compiled C code version. Here is how a Numba-optimized function, taking a Numpy array as argument, might look like: import numba @numba.jit def sum2d(arr): M, N = arr.shape result = 0.0 for i in range(M): for j in range(N): result += arr[i,j] return result the @numba.jit decorator instructs the Python interpreter to use the numba just-in-time compiler (jit) to translate the Python code int C. compile it, and use the compiled verson on every call. As pure Python loops are quite expensive and the function has a double Python loop over 'i' and 'j' the performance gain is considerable. As in principle it only involves adding a decorator (and possibly some hinting at data types) using numba can be a really quick win. Note The above function can also defined as np.sum(np.sum(arr)) , using numpy function calls exclusively. That might be even faster. Tip Numba is sensitive to data types. Changing the data type of the arguments can improve the situation a lot. Developing your own modules in C++/Fortran If none of the above techniques helps, you might consider to develop your own Python modules compiled from C++ or Fortran code, also known as binary Python extensions. This can be a real game changer. In fact, this is exactly what the HPC Python modules like Numpy and SciPy do, and they do it because that is the way to harness the power of modern CPUs and expose it to Python. Obviously, this requires good knowledge of C++ or Fortran, and good understanding of performance critical characteristics fo modern CPU architecture. The application micc2 facilitates building your C++ and Fortran modules. Checkout its tutorials . Principle 6 Parallelise if necessary If necessary, parallelise your code. Parallelisation is necessary is when the time to solution is still too long after paying attention to principle 4 and principle 5 , or if the research problem does not fit in the memory of a single machine. In that case it is advisable to optimise ( principle 5 ) anyway to not waste cycles on an expensive supercomputer. To parallelise Python projects these tools come in handy: dask : multi-core and multi-node parallellisation mpi4py : MPI parallellisation with python","title":"Chapter 5 - Developing Research Software"},{"location":"chapter-5/#chapter-5-developing-research-software","text":"","title":"Chapter 5 - Developing Research Software"},{"location":"chapter-5/#a-strategy-for-the-development-research-software","text":"You are facing a new research question, to be solved computationally and on your shelf of computational tools nothing useful is found. You start with an empty sheet of paper on you desk. or rather with an empty screen on your laptop. How you take on such a challenge? This chapter is about a strategy for (research) code development that minimizes coding efforts, allows for high performance, provides flexible and reusable software components. Coding efforts is more than just the time to type your program in an editor or IDE. It is also the time you spend making sure that your code is correct, and stays correct while you are working on it, restructuring its components, improving performance, parallelising it, and ensuring that it solves the problem you need to solve. High performance is essential when we run our problem on a supercomputer, but perhaps that is not necessary. We want to postpone performance optimisation until it is really needed. This principle was set in stone in a quote by Donald Knuth in 1974 already: \"Premature optimisation is the root of all evil\". Spending time on optimisation before it is needed is wasting time, and stopping progress. Initially, we want to focus on simplicity and correctness, and on understanding the characteristics of the problem at hand. If the algorithm we choose to solve is inadequate, we want to know that as soon as possible. On the other hand, when it is needed, we want our project to be in a state that facilitates performance optimisation where it is needed. Finally, we want an approach that builds experience. Flexible and reusable software components are materializing the experience that we build up. They enable us to proceed faster when pieces of a last year's problem may be useful today. Two important aspects of reusable code are writing simple functions and classes with a single functionality, and documentation. Well documenting your code increases the chance that when you check out your code three months later, you will not be staring baffled at your screen wondering what it was all about. It happens to me, it can certainly happen to you. None of these features come without effort. It is also not a bucket list of checkboxes to make sure that you did not overlook something. They comprise a learning process, require attention, disipline and research . Here is a story that demonstrates an anti-pattern for research code development. Note The term pattern refers to a set problems with common features that can be efficiently solved with the same approach, by applying the pattern. The term comes from a very useful book \"Design Patterns - Elements of reusable object-oriented software\", by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, 1995. Several \"pattern\" books later transferred the approach to other domains. An anti-pattern is a pattern that perhaps solves the problem, or not, but if it does, in pessimal way. It is a pattern NOT to follow. A PhD student once asked me for support. He had written a 10 000 line Fortran program. When he ran it, the results were not what he expected, and he suspected that there was a bug 'somewhere'. He asked if I could help him find the bug and - no kidding - by the end of next week, because the program had to go into production by then. I had to disappoint him and told him that he needed a 'true' magician, which I, unfortunately, was not. Obviously, the program was flawed in some sense, but other than a bug it might just as well be one or more of the situations below: The program may contain many bugs, which is very well possible in view of its size. On average a programmer introduces about 1 bug in every 10 lines of code! Check this link for some amazing facts The algorithm for solving the problem is inappropriate. There is an accuracy problem, related e.g. discretisation of time, space, or a insufficient basis function for expanding the solution, or related to the finite precision of floating point numbers. (Floating point numbers are a processor's approximation of the real numbers, but they do have different mathematical properties. E.g. floating point addition is not commutative.) The mathematical formulation itself could be flawed or misunderstood. The program is correct, but the researchers expectations are wrong. ... It could easily take weeks, if not months to write tests for the individual components to learn about the behaviour of the program and narrow down to the source of the error. This anti-pattern is a disaster waiting to happy. For this reason a sound strategy that makes sure that the code you write is correct, that does what you want, and builds understanding of the problem at hand as you proceed, is indispensable. The strategy proposed below has been shaped by a lifetime of research software developement. I use it in nearly every software project I take on, whether it is small or big, simple or complex, starting from scratch or from someone else code. The proposed strategy builds on five principles: Start out in a high level programming language. Python is an excellent choice. Start out simple, as simple as possible, in order to build up understanding of your problem and how it should be solved fast. Test and validate your code, continuously, and for small code fragments, to discover bug and mistakes as soon as possible. Improve your code, adding better algorithms (which usually are more complex), pay attention to data structure facilitating good data access patterns, think of common sense optimisations , gradually increase the complexity of the problem. Keep principle 3. in mind and continue testing and validating the improvements. If neccessary, optimise your code. Necessary is when the time to solution is too long. If necessary, parallelise your code. Necessary is when the time to solution is still too long after paying attention to principle 5., or if the problem does not fit in the memory of a single machine. In that case it it advisable to optimise anyway to not waste cycles on an expensive supercomputer. Tip for researchers of the University of Antwerp and institutes affiliated with the VSC Researcher of the University of Antwerp and institutes affiliated with the VSC are wellcome to contact me for support when developing research software. The best time to do that would be before having written any line of code at all for the problem, in order to follow all principles. The ordering of the list is important and reflects the successive steps in working on a project. Below, we explain these principles in depth. Much of the wisdom of this strategy is integrated into a Python application micc2 .","title":"A strategy for the development research software"},{"location":"chapter-5/#principle-1","text":"","title":"Principle 1"},{"location":"chapter-5/#start-out-in-a-high-level-language","text":"Python is an excellent choice: Python is a high-level general-purpose programming language that can be applied to many different classes of problems. Python is easy to learn. It is an interpreted and interactive language and programming in Python is intuitive, producing very readable code, and typically significantly more productive than in low-level languages as C/C++/Fortran. Scripting provides a very flexible approach to formulating a research problem, as compared to an input file of a low-level language program. It comes with a large standard Library There is wide variety of third-party extensions, the Python Package Index(PyPI) . Many packages are built with HPC in mind, on top of high quality HPC libraries. The functionality of standard library and extension packages is enabled easily as import module_name , and installing packages is as easy as: pip install numpy . The use of modules is so practical and natural to Python that researchers do not so often feel the need to reinvent wheels. Availability of High quality Python distributions ( Intel , Anaconda ), cross-platform Windows/Linux/MACOS Python is open source. In itself that is not necessarily an advantage, but its large community guarantees an very good documentation, and for many problems high-quality solutions are found readily on user forums. Python is used in probably any scientific domain, and may have many third party extension freely available in that domain. It is available with a lot scientific Python packages on all VSC -clusters. Several high-quality Integrated Development Environments (IDEs) are freely available for Python: e.g. PyCharm , VS Code , which at the same time provide support for C/C++/Fortran. Although Python in itself is a rather slow language, as we will see, there are many ways to cope with performance bottlenecks in Python. The quality of Python is embodied it its design principles \"The Zen of Python\" which are printed when you import this . Many of these also apply to our strategy for developing research software. > python >>> import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! In many ways, Python gently pushes you in the right direction, providing a pleasant programming experience. As an alternative on could consider Julia . Following a different programming paradigm, called multiple dispatch , its learning curve is probably a bit steeper and it has a smaller community. However, it is more performant than Python. On the other hand its programming productivity might not be that of Python. An other alternative could be Matlab . Although it has a long history of scientific computing, from as a language is it not as well designed as Python, and it is not fit for HPC. Moreover, it is a commercial product, and the community using it is not as willing to share its solutions as for Python, and there are much less extensions available. Often, you will need to solve the problem yourself, and, as a consequence, proceed at a slower pace. Starting out in a low-level programming language (C/C++/Fortran) is certainly a bad idea. Even if you are an experienced programmer you will proceed slower. The productivity of Python is easily 10 times larger than for C/C++/Fortran because: the advantages coming with an interpreted language, vs. a compiled language, the fact that Python is a very well designed, expressive and readable language with a rather flat learning curve, the availability of a large standard library and a wide range of domain-specific third party extensions, as well as the ease with which these are enabled in your code. Hence, this course sticks to Python.","title":"Start out in a high level language"},{"location":"chapter-5/#principle-2","text":"","title":"Principle 2"},{"location":"chapter-5/#start-out-as-simple-as-possible","text":"Take a small toy problem, the smallest you can think of that still represents the problem that you want to solve. There is a famous quote, attributed to Einstein ( although it seems he formulated it differently ) \" Make everything as simple as possible, but not simpler \". That applies very well here. The simpler the problem you start with, the faster you will build experience and understanding of the problem. Ideally, take a problem with a known analytical solution. Look for something you can easily visualise. Four atoms are easier to visualise than a hundred. Visualisation is a perfect way for obtaining insight (pun intended!) in your problem. Choose the simplest algorithm that will do the trick, don't bother about performance. Once it works correctly, use it as a reference case for validating improvements. Here are some interesting Python modules for visualisation: matplotlib bokeh plotly seaborn","title":"Start out as simple as possible"},{"location":"chapter-5/#principle-3","text":"","title":"Principle 3"},{"location":"chapter-5/#test-and-validate-your-code-changes","text":"In view of these amazing facts on the abundance of bugs in code there seems to be little chance that a programmer writes 100 lines of code without bugs. On average 7 bugs are to be expected in every 100 lines of code. Probably not all these bugs affect the outcome of the program, but in research code the outcome is of course crucial. How can we ensure that our code is correct, and remains so as we continue to work on it? The answer is unit-tests. Unit-testing are pieces of test-code together with verified outcomes. In view of the abundancy of bus it is best to test small pieces of code, in the order of 10 lines. Test code is also code and thus can contain bugs as well. The amount of test code for a system can be large. An example is probaly the best way to demonstrate the concept. In chapter 4 we discussed the case study Monte Carlo ground state energy calculation of a small atom cluster and a small project wetppr/mcgse where the original study is repeated with a Morse potential, described by the formula: V(r) = D_e(1 - e^{-\\alpha(r-r_e)})^2","title":"Test and validate your code (changes)"},{"location":"chapter-5/#todo-fix-the-link","text":"The code for this function is found in wetppr/mcgse/ init .py . Note that we provided default unit values for all the parameters: import numpy as np def morse_potential(r: float, D_e: float = 1, alpha: float = 1, r_e: float = 1) -> float: \"\"\"Compute the Morse potential for interatomic distance r. This is better than it looks, we can pass a numpy array for r, and it will use numpy array arithmetic to evaluate the expression for the array. Args: r: interatomic distance D_e: depth of the potential well, default = 1 alpha: width of the potential well, default = 1 r_e: location of the potential well, default = 1 \"\"\" return D_e * (1 - np.exp(-alpha*(r - r_e)))**2 The implementation of the function comprises only one line. How can we test its correctness? One approach would be to list a few r-values for which we know the outcome. E.g. V(r_e) = 0 . Here is a test funtion for it. Note that from math import isclose def test_morse_potential_at_r_e(): # the value at r_e=1 is 0 r = 1 Vr = mcgse.morse_potential(x) Vr_expected = 0 assert isclose(Vr, Vr_expected, rel_tol=1e-15) Because the function is using floating point arithmetic, the outcome could be subject to roundoff error. We account for a relative error of 1e-15 . When running the test, an AssertionError will be raised whenever the relative error is larger than 1e-15 . When it comes to testing functions it is practical to focus on mathematical properties of the function (fixing parameters to unity). E.g. 0 \\le V(r) for r \\in ]0,1] , 0 \\le V(r) < 1 for r \\in [1,+\\infty[ , V(r) is monotonously decreasing on ]0,1] , V(r) is monotonously increasing on [1,+\\infty[ , V''(r) is positive on ]0,1] , V''(r) is positive on ]1,r_i] , r_i=1 - \\log(1/2) being the inflection point, V''(r) is negative on ]r_i, +\\infty[ , ... See tests/wetppr/mcgse/test_mcgse.py for details. The file contains many more tests for other functions in the file wetppr/mcgse/__init__.py .","title":"TODO: ## fix the link"},{"location":"chapter-5/#automating-tests","text":"As soon as you have a few tests, running them manually after every code change, becomes impractical. We need to automate running the tests. Several automated test runners are available. Pytest is a good choice. When given a directory as a parameter, it will import all test_*.py files under it, look for methods starting with test , execute them and produce a concise report about which tests pass and which tests fail. Details about test discovery are found here . > cd path/to/wetppr > pytest tests ==================================== test session starts ===================================== platform darwin -- Python 3.9.5, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 rootdir: /Users/etijskens/software/dev/workspace/wetppr plugins: typeguard-2.13.3, mpi-0.5, anyio-3.6.2 collected 6 items tests/wetppr/mcgse/test_mcgse.py ..... [100%] ===================================== 5 passed in 1.65s ====================================== As is clear from the output above it found 5 tests in tests/wetppr/mcgse/test_mcgse.py , all executed successfully. In the presence of errors it is instructive to run pytest with -v -s options. For every new method added to your code, add some tests and run pytest . For every code change run all the tests again. Make sure they pass before you continue to improve or extend the code.","title":"Automating tests"},{"location":"chapter-5/#debugging-a-failing-test","text":"Debugging is the process of stepping through a program, executing it line by line, and examining the results in order to find the source of the error. IDEs like PyCharm and VS Code provide a most friendly debugging experience, but you can also use the Python debugging module pdb : Debugging a failing test is often the best way to investigate the source of the error. Python has an interesting idiom that allows a module to be run as a script, i.e. execute a module . To that end you put this code snippet at the end of a module: # # module code here # if __name__ == \"__main__\": # # script code here. # print(\"-*# finished #*-\") # if this line doesn't show up, something went wrong. The idiom has no common name :-(, we will refer to it as the if __name__ == \"__main__\": idiom. If a module file is imported as in import module_name , only the module code is executed, and the body of the if statement if __name__ == \"__main__\": is ignored, because the import statement sets the value of the __name__ variable to the module name. Thus, the condition evaluates to False . Most of the module code will consist of def and class statements, defining methods (the Python term for a function) and classes. When Python executes a def or a class statement, it interprets the code of the method or class and registers them under their respective names, so that they can be called by the module or script that imported the module. If the module file is executed on the command line, as in the command python module_name.py or in an IDE, the python executable sets the __name__ variable to \"__main__\" . Thus, the condition evaluates to True and its body is executed. This Python idiom provides us with a practical approach for executing a failing test. Assume that by running pytest tests we find out that there is an error in the test function test_morse_potential_mathematical_properties : > pytest tests ==================================== test session starts ===================================== platform darwin -- Python 3.9.5, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 rootdir: /Users/etijskens/software/dev/workspace/wetppr plugins: typeguard-2.13.3, mpi-0.5, anyio-3.6.2 collected 6 items tests/wetppr/test_wetppr.py . [ 16%] tests/wetppr/mcgse/test_mcgse.py .F... [100%] ========================================== FAILURES ========================================== ________________________ test_morse_potential_mathematical_properties ________________________ ... ================================== short test summary info =================================== FAILED tests/wetppr/mcgse/test_mcgse.py::test_morse_potential_mathematical_properties - ass... ================================ 1 failed, 5 passed in 4.27s ================================= At the end of the test module tests/wetppr/mcgse/test_mcgse.py , which contains the failing test test_morse_potential_mathematical_properties you will see this code snippet: # # test functions here # if __name__ == \"__main__\": the_test_you_want_to_debug = test_energy print(\"__main__ running\", the_test_you_want_to_debug) the_test_you_want_to_debug() print(\"-*# finished #*-\") When we run pytest tests , pytest imports the test module, setting __name__ to \"test_mcgse\" . Hence the condition __name__ == \"__main__\" evaluates to False and its body is not executed. Only when the module is executed the body will be executed too. Since we want to run the test_morse_potential_mathematical_properties function, change the first line in the body to: the_test_you_want_to_debug = test_morse_potential_mathematical_properties The variable the_test_you_want_to_debug now is an alias for the test function that we want to debug, test_morse_potential_mathematical_properties . The next statement is a print statement producing something like: __main__ running <function test_morse_potential_mathematical_properties at 0x11098c310> which ensures us that we called the right test function. The test function is then called in the next statement through its alias the_test_you_want_to_debug : the_test_you_want_to_debug() To debug the test using pdb execute: > python -m pdb tests/wetppr/mcgse/test_mcgse.py","title":"Debugging a failing test"},{"location":"chapter-5/#quick-and-dirty-testing","text":"The if __name__ == \"__main__\": idiom has another interesting application. When working on small projects, with only a few functions, sometimes we don't want to set up a tests directory with a test_small_module.py file. Instead we can use the if __name__ == \"__main__\": idiom to write a quick and dirty test inside the module, or just call the function to check by running or debugging the module that it does the right thing. In this way your work is restricted to a single file, and there is no need to switch between the test file and the module file.","title":"Quick and dirty testing"},{"location":"chapter-5/#principle-4","text":"","title":"Principle 4"},{"location":"chapter-5/#improve-and-extend-the-code","text":"Once the code for the simple problem you started with (see Principle 2 ) is tested, validated, and understood, you can gradually add complexity, approaching the true problem you need to solve. This may require improvements to the code because the time to solution will probably increase. Possible improvements are: better algorithms with lower computational complexity, better data structures, facilitating good data access patterns, common sense optimisations , increasing the flexibility of the code, restructuring the code, gradually increase the complexity of the problem. Obviously, continue to test and validate all code changes and extensions.","title":"Improve and extend the code"},{"location":"chapter-5/#principle-5","text":"","title":"Principle 5"},{"location":"chapter-5/#optimise-if-necessary","text":"If neccessary, optimise your code. Necessary is when the time to solution is too long. At this point you will probably already have a considerable Python code base. Bearing in mind that premature optimisation is the root of all evil , the important question is what needs optimisation and what not. The answer is provided by using a profiler. Profiler are tools that tell you how much time your program spends in the different parts of it and how many times tha part is executed. Some profilers provide information on a per function basis, and tell how many times that function was called and the average or cumulative time spent in it. Other profilers provide information on a per line basis, an tell how many times that line was executed and the average or cumulative time spent on it. This article provides the necessary details. You should start profiling on a per function basis and then profile the function(s) consuming most time with a line-by-line profiler. If it turns out that only a few lines of the function are responsible for the runtime consumption, split them off in a new function and try optimising the new function using one of the techniques discussed below. If not, try optimising the whole function with the very same techniques. Tip After optimising a (split-off) function, run the profiler again to check the performance.","title":"Optimise if necessary"},{"location":"chapter-5/#techniques-for-optimising-performance","text":"Below a series of optimisation techniques is presented, ordered by development effort needed.","title":"Techniques for optimising performance"},{"location":"chapter-5/#use-python-modules-built-for-hpc","text":"There exist excellent Python modules that provide very performant high level data structures and operations. Using them can make a dramatic difference. E.g. the difference in performance between Python lists and Numpy arrays can easily be a factor 100. By using them you do not only gain performance. Once you have learned how to use a Python module, such as Numpy, development time is also reduced as you no longer have to code the low-level logic yourself. Here are a few interesting scientific Python modules: Numpy : n-dimensional arrays, mathematical functions, linear algebra, ... SciPy : fundamental algorithms for scientific computing... sympy : symbolic computation pandas : data analysis","title":"Use Python modules built for HPC"},{"location":"chapter-5/#numba","text":"Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python. It takes your Python code, transforms it into C code, compiles it (on the fly) and calls the compiled C code version. Here is how a Numba-optimized function, taking a Numpy array as argument, might look like: import numba @numba.jit def sum2d(arr): M, N = arr.shape result = 0.0 for i in range(M): for j in range(N): result += arr[i,j] return result the @numba.jit decorator instructs the Python interpreter to use the numba just-in-time compiler (jit) to translate the Python code int C. compile it, and use the compiled verson on every call. As pure Python loops are quite expensive and the function has a double Python loop over 'i' and 'j' the performance gain is considerable. As in principle it only involves adding a decorator (and possibly some hinting at data types) using numba can be a really quick win. Note The above function can also defined as np.sum(np.sum(arr)) , using numpy function calls exclusively. That might be even faster. Tip Numba is sensitive to data types. Changing the data type of the arguments can improve the situation a lot.","title":"Numba"},{"location":"chapter-5/#developing-your-own-modules-in-cfortran","text":"If none of the above techniques helps, you might consider to develop your own Python modules compiled from C++ or Fortran code, also known as binary Python extensions. This can be a real game changer. In fact, this is exactly what the HPC Python modules like Numpy and SciPy do, and they do it because that is the way to harness the power of modern CPUs and expose it to Python. Obviously, this requires good knowledge of C++ or Fortran, and good understanding of performance critical characteristics fo modern CPU architecture. The application micc2 facilitates building your C++ and Fortran modules. Checkout its tutorials .","title":"Developing your own modules in C++/Fortran"},{"location":"chapter-5/#principle-6","text":"","title":"Principle 6"},{"location":"chapter-5/#parallelise-if-necessary","text":"If necessary, parallelise your code. Parallelisation is necessary is when the time to solution is still too long after paying attention to principle 4 and principle 5 , or if the research problem does not fit in the memory of a single machine. In that case it is advisable to optimise ( principle 5 ) anyway to not waste cycles on an expensive supercomputer. To parallelise Python projects these tools come in handy: dask : multi-core and multi-node parallellisation mpi4py : MPI parallellisation with python","title":"Parallelise if necessary"},{"location":"chapter-6/","text":"Chapter 6 - Tools for parallellization These tools can be used for parallellizing a problem on the cluster ( e.g. Vaughan), but also on your local machine if you install MPI, mpi4py and dask_mpi. To execute a python script script.py in parallel on Vaughan, you must execute a job script with the command srun python script.py <script parameters> This will automatically run the script with the requested resources. For more information about submitting job scripts see Submitting jobs on Vaughan . To run the python script in parallel on 4 CPUs on your local machine you must run this commmand in a terminal: mpirun -np 4 python script.py Mpi4py Mpi4py is a Python package that wraps the popular MPI framework for message passing between processes. It is very flexible, but typically requires quite a bit of work to set up all the communication correctly. Here are some useful links: Mpi4py documentation . The mpi4py documents the Python wrappers for MPI functions, not the wrapped functions themselves. The MPI documentation is therefor also useful. A github repo with som simple mpi4py examples . Some hints for setting up a master-slave configuration of cpus: MPI master slave Get master to do work in a task farm Note On your local machine you must first install a MPI library for your OS, before you can pip install mpi4py . Dask-MPI The Dask-MPI project makes it easy to deploy Dask from within an existing MPI environment, such as one created with the common MPI command-line launchers mpirun or mpiexec. Such environments are commonly found in high performance supercomputers, academic research institutions, and other clusters where MPI has already been installed. The dask documentation is here . Dask futures provide a simple approach to distribute the computation of a function for a list of arguments over a number of workers and gather the results. The approach reserves one cpu for the scheduler (rank 0) and one for the client script (rank 1). All remaining ranks are dask workers. Here is a typical example that computes the square of each item in an iterable using an arbitrary number of dask workers: from mpi4py import MPI from dask_mpi import initialize initialize() # Create a Client object from distributed import Client client = Client() def square(i): result = i*i # a print statement, just to see which worker is computing which result: print(f'rank {MPI.COMM_WORLD.Get_rank()}: {i=}**2 = {result}') # Note that the order of the output of the print statements does NOT correspond to the execution order. return result if __name__ == \"__main__\": # Apply the square method to 0, 1, ..., 999 using the available workers futures = client.map(square, range(1000)) # client.map() returns immediately after the work is distributed. # Typically, this is well before the actual work itself is finished. # Gather the results results = client.gather(futures) # client.gather() can only return after all the work is done, obviously. print('-*# finished #*-') Note At the time of writing, dask is not installed as an LMOD module on the cluster. So you must install it yourself. Make sure you first source the wetppr-env.sh script mentioned in LMOD modules . To install dask_mpi , run: > . path/to/wetppr-env.sh # to set $PYTHONUSERBASE > python -m pip install --user dask_mpi --upgrade > python -m pip install --user dask distributed --upgrade Note This installs dask_mpi , dask and dask.distributed into the directory specified by $PYTHONUSERBASE . This environment variable must also be available to the job. If $PYTHONUSERBASE is not set in your ~/.bashrc file, you must set it in the job script. Note Dask-MPI builds on mpi4py, so, MPI and mpi4py need to be available in your environment.","title":"Chapter 6 - Tools for parallellization"},{"location":"chapter-6/#chapter-6-tools-for-parallellization","text":"These tools can be used for parallellizing a problem on the cluster ( e.g. Vaughan), but also on your local machine if you install MPI, mpi4py and dask_mpi. To execute a python script script.py in parallel on Vaughan, you must execute a job script with the command srun python script.py <script parameters> This will automatically run the script with the requested resources. For more information about submitting job scripts see Submitting jobs on Vaughan . To run the python script in parallel on 4 CPUs on your local machine you must run this commmand in a terminal: mpirun -np 4 python script.py","title":"Chapter 6 - Tools for parallellization"},{"location":"chapter-6/#mpi4py","text":"Mpi4py is a Python package that wraps the popular MPI framework for message passing between processes. It is very flexible, but typically requires quite a bit of work to set up all the communication correctly. Here are some useful links: Mpi4py documentation . The mpi4py documents the Python wrappers for MPI functions, not the wrapped functions themselves. The MPI documentation is therefor also useful. A github repo with som simple mpi4py examples . Some hints for setting up a master-slave configuration of cpus: MPI master slave Get master to do work in a task farm Note On your local machine you must first install a MPI library for your OS, before you can pip install mpi4py .","title":"Mpi4py"},{"location":"chapter-6/#dask-mpi","text":"The Dask-MPI project makes it easy to deploy Dask from within an existing MPI environment, such as one created with the common MPI command-line launchers mpirun or mpiexec. Such environments are commonly found in high performance supercomputers, academic research institutions, and other clusters where MPI has already been installed. The dask documentation is here . Dask futures provide a simple approach to distribute the computation of a function for a list of arguments over a number of workers and gather the results. The approach reserves one cpu for the scheduler (rank 0) and one for the client script (rank 1). All remaining ranks are dask workers. Here is a typical example that computes the square of each item in an iterable using an arbitrary number of dask workers: from mpi4py import MPI from dask_mpi import initialize initialize() # Create a Client object from distributed import Client client = Client() def square(i): result = i*i # a print statement, just to see which worker is computing which result: print(f'rank {MPI.COMM_WORLD.Get_rank()}: {i=}**2 = {result}') # Note that the order of the output of the print statements does NOT correspond to the execution order. return result if __name__ == \"__main__\": # Apply the square method to 0, 1, ..., 999 using the available workers futures = client.map(square, range(1000)) # client.map() returns immediately after the work is distributed. # Typically, this is well before the actual work itself is finished. # Gather the results results = client.gather(futures) # client.gather() can only return after all the work is done, obviously. print('-*# finished #*-') Note At the time of writing, dask is not installed as an LMOD module on the cluster. So you must install it yourself. Make sure you first source the wetppr-env.sh script mentioned in LMOD modules . To install dask_mpi , run: > . path/to/wetppr-env.sh # to set $PYTHONUSERBASE > python -m pip install --user dask_mpi --upgrade > python -m pip install --user dask distributed --upgrade Note This installs dask_mpi , dask and dask.distributed into the directory specified by $PYTHONUSERBASE . This environment variable must also be available to the job. If $PYTHONUSERBASE is not set in your ~/.bashrc file, you must set it in the job script. Note Dask-MPI builds on mpi4py, so, MPI and mpi4py need to be available in your environment.","title":"Dask-MPI"},{"location":"course-text/","text":"Chapter 1 - Introduction Chapter 2 - Aspects of modern CPU architecture Chapter 3 - Optimise first, then parallelize Chapter 4 - Case studies Chapter 5 - A strategy for the development research software Chapter 6 - Tools for parallellization","title":"Course text"},{"location":"evaluation/","text":"Evaluation of this course In this course you will be learning by doing. You will be given an assignment, a (parallel) programming task on which you will work for several weeks, under my supervision and with my support. The exam consists of a presentation of your project work (usually in the last week of the course) in which you must explain the problems you encountered, explain your approach, provide performance measurements for the different versions your code, and for different node counts, explain the performance measurements, tell me what you found difficult during this course. During the presentation I will ask some questions, mainly because I am curious and eager to learn something, but also to ensure that you understand what you present. Assignment Here is this year's assignment . Use of VSC clusters for the assignment Students of the course 2000wetppr must use one of the VSC-clusters for the project work. Check out Access to VSC infrastructure and set up of your environment . Guide lines Learning by doing The assignment is there because imho programming is something you can only learn by doing . It involves important skills that you should develop while working on the assignment: Using the background information presented in chapters 1-4 Reason about the mathematical formulation of the problem and the algorithm to solve it, Do research on the problem, with respect to solution algorithms and implementation issues. Write and debug code in Python. Learn how slow Python functions can be sped up by converting them to either C++ or Fortran. Run your code on one of the UAntwerp HPC clusters. Learning is an incremental process. Especially for scientific software development the following is a good approach: Try, and test (We'll see what testing exactly means). Fail (often, the faster you fail, the faster you learn ! ). Think and do research ( Google - or any other good search engine, for that matter - is your best friend), and come up with an improvement. This is the hardest part, it requires intelligence and creativity. Iterate , i.e. restart at 1., until you no more fail and are satisfied with the solution. Document your itinerary . Document your classes, functions, variables, and keep track of the documents that guided you to solving the problems encountered. When you will look at your work three months (only!) after you left it as is, you will wonder what it was all about if you didn't document it. Although this approach may look as if you are supposed to find the solution to the problem in books or on the World Wide Web , this does not at all exclude creativity. Learning about how other researchers approached a problem, can easily spark new ideas that get you going. The fail fast, fail often principle also urges you to start as simple and small as possible and make incremental changes . Don't write a lot of code before you try and test. Typically, and this is corroborated by research, one bug is introduced with every new 10 lines. Finding 10 bugs in 100 lines is a lot more difficult than finding one bug in 10 lines (although sometimes there is more than one bug :( ). Anecdotical evidence A PhD student once asked me for support. He had written a ~10 000 line Fortran program (without tests). When he ran it, the results were not what he expected and he suspected that there was a bug 'somewhere'. He asked if I could help him find the bug and - no kidding - by the end of next week because the program had to go into production by then. I had to disappoint him and told him that he needed a true magician, which I am not. Obviously, the program was flawed in some sense, but other than a bug it might just as well be one or more of the issues below: The program may contain many bugs, which is very well possible in view of its size. The algorithm for solving the problem is inappropriate. There is an accuracy problem, related to the discretisation of time, space, or an insufficient set of basis functions for expanding the solution, or related to the finite precision of floating point numbers. (Floating point numbers are a processor's approximation of the real numbers, but they do have different mathematical properties. E.g. floating point addition is not commutative.) The mathematical formulation itself could be flawed or misunderstood. It is even possible that the program is correct but that the researcher's expectations are wrong. ... It could easily take weeks, if not months to write tests for the individual components to learn about the behaviour of the program and narrow down to the source of the error. For this reason a sound strategy for scientific software development that makes sure that the code you write has a sense of correctness is indispensable. Had the researcher come to me before he started programming this is the advice he would have been given: Tip Write 5 lines of code and write a test for them. Do not proceed (with the next 5 lines) before the test passes . Just 5, not 10! Your test code is also code and will initially contain bugs as well. As you get more experienced you may increase that number to 6, even 7, ... Admittedly, this advice is slightly biased to the conservative side, but I hope you get the point. You will be surprised how many mistakes you make, being a novice. But as you will discover the source of error soon, your progress will not come to a halt. Instead, you will learn fast and your progress will even speed up. You will be given practical tools to accomplish this. Caveat There is a small disadvantage to the approach we are following. It is biased towards bottom-up design . In bottom-up design your start from the details, gradually aggregating them into larger parts, and, eventually, into the final application. Its opponent is top-down design, in which you start with the a high-level formulation of the problem. This is then broken down in smaller components and gradually refined until all details are covered. With a bit of imagination it is, however, possible to write tests for a top-down approach by focussing on how the components work together, rather than on what they are doing. Top-down design is important because it forces you to think on the high-level structure of your application. This is paramount to how fast users will adopt your application, because it relates to user-friendly-ness, intuitive understanding, flexibility, ... In general, however, research scientists seem to better at ease with bottom-up thinking.","title":"Evaluation of this course"},{"location":"evaluation/#evaluation-of-this-course","text":"In this course you will be learning by doing. You will be given an assignment, a (parallel) programming task on which you will work for several weeks, under my supervision and with my support. The exam consists of a presentation of your project work (usually in the last week of the course) in which you must explain the problems you encountered, explain your approach, provide performance measurements for the different versions your code, and for different node counts, explain the performance measurements, tell me what you found difficult during this course. During the presentation I will ask some questions, mainly because I am curious and eager to learn something, but also to ensure that you understand what you present.","title":"Evaluation of this course"},{"location":"evaluation/#assignment","text":"Here is this year's assignment .","title":"Assignment"},{"location":"evaluation/#use-of-vsc-clusters-for-the-assignment","text":"Students of the course 2000wetppr must use one of the VSC-clusters for the project work. Check out Access to VSC infrastructure and set up of your environment .","title":"Use of VSC clusters for the assignment"},{"location":"evaluation/#guide-lines","text":"","title":"Guide lines"},{"location":"evaluation/#learning-by-doing","text":"The assignment is there because imho programming is something you can only learn by doing . It involves important skills that you should develop while working on the assignment: Using the background information presented in chapters 1-4 Reason about the mathematical formulation of the problem and the algorithm to solve it, Do research on the problem, with respect to solution algorithms and implementation issues. Write and debug code in Python. Learn how slow Python functions can be sped up by converting them to either C++ or Fortran. Run your code on one of the UAntwerp HPC clusters. Learning is an incremental process. Especially for scientific software development the following is a good approach: Try, and test (We'll see what testing exactly means). Fail (often, the faster you fail, the faster you learn ! ). Think and do research ( Google - or any other good search engine, for that matter - is your best friend), and come up with an improvement. This is the hardest part, it requires intelligence and creativity. Iterate , i.e. restart at 1., until you no more fail and are satisfied with the solution. Document your itinerary . Document your classes, functions, variables, and keep track of the documents that guided you to solving the problems encountered. When you will look at your work three months (only!) after you left it as is, you will wonder what it was all about if you didn't document it. Although this approach may look as if you are supposed to find the solution to the problem in books or on the World Wide Web , this does not at all exclude creativity. Learning about how other researchers approached a problem, can easily spark new ideas that get you going. The fail fast, fail often principle also urges you to start as simple and small as possible and make incremental changes . Don't write a lot of code before you try and test. Typically, and this is corroborated by research, one bug is introduced with every new 10 lines. Finding 10 bugs in 100 lines is a lot more difficult than finding one bug in 10 lines (although sometimes there is more than one bug :( ). Anecdotical evidence A PhD student once asked me for support. He had written a ~10 000 line Fortran program (without tests). When he ran it, the results were not what he expected and he suspected that there was a bug 'somewhere'. He asked if I could help him find the bug and - no kidding - by the end of next week because the program had to go into production by then. I had to disappoint him and told him that he needed a true magician, which I am not. Obviously, the program was flawed in some sense, but other than a bug it might just as well be one or more of the issues below: The program may contain many bugs, which is very well possible in view of its size. The algorithm for solving the problem is inappropriate. There is an accuracy problem, related to the discretisation of time, space, or an insufficient set of basis functions for expanding the solution, or related to the finite precision of floating point numbers. (Floating point numbers are a processor's approximation of the real numbers, but they do have different mathematical properties. E.g. floating point addition is not commutative.) The mathematical formulation itself could be flawed or misunderstood. It is even possible that the program is correct but that the researcher's expectations are wrong. ... It could easily take weeks, if not months to write tests for the individual components to learn about the behaviour of the program and narrow down to the source of the error. For this reason a sound strategy for scientific software development that makes sure that the code you write has a sense of correctness is indispensable. Had the researcher come to me before he started programming this is the advice he would have been given: Tip Write 5 lines of code and write a test for them. Do not proceed (with the next 5 lines) before the test passes . Just 5, not 10! Your test code is also code and will initially contain bugs as well. As you get more experienced you may increase that number to 6, even 7, ... Admittedly, this advice is slightly biased to the conservative side, but I hope you get the point. You will be surprised how many mistakes you make, being a novice. But as you will discover the source of error soon, your progress will not come to a halt. Instead, you will learn fast and your progress will even speed up. You will be given practical tools to accomplish this. Caveat There is a small disadvantage to the approach we are following. It is biased towards bottom-up design . In bottom-up design your start from the details, gradually aggregating them into larger parts, and, eventually, into the final application. Its opponent is top-down design, in which you start with the a high-level formulation of the problem. This is then broken down in smaller components and gradually refined until all details are covered. With a bit of imagination it is, however, possible to write tests for a top-down approach by focussing on how the components work together, rather than on what they are doing. Top-down design is important because it forces you to think on the high-level structure of your application. This is paramount to how fast users will adopt your application, because it relates to user-friendly-ness, intuitive understanding, flexibility, ... In general, however, research scientists seem to better at ease with bottom-up thinking.","title":"Learning by doing"},{"location":"exercise-1-solution/","text":"Exercise 1 - Getting started with wip 1. A simple Python module with a method to compute the dot product of two arrays Setting up a wip project We must first choose a name for our project, e.g. Dot . We create a Dot project in our workspace directory. We are assuming that our environment has Python and wip installed. > cd workspace > wip init Dot Project info needed: Enter a short description for the project: [<project_short_description>]: dot product implementations Enter the minimal Python version [3.9]: [[Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/project` ... ]] (done Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/project`) [[Creating a local git repo ... [[Running `git init --initial-branch=main` in project folder Dot ... Initialized empty Git repository in /Users/etijskens/software/dev/workspace/Dot/.git/ ]] (done Running `git init --initial-branch=main`) [[Running `git add *` in project folder Dot ... ]] (done Running `git add *`) [[Running `git add .gitignore` in project folder Dot ... ]] (done Running `git add .gitignore`) [[Running `git commit -m \"Initial commit from wip init Dot\"` in project folder Dot ... [main (root-commit) fbcb8a9] Initial commit from wip init Dot 7 files changed, 226 insertions(+) create mode 100644 .gitignore create mode 100644 CHANGELOG.md create mode 100644 README.md create mode 100644 dot/__init__.py create mode 100644 pyproject.toml create mode 100644 tests/dot/test_dot.py create mode 100644 wip-cookiecutter.json ]] (done Running `git commit -m \"Initial commit from wip init Dot\"`) ]] (done Creating a local git repo) After providing a short project description and accepting the default minimal Python version, wip sets up the project directory. We cd into the Dot project directory, because that's where the work will be done, and tools as wip and git look for file in the project directory. > cd Dot Tip By default wip creates a public remote GitHub repo. If you want it to be private add the flag --remote=private to the wip init command. If you do not want a remote repo, add --remote=none (not recommended). Wip has created quite a bit of files for us: > tree . \u251c\u2500\u2500 .bumpversion.cfg # coonfiguration file for bump2version (version management) \u251c\u2500\u2500 .git # The local git repo ((contents not shown) \u251c\u2500\u2500 .gitignore # a list of filename to be ignored by git \u251c\u2500\u2500 CHANGELOG.md # list your project changes here \u251c\u2500\u2500 README.md # users should look here to get started with the dot package \u251c\u2500\u2500 dot # the Python package we are creating \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 pyproject.toml # project configuration file used by poetry \u251c\u2500\u2500 tests # unit-tests go here \u2502 \u2514\u2500\u2500 dot \u2502 \u2514\u2500\u2500 test_dot.py \u2514\u2500\u2500 wip-cookiecutter.json # project configuration file used by wip itself+ The Python package dot is not empty. It has a working hello method that serves as an example. The tests/dot/test_dot.py contains some working tests for this hello method. In general, wip always creates files and components with working parts that show you how things are supposed to work and can be extended easily. A first dot product implementation Use your favourite editor to edit the file dot/__init__.py , remove the hello method and add a dot method that implements the mathmatical formula {a}\\cdot{b} = \\sum_{i=1}^{n}{a_i}{b_i} Note vscode is recommended, both for local and remote editing. It also offers a nice and intuitive run and debug environment. Here is a first implementation for the dot product of two arrays: # -*- coding: utf-8 -*- # File dot/__init__.py \"\"\" ## Python package dot Provides several implementations of the dot product of two arrays. \"\"\" __version__ = '0.0.0' def dot(a, b): \"\"\"Compute the dot product of a and b Args: a: array of numbers b: array of numbers, of same length as a Returns: the dot product of a and b \"\"\" d = 0 for i in range(len(a)): d += a[i] * b[i] return d Note that the dot method above is agnostic about the types of a and b . It only assumes the existence of an indexing operator [] . The implementation is also not foolproof. It assumes that the multiplication operator * is valid for all a[i] and b[i] , but it does not perform any validation on the arguments, neither can it assure that the return value is a number indeed, as promised by the doc-string. However flawed this implementation is, it is a start. Testing should expose the flaws, and we should feel urged to correct them. Unit testing Together with the package file dot/__init__.py , wip has also created a test file for it, tests/dot/test_dot.py . As a first simple test we could choose two arrays for which we can compute the dot product by hand easily and check that our dot method gives indeed the right result: # File tests/dot/test_dot.py from dot import dot def test_dot_aa(): a = [1, 2, 3] expected = 1 + 4 + 9 result = dot(a,a) assert result == expected The command pytest tests will discover ( learn how ) all tests in the tests directory, run them and report whether they succeeded or failed and in case of failure, provide some indication of what went wrong. > pytest tests ======================================== test session starts ======================================= platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 1 item tests/dot/test_dot.py . [100%] ========================================= 1 passed in 0.01s ======================================== The output shows that pytest discovered 1 test file, tests/dot/test_dot.py , with one test. Every successful test shows up as a . , a failed test as F . This test passed. If you want a more detailed output, you can pass in the -v option. Output from the test methods is generally suppressed by pytest . Pass in -s to not suppress the output. Obviously, our test tests only one specific case of the infinite set of possible arguments. Writing good tests is not easy. When dealing with mathematical concepts, like the dot product, it is practical to focus on mathematical properties. E.g. the dot product is commutative. Let's write a test that verifies commutativity. Rather that trying a single argument, we generate 1000 pairs of random arrays. We also choose a random array size in [0,20] for every pair. # File tests/dot/test_dot.py from dot import dot import random def test_dot_commutative(): # Fix the seed for the random number generator of module random. random.seed(0) # repeat the test 1000 times: for _ in range(1000): # choose a random array size n = random.randint(0,20) # generate two random arrays a = [random.random() for i in range(n)] b = [random.random() for i in range(n)] # test commutativity: ab = et_dot.dot(a,b) ba = et_dot.dot(b,a) assert ab == ba Let's run the tests: > pytest tests ======================================= test session starts ======================================== platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 2 items tests/dot/test_dot.py .. [100%] ========================================= 2 passed in 0.06s ======================================== This test passes too. Note that we fixed the seed of the random number generator. This way the test generates the same 1000 pairs of random arrays at every run. If, e.g. , pair 134 would fail the test, it will fail every time we run it. Hence, we can run it again in debug mode, and examine the cause of the failure. Without fixing the seed every run will generate a different sequence of array pairs, and we would lose the ability to reproduce a failure. Another property of the dot product is that the dot product with an array of zeroes yelds zero, and with an array of ones yields the sum of the other array. Here are the test methods: def test_a_zero(): # Fix the seed for the random number generator of module random. random.seed(0) # repeat the test 1000 times: for _ in range(1000): # choose a random array size n = random.randint(0,20) # generate two random arrays a = [random.random() for i in range(n)] zero = [0 for i in range(n)] # test commutativity: a_dot_zero = et_dot.dot(a, zero) assert a_dot_zero == 0 zero_dot_a = et_dot.dot(zero, a) assert zero_dot_a == 0 def test_a_one(): # Fix the seed for the random number generator of module random. random.seed(0) # repeat the test 1000 times: for _ in range(1000): # choose a random array size n = random.randint(0,20) # generate two random arrays one = [1 for i in range(n)] a = [random.random() for i in range(n)] sum_a = sum(a) # test a_dot_one = et_dot.dot(a, one) assert a_dot_one == sum_a one_dot_a = et_dot.dot(one, a) assert one_dot_a == sum_a Also these succeed: > pytest tests ======================================= test session starts ======================================== platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 4 items tests/dot/test_dot.py .... [100%] ======================================== 4 passed in 0.10s ========================================= As our tests pass, our confidence builds up. As mentioned before, our dot method does not do any validation on the arguments, although the doc-string says that the arguments are expected to be of the same length. >>> from dot import dot >>> help(dot) Help on function dot in module dot: dot(a, b) Compute the dot product of a and b. Args: a: array of numbers b: array of numbers, of same length as a Returns: the dot product of a and b Here's a test for commutativity of the dot method with two arrays of different length: def test_different_length(): a = [1, 2] b = [1, 1, 1] # test commutativity: ab = dot(a, b) ba = dot(b, a) assert ab == ba When run: > pytest tests ======================================= test session starts ======================================== platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 5 items tests/dot/test_dot.py ....F [100%] ============================================ FAILURES ============================================== ______________________________________ test_different_length _______________________________________ def test_different_length(): a = [1, 2] b = [1, 1, 1] # test commutativity: ab = dot(a, b) > ba = dot(b, a) tests/dot/test_dot.py:74: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ a = [1, 1, 1], b = [1, 2] def dot(a, b): \"\"\"Compute the dot product of a and b. Args: a: array of numbers b: array of numbers, of same length as a Returns: the dot product of a and b \"\"\" d = 0 for i in range(len(a)): > d += a[i] * b[i] E IndexError: list index out of range dot/__init__.py:21: IndexError ===================================== short test summary info ====================================== FAILED tests/dot/test_dot.py::test_different_length - IndexError: list index out of range =================================== 1 failed, 4 passed in 0.47s ==================================== This test fails. Inspection of the output reveals an IndexError when computing dot(b,a) . This is due to the dot method takes the loop length from the first argument, in this case 3. Since the second argument's length is 2 the last loop iteration ( i being equal to 2) fails. The mathematically inclined may argue that this is an improper use of the dot product as both arrays have different length, and, mathematically, the dot product of a and b is not defined in that case. This could be fixed by verifying that both arguments have the same length and raise an exception if that is the case. E.g. : def dot(a, b): \"\"\"Compute the dot product of a and b. Args: a: array of numbers b: array of numbers, of same length as a Returns: the dot product of a and b Raises: ValueError if len(a) != len(b) \"\"\" n = len(a) if len(b) != n: raise ValueError(\"Unequal array length.\") d = 0 for i in range(n): d += a[i] * b[i] return d Now the test will still fail, but instead of an IndexError a ValueError is raised as soon as the argument arrays have different sizes. This urges us to rewrite the test to assert that a ValueError is raised, because that is the expected behavior: def test_different_length(): a = [1, 2] b = [1, 1, 1] with pytest.raises(ValueError): ab = dot(a, b) with pytest.raises(ValueError): ba = dot(b, a) All tests succeed. It is important that we re-run all tests, not just the failing test. There is always a chance that changes to our implementation might fix one issue but create another. > pytest tests ======================================= test session starts ======================================== platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 5 items tests/dot/test_dot.py ..... [100%] ======================================== 5 passed in 0.09s ========================================= More pragmatic readers might argue that sometimes it may be useful that in case the two arrays have different length, the dot method returns a value corresponding to the shortest length of the arrays. The original test can be fixed then by modifying the implementation as follows: def dot(a, b): \"\"\"Compute the dot product of a and b. If one of the arrays is longer than the other, the extra entries are ignored. Args: a: array of numbers b: array of numbers Returns: the dot product of a and b \"\"\" n = min(len(a), len(b)) d = 0 for i in range(n): d += a[i] * b[i] return d Note that both approaches have their merits, but in the sake of clarity, it is important that the doc-string correctly explains what the method does. As shown above, useful tests can be based on mathematical properties. However, tests occasionally fail because our expectations are typically based on the behavior of real numbers. Generally, computers use single precision (SP) or double precision (DP) numbers, which have a finite precision. SP stores approximately 8 digits and DP approximately 16. The finite precision destroys some of the mathematical properties of real numbers we are so used to. E.g. what is the outcome of these dot products? \\left[ \\begin{array}{ccc} 1 & 1 & 1 \\end{array} \\right] \\cdot \\left[ \\begin{array}{ccc} 1e16 & 1 & -1e16 \\end{array} \\right] \\left[ \\begin{array}{ccc} 1 & 1 & 1 & 1 \\end{array} \\right] \\cdot \\left[ \\begin{array}{ccc} 0.1 & 0.1 & 0.1 & 1 & -0.3 \\end{array} \\right] Your expectations are, rightfully, 1e16 + 1 - 1e16 = 1 and 0.1 + 0.1 + 0.1 - 0.3 = 0 , respectively. However, surprisingly, Python does not agree: >>> from dot import dot >>> dot([1, 1, 1], [1e16, 1, -1e16]) 0 >>> dot([1, 1, 1, 1], [0.1, 0.1, 0.1, -0.3]) 5.551115123125783e-17 The default Python type for floating point numbers is float and is DP. This has nothing to do with our dot product implementation: >>> 1e16 + 1 - 1e16 0 >>> 0.1 + 0.1 + 0.1 - 0.3 5.551115123125783e-17 Things become a little clearer when dropping the subtraction: >>> 1e16 + 1 1e+16 >>> 0.1 + 0.1 + 0.1 0.30000000000000004 The outcome of 1e16 + 1 is not 1e16 + 1 but 1e16 . Because the 1 we are adding would be the 17th digit, which is beyond the precision of a Python float , adding it has no effect and we are effectively computing 1e16 - 1e16 . Similarly, 0.1 and 0.3 cannot be represented exactly as a float and the errors do not cancel out. This behavior can make a test fail surprisingly, so it is important to be aware. Note that, contrary to the real numbres thy are supposed to represent, the addition of floating point numbers is NOT commutative: >>> 1e16 + 1 - 1e16 0 >>> 1e16 - 1e16 + 1 1 >>> 0.1 + 0.1 + 0.1 - 0.3 5.551115123125783e-17 >>> 0.1 + 0.1 - 0.3 + 0.1 2.7755575615628914e-17 2. Timing your code Parallel programming is about getting the maximum performance out of the machine you are computing on. Whether that is your laptop, or a Tier-0 supercomputer does not matter. Timing your code is essential to evaluate its computational efficiency, and to understand the architectural features that influence performance. Timing means measuring how much time it takes to execute a part of the code. The Python standard library time has the perf_counter() method for this. Here is a typical example. from time import perf_counter t0 = perf_counter() # code to be timed t = perf_counter() - t0 print(f'This took {t} seconds') Here's a test function that performs the timing. The dot product computation is repeated a number of times in order to provide meaningfull results for small array sizes: from time import perf_counter def test_time(): n_repetitions = 10 print(f'\\ntest_time()') print(f'{n_repetitions=}') for n in [1_000, 10_000, 100_000, 1_000_000]: a = [random.random() for i in range(n)] b = [random.random() for i in range(n)] t0 = perf_counter() for _ in range(n_repetitions): a_dot_b = dot(a, b) seconds = (perf_counter() - t0) / n_repetitions print(f'{n=} dot(a, b) took {seconds}s') Here is the output: > pytest tests -s ======================================== test session starts ======================================= platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 6 items tests/dot/test_dot.py ..... test_time() n_repetitions=10 n=1000 dot(a, b) took 0.00026637590000000433s n=10000 dot(a, b) took 0.0022314830000000007s n=100000 dot(a, b) took 0.017176923499999996s n=1000000 dot(a, b) took 0.1444069864s . ======================================== 6 passed in 10.63s ======================================== 3.Using numba for speeding up your code Python is notoriously slow at executing loops. Moreover, the Python list is an extremely flexible data structure, mimicking an array (it has an indexing operator that takes an integer). Actually, it is and array of pointers, each of which may point to an arbitrary type. As a consequence, iterating over a Python list is extra slow because it does not use the cache optimally. A true array is a contiguous piece of memory where each entry has the same data type. Numpy arrays are extremly useful for scientific computing. The Numba [jit] decorator takes a Python method, translates the code into C andcompiles it. Together with a contiguous data structure this can achieve significant speedups. A Numba accelerated version of the dot method The standard way to create a Numba accelerated version of a method is to precede its definition with the @jit decorator: from numba import jit @jit def dot(a, b): ... The first time the dot method is called, it is compiled and then applied. All subsequent calls directly apply the compiled version. The decorator approach hides the original Python version. This is impractical if we want to compare the timings of both versions. We can keep both versions like this: # File dot/__init__.py from numba import jit def dot(a, b): ... jit_dot = jit(dot) If we want the Python version, we call dot.dot , the numba.jit accelerated version is called as dot.jit_dot . Although we did not change the Python implementation, it is wise to subject dot.jit_dot to the same tests as dot.dot . Let's add a timing function to our tests (using Numpy arrays instead of Python lists): import numpy as np def test_time_numba(): print(f'\\ntest_time_numba()') n_repetitions = 10 print(f'{n_repetitions=}') for n in [1_000, 10_000, 100_000, 1_000_000]: # a = [random.random() for i in range(n)] # b = [random.random() for i in range(n)] # python lists are replaced with numpy arrays a = np.random.random(n) b = np.random.random(n) t0 = perf_counter() for _ in range(n_repetitions): a_dot_b = jit_dot(a, b) seconds = (perf_counter() - t0) / n_repetitions print(f'{n=} dot(a, b) took {seconds}s') Here are the timings: > pytest tests -s ======================================== test session starts ======================================= platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 8 items tests/dot/test_dot.py ..... test_time() n_repetitions=10 n=1000 dot(a, b) took 9.876120000000821e-05s n=10000 dot(a, b) took 0.0012254463999999965s n=100000 dot(a, b) took 0.011126110799999988s n=1000000 dot(a, b) took 0.1173576513s . test_time_numba() n_repetitions=10 n=1000 dot(a, b) took 0.055231137799999885s n=10000 dot(a, b) took 1.419590000004689e-05s n=100000 dot(a, b) took 0.00015608759999992118s n=1000000 dot(a, b) took 0.0019177367999999363s . =================================== 8 passed, 1 warning in 10.99s ================================== The speedup for the largest arrays is 0.1174/0.001918 = 61.20. 4. Adding your own C++/Modern Fortran implementations Numba proves that low-level language implementations can be significantly faster than Pythons itself. It is not too hard to provide your own low-level implementations in C++ or Morern Fortran. Wip facilitates it greatly. C++ implementation First, add a C++ binary extension module to the project: > wip add cpp_impl --cpp [[Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/module-cpp` ... ]] (done Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/module-cpp`) [[Expanding cookiecutter template `cookiecutters/module-cpp-tests` ... ]] (done Expanding cookiecutter template `cookiecutters/module-cpp-tests`) Inspection of the project directory shows a cpp_impl subdirectory in the package directory dot as well as in the tests/dot directory: > tree . \u251c\u2500\u2500 CHANGELOG.md \u251c\u2500\u2500 README.md \u251c\u2500\u2500 dot \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 cpp_impl \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u251c\u2500\u2500 cpp_impl.cpp \u2502 \u2514\u2500\u2500 cpp_impl.md \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 tests \u2502 \u2514\u2500\u2500 dot \u2502 \u251c\u2500\u2500 cpp_impl \u2502 \u2502 \u2514\u2500\u2500 test_cpp_impl.py \u2502 \u2514\u2500\u2500 test_dot.py \u2514\u2500\u2500 wip-cookiecutter.json As usual the files added by wip contain working example code. Edit the file dot/cpp_impl/cpp_impl.cpp as below: /* * C++ source file for module dot.cpp_impl * File dot/cpp_impl/cpp_impl.cpp */ #include <nanobind/nanobind.h> #include <nanobind/ndarray.h> // add support for multi-dimensional arrays #include <string> namespace nb = nanobind; double dot ( nb::ndarray<double> a // in , nb::ndarray<double> b // in ) {// detect argument errors. if( a.ndim() != 1 ) throw std::domain_error(std::string(\"Argument 1 is not a 1D-array.\")); if( b.ndim() != 1 ) throw std::domain_error(std::string(\"Argument 2 is not a 1D-array.\")); size_t n = a.shape(0); if ( n != b.shape(0) ) throw std::domain_error(std::string(\"The arguments do not have the same length.\")); // we do not intend to modify a, nor b, hence declare const double const * a_ = a.data(); double const * b_ = b.data(); // do the actual work double d = 0; for(size_t i = 0; i < n; ++i) { d += a_[i] * b_[i]; } return d; } NB_MODULE(cpp_impl, m) { m.doc() = \"A binary python extension\"; m.def(\"dot\", &dot, \"Dot product of two float 1D arrays.\"); } Build the binary extension module: > wip build C++ [[Building C++ binary extension `dot/cpp_impl` ... [[Running `cmake -S . -B _cmake_build` in folder `dot/cpp_impl ... -- Configuring done -- Generating done -- Build files have been written to: /Users/etijskens/software/dev/workspace/Dot/dot/cpp_impl/_cmake_build ]] (done Running `cmake -S . -B _cmake_build`) [[Running `cmake --build _cmake_build` in folder `dot/cpp_impl ... Consolidate compiler generated dependencies of target nanobind-static [ 84%] Built target nanobind-static Consolidate compiler generated dependencies of target cpp_impl [ 92%] Building CXX object CMakeFiles/cpp_impl.dir/cpp_impl.cpp.o [100%] Linking CXX shared module cpp_impl.cpython-39-darwin.so [100%] Built target cpp_impl ]] (done Running `cmake --build _cmake_build`) [[Running `cmake --install _cmake_build` in folder `dot/cpp_impl ... -- Install configuration: \"Release\" -- Installing: /Users/etijskens/software/dev/workspace/Dot/dot/cpp_impl/../cpp_impl.cpython-39-darwin.so ]] (done Running `cmake --install _cmake_build`) ]] (done Building C++ binary extension `dot/cpp_impl`) The dot package directory now contains a dynamic library file cpp_impl.cpython-39-darwin.so (the extension depends on your OS and Python version) Add these line to the dot/__init__.py file to expose the new implementation: # Expose the C++ implementation as cpp_dot from dot.cpp_impl import dot as cpp_dot Here's a test function. Ideally, all tests devised for the initial Python implementation dot.dot would be repeated for doc.cpp_dot . # File tests/dot/cpp_impl/test_cpp_impl.py import numpy as np import dot def test_cpp_dot_one(): a = np.array([0,1,2,3,4],dtype=float) b = np.ones(a.shape,dtype=float) expected = np.sum(a) d = dot.cpp_dot(a, b) assert d == expected The test passes: > pytest tests/dot/cpp_impl/test_cpp_impl.py ======================================== test session starts ======================================= platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 1 item tests/dot/cpp_impl/test_cpp_impl.py . [100%] =================================== 1 passed, 1 warning in 0.81s =================================== Modern Fortran implementation First, add a Modern Fortran binary extension module to the project: > wip add f90_impl --f90 [[Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/module-f90` ... ]] (done Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/module-f90`) [[Expanding cookiecutter template `cookiecutters/module-f90-tests` ... ]] (done Expanding cookiecutter template `cookiecutters/module-f90-tests`) Inspection of the project directory shows a f90_impl subdirectory in the package directory dot as well as in the tests/dot directory: > tree . \u251c\u2500\u2500 CHANGELOG.md \u251c\u2500\u2500 README.md \u251c\u2500\u2500 dot \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 cpp_impl \u2502 \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u2502 \u251c\u2500\u2500 cpp_impl.cpp \u2502 \u2502 \u2514\u2500\u2500 cpp_impl.md \u2502 \u251c\u2500\u2500 cpp_impl.cpython-39-darwin.so \u2502 \u2514\u2500\u2500 f90_impl \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u251c\u2500\u2500 f90_impl.f90 \u2502 \u2514\u2500\u2500 f90_impl.md \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 tests \u2502 \u2514\u2500\u2500 dot \u2502 \u251c\u2500\u2500 cpp_impl \u2502 \u2502 \u2514\u2500\u2500 test_cpp_impl.py \u2502 \u251c\u2500\u2500 f90_impl \u2502 \u2502 \u2514\u2500\u2500 test_f90_impl.py \u2502 \u2514\u2500\u2500 test_dot.py \u2514\u2500\u2500 wip-cookiecutter.json Edit the file dot/f90_impl/f900_impl.f90 as below: real*8 function dot(a,b,n) ! Compute the dot product of two 1d arrays. ! implicit none !------------------------------------------------------------------------------------------------- integer*4 , intent(in) :: n real*8 , dimension(n), intent(in) :: a,b ! real*8 , intent(out) :: dot ! intent is inout because we do not want to return an array to avoid needless copying !------------------------------------------------------------------------------------------------- ! declare local variables integer*4 :: i !------------------------------------------------------------------------------------------------- dot = 0.0 do i=1,n dot = dot + a(i) * b(i) end do end function dot Add this line to the file dot/__init__.py to expose the fortran implementation: # File dot/__init__.py from f90_impl import dot as f90_dot Add a test method: # File tests/dot/cpp_impl/test_cpp_impl.py import numpy as np import dot def test_f90_dot_one(): a = np.array([0,1,2,3,4],dtype=float) b = np.ones(a.shape,dtype=float) expected = np.sum(a) d = dot.f90_dot(a, b) assert d == expected and run it: > pytest tests/dot/f90_impl/test_f90_impl.py ======================================== test session starts ======================================= platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 1 item tests/dot/f90_impl/test_f90_impl.py . [100%] ================================== 1 passed, 1 warning in 0.41s ==================================== 5. Comparison to Numpy's dot product implementation Let's add timing test methods for the C++ and Modern Fortran implementation, and for Numpy 's own dot product implementation . def test_time_cpp(): print(f'\\ntest_time_cpp()') n_repetitions = 10 print(f'{n_repetitions=}') for n in [1_000, 10_000, 100_000, 1_000_000]: # a = [random.random() for i in range(n)] # b = [random.random() for i in range(n)] a = np.random.random(n) b = np.random.random(n) t0 = perf_counter() for _ in range(n_repetitions): a_dot_b = cpp_dot(a, b) seconds = (perf_counter() - t0) / n_repetitions print(f'{n=} dot(a, b) took {seconds}s') def test_time_f90(): print(f'\\ntest_time_f90()') n_repetitions = 10 print(f'{n_repetitions=}') for n in [1_000, 10_000, 100_000, 1_000_000]: # a = [random.random() for i in range(n)] # b = [random.random() for i in range(n)] a = np.random.random(n) b = np.random.random(n) t0 = perf_counter() for _ in range(n_repetitions): a_dot_b = f90_dot(a, b) seconds = (perf_counter() - t0) / n_repetitions print(f'{n=} dot(a, b) took {seconds}s') def test_time_numpy(): print(f'\\ntest_time_numpy()') n_repetitions = 10 print(f'{n_repetitions=}') for n in [1_000, 10_000, 100_000, 1_000_000]: # a = [random.random() for i in range(n)] # b = [random.random() for i in range(n)] a = np.random.random(n) b = np.random.random(n) t0 = perf_counter() for _ in range(n_repetitions): a_dot_b = np.dot(a, b) seconds = (perf_counter() - t0) / n_repetitions print(f'{n=} dot(a, b) took {seconds}s') and run it: > pytest tests -s ... test_time_numba() n_repetitions=10 n=1000 dot(a, b) took 0.13340862129999992s n=10000 dot(a, b) took 1.2983299999902442e-05s n=100000 dot(a, b) took 0.0001236856999998537s n=1000000 dot(a, b) took 0.0016462048000001062s . test_time_cpp() n_repetitions=10 n=1000 dot(a, b) took 4.069380000011335e-05s n=10000 dot(a, b) took 1.2986199999787118e-05s n=100000 dot(a, b) took 0.00012309770000022978s n=1000000 dot(a, b) took 0.0014566837000000276s . test_time_f90() n_repetitions=10 n=1000 dot(a, b) took 1.8575000002130082e-06s n=10000 dot(a, b) took 1.2266699999941012e-05s n=100000 dot(a, b) took 0.00011799439999968798s n=1000000 dot(a, b) took 0.001525724200000056s . test_time_numpy() n_repetitions=10 n=1000 dot(a, b) took 0.00020460570000011558s n=10000 dot(a, b) took 5.809499999998025e-06s n=100000 dot(a, b) took 4.74010000001357e-05s n=1000000 dot(a, b) took 0.0006969873999999266s ... Note that the C++ and Modern Fortran implementations are only marginally faster than the numba.jit accelerated version, but that the numpy implementation is still 2.36 times faster. This is probably because when compiling the former no measures were taken to ensure vectorisation. When vectorisation is enabled, these implementations would perform similar to the numpy implementation. The moral of the story You won't beat implementations of HPC libraries like Numpy. Specialists have gone great lengths to ensure they squeeze out the last bit of performance out of your cpu (avoiding slow Python loops, using the cache efficiently, using SIMD vectorisation and shared memory parallellization). Numpy arrays perform better than Python lists because they are contiguous blocks of data items of the same numeric type. In a Python list each entry is actually a pointer that may point to whatever object type. When the entry is accessed, Python must first find out the data type of the object and interpret it, before it can act on the object. Python itself is bad at number crunching, especially if raw Python loops are involved. You can transform Python code into low-level code using Numba and obtain significant speedups, without much effort. Sometimes the wins are close to optimal, sometimes not. Much depends on the data structures used in the Python implementation. You can also provide your own number-crunching routines in C++ or Modern Fortran. This gives you much more control over performance than Numba and allows you to squeeze the last bit of performance out of your cpu, but this requires expertise in C++ and/or Modern Fortran and performance programming principles. In principle both C++ and Modern Fortran are capable of achieving the same performance. Wip greatly facilitates the creation of binary extension modules for use with Python. How to make sure all implementations pass the same tests This is a useful but somewhat advanced topic, illustrating using methods as objects and the concept of generator functions. If you are a Python novice, you might want to skip this section and move on to section 6 . So far, all we provided separate tests and timings for all our implementations. As there is 5 of them, this implies a lot of code duplication, which we would like to avoid. Also we might forget to fully test all implementations, and miss bugs as a consequence. How can we subject all implementations to the same tests, and the same timing tests? In the example above, we have 5 different implementations which can be applied to three different types of arrays. Let's put them in a list , IMPLEMENTATIONS (we use capitalized naming to indicate it is a global variable). import numpy as np from dot import * IMPLS = [ dot # the initial Python implementation , jit_dot # numba.jit(dot) , cpp_dot # the C++ implementation , f90_dot # the Modern Fortran implementation , np.dot # the Numpy implementation ] ARRAY_TYPES = [ list, np.array ] We can iterate over the list and apply the implementations to some arguments: a = ... b = ... for dot_impl in IMPLEMENTATIONS: print(f'{dot_impl(a, b)}=') If all goes well, this will print 5 times the dot product of a and b . Because the implementations are basically function pointers, their value is not very informative. We create a dict that maps the values to more descriptive strings and print them: IMPLEMENTATION_DESCRIPTIONS = { dot : 'python dot version', jit_dot: 'numba.jit()dot version', cpp_dot: 'C++ dot version', f90_dot: 'f90 dot version', npy_dot: 'numpy.dot version', } print(\"\\nList of implementations:\") print(\"\\nImplementation descriptions:\") for dot_impl, desc in IMPLEMENTATION_DESCRIPTIONS.items(): print(f'{str(dot_impl) : <44} {desc:>20}') print() The print statements yield the following output: Implementation descriptions: <function dot at 0x1085efd30> python dot implementation CPUDispatcher(<function dot at 0x1085efd30>) numba.jit() dot implementation <nanobind.nb_func object at 0x10b957540> C++ dot implementation <fortran dot> f90 dot implementation <function dot at 0x10794e280> numpy.dot implementation When we run the tests with pytest , a failing test will be reported printing the values of all variables in the context, including the implementation that failed (the value of dot_impl ), and we can look up its value in the table above. Additionally, these implementations can be applied to both Python list s and Numpy ndarray s, except for the C++ implementation cpp_impl , which can only handle Numpy array s. This complicates things. Either the tests must know the implementation and do something sensible when an implementation is presented a non-supported array type, or the code generating the arrays must avoid generating unsupported array type for each implementation. Preferentially, this should happen in a generic way, rather than as an ad hoc treatment. A generic approach means that the code has some way of discovering which array types are valid for which implementations, without hard coding any exceptional cases. This can be implemented using a dict listing the acceptable array types for each implementation: IMPLEMENTATION_ARRAY_TYPES = { dot : [np.ndarray, list] , jit_dot : [np.ndarray, list] , cpp_dot : [np.ndarray,] , f90_dot : [np.ndarray, list] , np.dot : [np.ndarray, list] } This approach can be easily extended, when new implementations or array types are added. Let's look at how the code can use these data structures. We want to automate two different things, tests, and timings. A test needs to be executed only once, whereas for timings it is often useful to repeat many times to obtain meaningful timings. Test methods thus have this signature: def test_it(dot_impl, a, b): \"\"\"Test some property of the dot product for arrays `a` and `b`, admissible array types, using dot product implemtentation `dot_impl`.\"\"\" Using the framework of tests/dot/test_dot_impls.py tests are as simple as: def test_dot_commutative(): \"\"\"The test driver for commutativity of dot product implementations\"\"\" # a locally defined method testing commutativity def assert_dot_commutative(dot_impl, a, b): \"\"\"The test itself\"\"\" ab = dot_impl(a, b) ba = dot_impl(b, a) assert ab == ba # run the test for all implementations and all array pairs generated by # _random_array_pair_generator _test(assert_dot_commutative , implementations=IMPLEMENTATIONS , array_pair_generator=_random_array_pair_generator(n=2) ) _test is a helper function defined in the same file. 6. Report Report all timings in and explain your obsservations in the README.md file. See The moral of the story above. The extension .md implies the Markdown file format allowing for simple text formatting instructions . GitHub understands MarkDown, and displays the formatted version when you check .md files on https://github.com. This course text is written entirely in MarkDown. 7. Storing your project files on GitHub Git is a Version Control system. One of the main advantages is that it is a backup of all the different versions of your project that you committed. In the terminal issue the command. > git status On branch main Your branch is up to date with 'origin/main'. Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git restore <file>...\" to discard changes in working directory) modified: README.md Untracked files: (use \"git add <file>...\" to include in what will be committed) docs/ mkdocs.yml no changes added to commit (use \"git add\" and/or \"git commit -a\") This tells us on which branch our local repo is, which files have changed, and which files are new, and therefor unknown to our git repo sofar. Files unknown to th git repo are called untracked . Note Which files are listed depends - obviously - on which files you modified since the last commit. When the project was created with wip init ... a first commit is automatically executed. Unless you explicitly requested no to create a remote repo, a push is also automatically executed. Tip It is useful to commit and push your work at every point where something was added and/or tested successfully. Tip If you are not the only developer on a project, or your project has many users, study the concept of git branches . Untracked files must be added before you can store them in the git repo. Adding a directory automatically add all files in it: > git add docs > git status On branch main Your branch is up to date with 'origin/main'. Changes to be committed: (use \"git restore --staged <file>...\" to unstage) new file: docs/api-reference.md new file: docs/index.md new file: docs/overview.md Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git restore <file>...\" to discard changes in working directory) modified: README.md Untracked files: (use \"git add <file>...\" to include in what will be committed) mkdocs.yml The docs/ files are now no longer untracked, but identified as new , but to be committed . > git add mkdocs.yml > git status On branch main Your branch is up to date with 'origin/main'. Changes to be committed: (use \"git restore --staged <file>...\" to unstage) new file: docs/api-reference.md new file: docs/index.md new file: docs/overview.md new file: mkdocs.yml Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git restore <file>...\" to discard changes in working directory) modified: README.md The file README.md ( modified ) can be added too, to make it to be committed , but since it is already known to the repo ( i.e. the previous version is already in the local repo), we can commit it right away using the -a flag. To store the to be committed files in the local repo we thus use the command: > git commit -a -m \"added documentation\" [main 11254ea] added documentation 5 files changed, 57 insertions(+) create mode 100644 docs/api-reference.md create mode 100644 docs/index.md create mode 100644 docs/overview.md create mode 100644 mkdocs.yml The -a flag tells git to add all modified files not staged for commit before actually committing. the -m \"added documentation\" flag specifies the commit message. In the remote GitHub repo each file will be labeled with the commit message of the latest commit in which the file was changed. When we ask for the status now, we see that there is nothing to commit, but that the local repo is one commit ahead (that we just executed) of the remote repo origin/main . That means that the changes of that last commit are in the local repo, but not yet in the remote GitHub repo. > git status On branch main Your branch is ahead of 'origin/main' by 1 commit. (use \"git push\" to publish your local commits) nothing to commit, working tree clean That is fixed by pushing: > git push Enumerating objects: 10, done. Counting objects: 100% (10/10), done. Delta compression using up to 8 threads Compressing objects: 100% (8/8), done. Writing objects: 100% (8/8), 1.44 KiB | 736.00 KiB/s, done. Total 8 (delta 1), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (1/1), completed with 1 local object. To https://github.com/etijskens/Dot.git 2024c98..11254ea main -> main > git status On branch main Your branch is up to date with 'origin/main'. nothing to commit, working tree clean","title":"Exercise 1 - Getting started with `wip`"},{"location":"exercise-1-solution/#exercise-1-getting-started-with-wip","text":"","title":"Exercise 1 - Getting started with wip"},{"location":"exercise-1-solution/#1-a-simple-python-module-with-a-method-to-compute-the-dot-product-of-two-arrays","text":"","title":"1. A simple Python module with a method to compute the dot product of two arrays"},{"location":"exercise-1-solution/#setting-up-a-wip-project","text":"We must first choose a name for our project, e.g. Dot . We create a Dot project in our workspace directory. We are assuming that our environment has Python and wip installed. > cd workspace > wip init Dot Project info needed: Enter a short description for the project: [<project_short_description>]: dot product implementations Enter the minimal Python version [3.9]: [[Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/project` ... ]] (done Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/project`) [[Creating a local git repo ... [[Running `git init --initial-branch=main` in project folder Dot ... Initialized empty Git repository in /Users/etijskens/software/dev/workspace/Dot/.git/ ]] (done Running `git init --initial-branch=main`) [[Running `git add *` in project folder Dot ... ]] (done Running `git add *`) [[Running `git add .gitignore` in project folder Dot ... ]] (done Running `git add .gitignore`) [[Running `git commit -m \"Initial commit from wip init Dot\"` in project folder Dot ... [main (root-commit) fbcb8a9] Initial commit from wip init Dot 7 files changed, 226 insertions(+) create mode 100644 .gitignore create mode 100644 CHANGELOG.md create mode 100644 README.md create mode 100644 dot/__init__.py create mode 100644 pyproject.toml create mode 100644 tests/dot/test_dot.py create mode 100644 wip-cookiecutter.json ]] (done Running `git commit -m \"Initial commit from wip init Dot\"`) ]] (done Creating a local git repo) After providing a short project description and accepting the default minimal Python version, wip sets up the project directory. We cd into the Dot project directory, because that's where the work will be done, and tools as wip and git look for file in the project directory. > cd Dot Tip By default wip creates a public remote GitHub repo. If you want it to be private add the flag --remote=private to the wip init command. If you do not want a remote repo, add --remote=none (not recommended). Wip has created quite a bit of files for us: > tree . \u251c\u2500\u2500 .bumpversion.cfg # coonfiguration file for bump2version (version management) \u251c\u2500\u2500 .git # The local git repo ((contents not shown) \u251c\u2500\u2500 .gitignore # a list of filename to be ignored by git \u251c\u2500\u2500 CHANGELOG.md # list your project changes here \u251c\u2500\u2500 README.md # users should look here to get started with the dot package \u251c\u2500\u2500 dot # the Python package we are creating \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 pyproject.toml # project configuration file used by poetry \u251c\u2500\u2500 tests # unit-tests go here \u2502 \u2514\u2500\u2500 dot \u2502 \u2514\u2500\u2500 test_dot.py \u2514\u2500\u2500 wip-cookiecutter.json # project configuration file used by wip itself+ The Python package dot is not empty. It has a working hello method that serves as an example. The tests/dot/test_dot.py contains some working tests for this hello method. In general, wip always creates files and components with working parts that show you how things are supposed to work and can be extended easily.","title":"Setting up a wip project"},{"location":"exercise-1-solution/#a-first-dot-product-implementation","text":"Use your favourite editor to edit the file dot/__init__.py , remove the hello method and add a dot method that implements the mathmatical formula {a}\\cdot{b} = \\sum_{i=1}^{n}{a_i}{b_i} Note vscode is recommended, both for local and remote editing. It also offers a nice and intuitive run and debug environment. Here is a first implementation for the dot product of two arrays: # -*- coding: utf-8 -*- # File dot/__init__.py \"\"\" ## Python package dot Provides several implementations of the dot product of two arrays. \"\"\" __version__ = '0.0.0' def dot(a, b): \"\"\"Compute the dot product of a and b Args: a: array of numbers b: array of numbers, of same length as a Returns: the dot product of a and b \"\"\" d = 0 for i in range(len(a)): d += a[i] * b[i] return d Note that the dot method above is agnostic about the types of a and b . It only assumes the existence of an indexing operator [] . The implementation is also not foolproof. It assumes that the multiplication operator * is valid for all a[i] and b[i] , but it does not perform any validation on the arguments, neither can it assure that the return value is a number indeed, as promised by the doc-string. However flawed this implementation is, it is a start. Testing should expose the flaws, and we should feel urged to correct them.","title":"A first dot product implementation"},{"location":"exercise-1-solution/#unit-testing","text":"Together with the package file dot/__init__.py , wip has also created a test file for it, tests/dot/test_dot.py . As a first simple test we could choose two arrays for which we can compute the dot product by hand easily and check that our dot method gives indeed the right result: # File tests/dot/test_dot.py from dot import dot def test_dot_aa(): a = [1, 2, 3] expected = 1 + 4 + 9 result = dot(a,a) assert result == expected The command pytest tests will discover ( learn how ) all tests in the tests directory, run them and report whether they succeeded or failed and in case of failure, provide some indication of what went wrong. > pytest tests ======================================== test session starts ======================================= platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 1 item tests/dot/test_dot.py . [100%] ========================================= 1 passed in 0.01s ======================================== The output shows that pytest discovered 1 test file, tests/dot/test_dot.py , with one test. Every successful test shows up as a . , a failed test as F . This test passed. If you want a more detailed output, you can pass in the -v option. Output from the test methods is generally suppressed by pytest . Pass in -s to not suppress the output. Obviously, our test tests only one specific case of the infinite set of possible arguments. Writing good tests is not easy. When dealing with mathematical concepts, like the dot product, it is practical to focus on mathematical properties. E.g. the dot product is commutative. Let's write a test that verifies commutativity. Rather that trying a single argument, we generate 1000 pairs of random arrays. We also choose a random array size in [0,20] for every pair. # File tests/dot/test_dot.py from dot import dot import random def test_dot_commutative(): # Fix the seed for the random number generator of module random. random.seed(0) # repeat the test 1000 times: for _ in range(1000): # choose a random array size n = random.randint(0,20) # generate two random arrays a = [random.random() for i in range(n)] b = [random.random() for i in range(n)] # test commutativity: ab = et_dot.dot(a,b) ba = et_dot.dot(b,a) assert ab == ba Let's run the tests: > pytest tests ======================================= test session starts ======================================== platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 2 items tests/dot/test_dot.py .. [100%] ========================================= 2 passed in 0.06s ======================================== This test passes too. Note that we fixed the seed of the random number generator. This way the test generates the same 1000 pairs of random arrays at every run. If, e.g. , pair 134 would fail the test, it will fail every time we run it. Hence, we can run it again in debug mode, and examine the cause of the failure. Without fixing the seed every run will generate a different sequence of array pairs, and we would lose the ability to reproduce a failure. Another property of the dot product is that the dot product with an array of zeroes yelds zero, and with an array of ones yields the sum of the other array. Here are the test methods: def test_a_zero(): # Fix the seed for the random number generator of module random. random.seed(0) # repeat the test 1000 times: for _ in range(1000): # choose a random array size n = random.randint(0,20) # generate two random arrays a = [random.random() for i in range(n)] zero = [0 for i in range(n)] # test commutativity: a_dot_zero = et_dot.dot(a, zero) assert a_dot_zero == 0 zero_dot_a = et_dot.dot(zero, a) assert zero_dot_a == 0 def test_a_one(): # Fix the seed for the random number generator of module random. random.seed(0) # repeat the test 1000 times: for _ in range(1000): # choose a random array size n = random.randint(0,20) # generate two random arrays one = [1 for i in range(n)] a = [random.random() for i in range(n)] sum_a = sum(a) # test a_dot_one = et_dot.dot(a, one) assert a_dot_one == sum_a one_dot_a = et_dot.dot(one, a) assert one_dot_a == sum_a Also these succeed: > pytest tests ======================================= test session starts ======================================== platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 4 items tests/dot/test_dot.py .... [100%] ======================================== 4 passed in 0.10s ========================================= As our tests pass, our confidence builds up. As mentioned before, our dot method does not do any validation on the arguments, although the doc-string says that the arguments are expected to be of the same length. >>> from dot import dot >>> help(dot) Help on function dot in module dot: dot(a, b) Compute the dot product of a and b. Args: a: array of numbers b: array of numbers, of same length as a Returns: the dot product of a and b Here's a test for commutativity of the dot method with two arrays of different length: def test_different_length(): a = [1, 2] b = [1, 1, 1] # test commutativity: ab = dot(a, b) ba = dot(b, a) assert ab == ba When run: > pytest tests ======================================= test session starts ======================================== platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 5 items tests/dot/test_dot.py ....F [100%] ============================================ FAILURES ============================================== ______________________________________ test_different_length _______________________________________ def test_different_length(): a = [1, 2] b = [1, 1, 1] # test commutativity: ab = dot(a, b) > ba = dot(b, a) tests/dot/test_dot.py:74: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ a = [1, 1, 1], b = [1, 2] def dot(a, b): \"\"\"Compute the dot product of a and b. Args: a: array of numbers b: array of numbers, of same length as a Returns: the dot product of a and b \"\"\" d = 0 for i in range(len(a)): > d += a[i] * b[i] E IndexError: list index out of range dot/__init__.py:21: IndexError ===================================== short test summary info ====================================== FAILED tests/dot/test_dot.py::test_different_length - IndexError: list index out of range =================================== 1 failed, 4 passed in 0.47s ==================================== This test fails. Inspection of the output reveals an IndexError when computing dot(b,a) . This is due to the dot method takes the loop length from the first argument, in this case 3. Since the second argument's length is 2 the last loop iteration ( i being equal to 2) fails. The mathematically inclined may argue that this is an improper use of the dot product as both arrays have different length, and, mathematically, the dot product of a and b is not defined in that case. This could be fixed by verifying that both arguments have the same length and raise an exception if that is the case. E.g. : def dot(a, b): \"\"\"Compute the dot product of a and b. Args: a: array of numbers b: array of numbers, of same length as a Returns: the dot product of a and b Raises: ValueError if len(a) != len(b) \"\"\" n = len(a) if len(b) != n: raise ValueError(\"Unequal array length.\") d = 0 for i in range(n): d += a[i] * b[i] return d Now the test will still fail, but instead of an IndexError a ValueError is raised as soon as the argument arrays have different sizes. This urges us to rewrite the test to assert that a ValueError is raised, because that is the expected behavior: def test_different_length(): a = [1, 2] b = [1, 1, 1] with pytest.raises(ValueError): ab = dot(a, b) with pytest.raises(ValueError): ba = dot(b, a) All tests succeed. It is important that we re-run all tests, not just the failing test. There is always a chance that changes to our implementation might fix one issue but create another. > pytest tests ======================================= test session starts ======================================== platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 5 items tests/dot/test_dot.py ..... [100%] ======================================== 5 passed in 0.09s ========================================= More pragmatic readers might argue that sometimes it may be useful that in case the two arrays have different length, the dot method returns a value corresponding to the shortest length of the arrays. The original test can be fixed then by modifying the implementation as follows: def dot(a, b): \"\"\"Compute the dot product of a and b. If one of the arrays is longer than the other, the extra entries are ignored. Args: a: array of numbers b: array of numbers Returns: the dot product of a and b \"\"\" n = min(len(a), len(b)) d = 0 for i in range(n): d += a[i] * b[i] return d Note that both approaches have their merits, but in the sake of clarity, it is important that the doc-string correctly explains what the method does. As shown above, useful tests can be based on mathematical properties. However, tests occasionally fail because our expectations are typically based on the behavior of real numbers. Generally, computers use single precision (SP) or double precision (DP) numbers, which have a finite precision. SP stores approximately 8 digits and DP approximately 16. The finite precision destroys some of the mathematical properties of real numbers we are so used to. E.g. what is the outcome of these dot products? \\left[ \\begin{array}{ccc} 1 & 1 & 1 \\end{array} \\right] \\cdot \\left[ \\begin{array}{ccc} 1e16 & 1 & -1e16 \\end{array} \\right] \\left[ \\begin{array}{ccc} 1 & 1 & 1 & 1 \\end{array} \\right] \\cdot \\left[ \\begin{array}{ccc} 0.1 & 0.1 & 0.1 & 1 & -0.3 \\end{array} \\right] Your expectations are, rightfully, 1e16 + 1 - 1e16 = 1 and 0.1 + 0.1 + 0.1 - 0.3 = 0 , respectively. However, surprisingly, Python does not agree: >>> from dot import dot >>> dot([1, 1, 1], [1e16, 1, -1e16]) 0 >>> dot([1, 1, 1, 1], [0.1, 0.1, 0.1, -0.3]) 5.551115123125783e-17 The default Python type for floating point numbers is float and is DP. This has nothing to do with our dot product implementation: >>> 1e16 + 1 - 1e16 0 >>> 0.1 + 0.1 + 0.1 - 0.3 5.551115123125783e-17 Things become a little clearer when dropping the subtraction: >>> 1e16 + 1 1e+16 >>> 0.1 + 0.1 + 0.1 0.30000000000000004 The outcome of 1e16 + 1 is not 1e16 + 1 but 1e16 . Because the 1 we are adding would be the 17th digit, which is beyond the precision of a Python float , adding it has no effect and we are effectively computing 1e16 - 1e16 . Similarly, 0.1 and 0.3 cannot be represented exactly as a float and the errors do not cancel out. This behavior can make a test fail surprisingly, so it is important to be aware. Note that, contrary to the real numbres thy are supposed to represent, the addition of floating point numbers is NOT commutative: >>> 1e16 + 1 - 1e16 0 >>> 1e16 - 1e16 + 1 1 >>> 0.1 + 0.1 + 0.1 - 0.3 5.551115123125783e-17 >>> 0.1 + 0.1 - 0.3 + 0.1 2.7755575615628914e-17","title":"Unit testing"},{"location":"exercise-1-solution/#2-timing-your-code","text":"Parallel programming is about getting the maximum performance out of the machine you are computing on. Whether that is your laptop, or a Tier-0 supercomputer does not matter. Timing your code is essential to evaluate its computational efficiency, and to understand the architectural features that influence performance. Timing means measuring how much time it takes to execute a part of the code. The Python standard library time has the perf_counter() method for this. Here is a typical example. from time import perf_counter t0 = perf_counter() # code to be timed t = perf_counter() - t0 print(f'This took {t} seconds') Here's a test function that performs the timing. The dot product computation is repeated a number of times in order to provide meaningfull results for small array sizes: from time import perf_counter def test_time(): n_repetitions = 10 print(f'\\ntest_time()') print(f'{n_repetitions=}') for n in [1_000, 10_000, 100_000, 1_000_000]: a = [random.random() for i in range(n)] b = [random.random() for i in range(n)] t0 = perf_counter() for _ in range(n_repetitions): a_dot_b = dot(a, b) seconds = (perf_counter() - t0) / n_repetitions print(f'{n=} dot(a, b) took {seconds}s') Here is the output: > pytest tests -s ======================================== test session starts ======================================= platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 6 items tests/dot/test_dot.py ..... test_time() n_repetitions=10 n=1000 dot(a, b) took 0.00026637590000000433s n=10000 dot(a, b) took 0.0022314830000000007s n=100000 dot(a, b) took 0.017176923499999996s n=1000000 dot(a, b) took 0.1444069864s . ======================================== 6 passed in 10.63s ========================================","title":"2. Timing your code"},{"location":"exercise-1-solution/#3using-numba-for-speeding-up-your-code","text":"Python is notoriously slow at executing loops. Moreover, the Python list is an extremely flexible data structure, mimicking an array (it has an indexing operator that takes an integer). Actually, it is and array of pointers, each of which may point to an arbitrary type. As a consequence, iterating over a Python list is extra slow because it does not use the cache optimally. A true array is a contiguous piece of memory where each entry has the same data type. Numpy arrays are extremly useful for scientific computing. The Numba [jit] decorator takes a Python method, translates the code into C andcompiles it. Together with a contiguous data structure this can achieve significant speedups.","title":"3.Using numba for speeding up your code"},{"location":"exercise-1-solution/#a-numba-accelerated-version-of-the-dot-method","text":"The standard way to create a Numba accelerated version of a method is to precede its definition with the @jit decorator: from numba import jit @jit def dot(a, b): ... The first time the dot method is called, it is compiled and then applied. All subsequent calls directly apply the compiled version. The decorator approach hides the original Python version. This is impractical if we want to compare the timings of both versions. We can keep both versions like this: # File dot/__init__.py from numba import jit def dot(a, b): ... jit_dot = jit(dot) If we want the Python version, we call dot.dot , the numba.jit accelerated version is called as dot.jit_dot . Although we did not change the Python implementation, it is wise to subject dot.jit_dot to the same tests as dot.dot . Let's add a timing function to our tests (using Numpy arrays instead of Python lists): import numpy as np def test_time_numba(): print(f'\\ntest_time_numba()') n_repetitions = 10 print(f'{n_repetitions=}') for n in [1_000, 10_000, 100_000, 1_000_000]: # a = [random.random() for i in range(n)] # b = [random.random() for i in range(n)] # python lists are replaced with numpy arrays a = np.random.random(n) b = np.random.random(n) t0 = perf_counter() for _ in range(n_repetitions): a_dot_b = jit_dot(a, b) seconds = (perf_counter() - t0) / n_repetitions print(f'{n=} dot(a, b) took {seconds}s') Here are the timings: > pytest tests -s ======================================== test session starts ======================================= platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 8 items tests/dot/test_dot.py ..... test_time() n_repetitions=10 n=1000 dot(a, b) took 9.876120000000821e-05s n=10000 dot(a, b) took 0.0012254463999999965s n=100000 dot(a, b) took 0.011126110799999988s n=1000000 dot(a, b) took 0.1173576513s . test_time_numba() n_repetitions=10 n=1000 dot(a, b) took 0.055231137799999885s n=10000 dot(a, b) took 1.419590000004689e-05s n=100000 dot(a, b) took 0.00015608759999992118s n=1000000 dot(a, b) took 0.0019177367999999363s . =================================== 8 passed, 1 warning in 10.99s ================================== The speedup for the largest arrays is 0.1174/0.001918 = 61.20.","title":"A Numba accelerated version of the dot method"},{"location":"exercise-1-solution/#4-adding-your-own-cmodern-fortran-implementations","text":"Numba proves that low-level language implementations can be significantly faster than Pythons itself. It is not too hard to provide your own low-level implementations in C++ or Morern Fortran. Wip facilitates it greatly.","title":"4. Adding your own C++/Modern Fortran implementations"},{"location":"exercise-1-solution/#c-implementation","text":"First, add a C++ binary extension module to the project: > wip add cpp_impl --cpp [[Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/module-cpp` ... ]] (done Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/module-cpp`) [[Expanding cookiecutter template `cookiecutters/module-cpp-tests` ... ]] (done Expanding cookiecutter template `cookiecutters/module-cpp-tests`) Inspection of the project directory shows a cpp_impl subdirectory in the package directory dot as well as in the tests/dot directory: > tree . \u251c\u2500\u2500 CHANGELOG.md \u251c\u2500\u2500 README.md \u251c\u2500\u2500 dot \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 cpp_impl \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u251c\u2500\u2500 cpp_impl.cpp \u2502 \u2514\u2500\u2500 cpp_impl.md \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 tests \u2502 \u2514\u2500\u2500 dot \u2502 \u251c\u2500\u2500 cpp_impl \u2502 \u2502 \u2514\u2500\u2500 test_cpp_impl.py \u2502 \u2514\u2500\u2500 test_dot.py \u2514\u2500\u2500 wip-cookiecutter.json As usual the files added by wip contain working example code. Edit the file dot/cpp_impl/cpp_impl.cpp as below: /* * C++ source file for module dot.cpp_impl * File dot/cpp_impl/cpp_impl.cpp */ #include <nanobind/nanobind.h> #include <nanobind/ndarray.h> // add support for multi-dimensional arrays #include <string> namespace nb = nanobind; double dot ( nb::ndarray<double> a // in , nb::ndarray<double> b // in ) {// detect argument errors. if( a.ndim() != 1 ) throw std::domain_error(std::string(\"Argument 1 is not a 1D-array.\")); if( b.ndim() != 1 ) throw std::domain_error(std::string(\"Argument 2 is not a 1D-array.\")); size_t n = a.shape(0); if ( n != b.shape(0) ) throw std::domain_error(std::string(\"The arguments do not have the same length.\")); // we do not intend to modify a, nor b, hence declare const double const * a_ = a.data(); double const * b_ = b.data(); // do the actual work double d = 0; for(size_t i = 0; i < n; ++i) { d += a_[i] * b_[i]; } return d; } NB_MODULE(cpp_impl, m) { m.doc() = \"A binary python extension\"; m.def(\"dot\", &dot, \"Dot product of two float 1D arrays.\"); } Build the binary extension module: > wip build C++ [[Building C++ binary extension `dot/cpp_impl` ... [[Running `cmake -S . -B _cmake_build` in folder `dot/cpp_impl ... -- Configuring done -- Generating done -- Build files have been written to: /Users/etijskens/software/dev/workspace/Dot/dot/cpp_impl/_cmake_build ]] (done Running `cmake -S . -B _cmake_build`) [[Running `cmake --build _cmake_build` in folder `dot/cpp_impl ... Consolidate compiler generated dependencies of target nanobind-static [ 84%] Built target nanobind-static Consolidate compiler generated dependencies of target cpp_impl [ 92%] Building CXX object CMakeFiles/cpp_impl.dir/cpp_impl.cpp.o [100%] Linking CXX shared module cpp_impl.cpython-39-darwin.so [100%] Built target cpp_impl ]] (done Running `cmake --build _cmake_build`) [[Running `cmake --install _cmake_build` in folder `dot/cpp_impl ... -- Install configuration: \"Release\" -- Installing: /Users/etijskens/software/dev/workspace/Dot/dot/cpp_impl/../cpp_impl.cpython-39-darwin.so ]] (done Running `cmake --install _cmake_build`) ]] (done Building C++ binary extension `dot/cpp_impl`) The dot package directory now contains a dynamic library file cpp_impl.cpython-39-darwin.so (the extension depends on your OS and Python version) Add these line to the dot/__init__.py file to expose the new implementation: # Expose the C++ implementation as cpp_dot from dot.cpp_impl import dot as cpp_dot Here's a test function. Ideally, all tests devised for the initial Python implementation dot.dot would be repeated for doc.cpp_dot . # File tests/dot/cpp_impl/test_cpp_impl.py import numpy as np import dot def test_cpp_dot_one(): a = np.array([0,1,2,3,4],dtype=float) b = np.ones(a.shape,dtype=float) expected = np.sum(a) d = dot.cpp_dot(a, b) assert d == expected The test passes: > pytest tests/dot/cpp_impl/test_cpp_impl.py ======================================== test session starts ======================================= platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 1 item tests/dot/cpp_impl/test_cpp_impl.py . [100%] =================================== 1 passed, 1 warning in 0.81s ===================================","title":"C++ implementation"},{"location":"exercise-1-solution/#modern-fortran-implementation","text":"First, add a Modern Fortran binary extension module to the project: > wip add f90_impl --f90 [[Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/module-f90` ... ]] (done Expanding cookiecutter template `/Users/etijskens/software/dev/workspace/wiptools/wiptools/cookiecutters/module-f90`) [[Expanding cookiecutter template `cookiecutters/module-f90-tests` ... ]] (done Expanding cookiecutter template `cookiecutters/module-f90-tests`) Inspection of the project directory shows a f90_impl subdirectory in the package directory dot as well as in the tests/dot directory: > tree . \u251c\u2500\u2500 CHANGELOG.md \u251c\u2500\u2500 README.md \u251c\u2500\u2500 dot \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 cpp_impl \u2502 \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u2502 \u251c\u2500\u2500 cpp_impl.cpp \u2502 \u2502 \u2514\u2500\u2500 cpp_impl.md \u2502 \u251c\u2500\u2500 cpp_impl.cpython-39-darwin.so \u2502 \u2514\u2500\u2500 f90_impl \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u251c\u2500\u2500 f90_impl.f90 \u2502 \u2514\u2500\u2500 f90_impl.md \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 tests \u2502 \u2514\u2500\u2500 dot \u2502 \u251c\u2500\u2500 cpp_impl \u2502 \u2502 \u2514\u2500\u2500 test_cpp_impl.py \u2502 \u251c\u2500\u2500 f90_impl \u2502 \u2502 \u2514\u2500\u2500 test_f90_impl.py \u2502 \u2514\u2500\u2500 test_dot.py \u2514\u2500\u2500 wip-cookiecutter.json Edit the file dot/f90_impl/f900_impl.f90 as below: real*8 function dot(a,b,n) ! Compute the dot product of two 1d arrays. ! implicit none !------------------------------------------------------------------------------------------------- integer*4 , intent(in) :: n real*8 , dimension(n), intent(in) :: a,b ! real*8 , intent(out) :: dot ! intent is inout because we do not want to return an array to avoid needless copying !------------------------------------------------------------------------------------------------- ! declare local variables integer*4 :: i !------------------------------------------------------------------------------------------------- dot = 0.0 do i=1,n dot = dot + a(i) * b(i) end do end function dot Add this line to the file dot/__init__.py to expose the fortran implementation: # File dot/__init__.py from f90_impl import dot as f90_dot Add a test method: # File tests/dot/cpp_impl/test_cpp_impl.py import numpy as np import dot def test_f90_dot_one(): a = np.array([0,1,2,3,4],dtype=float) b = np.ones(a.shape,dtype=float) expected = np.sum(a) d = dot.f90_dot(a, b) assert d == expected and run it: > pytest tests/dot/f90_impl/test_f90_impl.py ======================================== test session starts ======================================= platform darwin -- Python 3.9.5, pytest-7.4.0, pluggy-1.2.0 rootdir: /Users/etijskens/software/dev/workspace/Dot collected 1 item tests/dot/f90_impl/test_f90_impl.py . [100%] ================================== 1 passed, 1 warning in 0.41s ====================================","title":"Modern Fortran implementation"},{"location":"exercise-1-solution/#5-comparison-to-numpys-dot-product-implementation","text":"Let's add timing test methods for the C++ and Modern Fortran implementation, and for Numpy 's own dot product implementation . def test_time_cpp(): print(f'\\ntest_time_cpp()') n_repetitions = 10 print(f'{n_repetitions=}') for n in [1_000, 10_000, 100_000, 1_000_000]: # a = [random.random() for i in range(n)] # b = [random.random() for i in range(n)] a = np.random.random(n) b = np.random.random(n) t0 = perf_counter() for _ in range(n_repetitions): a_dot_b = cpp_dot(a, b) seconds = (perf_counter() - t0) / n_repetitions print(f'{n=} dot(a, b) took {seconds}s') def test_time_f90(): print(f'\\ntest_time_f90()') n_repetitions = 10 print(f'{n_repetitions=}') for n in [1_000, 10_000, 100_000, 1_000_000]: # a = [random.random() for i in range(n)] # b = [random.random() for i in range(n)] a = np.random.random(n) b = np.random.random(n) t0 = perf_counter() for _ in range(n_repetitions): a_dot_b = f90_dot(a, b) seconds = (perf_counter() - t0) / n_repetitions print(f'{n=} dot(a, b) took {seconds}s') def test_time_numpy(): print(f'\\ntest_time_numpy()') n_repetitions = 10 print(f'{n_repetitions=}') for n in [1_000, 10_000, 100_000, 1_000_000]: # a = [random.random() for i in range(n)] # b = [random.random() for i in range(n)] a = np.random.random(n) b = np.random.random(n) t0 = perf_counter() for _ in range(n_repetitions): a_dot_b = np.dot(a, b) seconds = (perf_counter() - t0) / n_repetitions print(f'{n=} dot(a, b) took {seconds}s') and run it: > pytest tests -s ... test_time_numba() n_repetitions=10 n=1000 dot(a, b) took 0.13340862129999992s n=10000 dot(a, b) took 1.2983299999902442e-05s n=100000 dot(a, b) took 0.0001236856999998537s n=1000000 dot(a, b) took 0.0016462048000001062s . test_time_cpp() n_repetitions=10 n=1000 dot(a, b) took 4.069380000011335e-05s n=10000 dot(a, b) took 1.2986199999787118e-05s n=100000 dot(a, b) took 0.00012309770000022978s n=1000000 dot(a, b) took 0.0014566837000000276s . test_time_f90() n_repetitions=10 n=1000 dot(a, b) took 1.8575000002130082e-06s n=10000 dot(a, b) took 1.2266699999941012e-05s n=100000 dot(a, b) took 0.00011799439999968798s n=1000000 dot(a, b) took 0.001525724200000056s . test_time_numpy() n_repetitions=10 n=1000 dot(a, b) took 0.00020460570000011558s n=10000 dot(a, b) took 5.809499999998025e-06s n=100000 dot(a, b) took 4.74010000001357e-05s n=1000000 dot(a, b) took 0.0006969873999999266s ... Note that the C++ and Modern Fortran implementations are only marginally faster than the numba.jit accelerated version, but that the numpy implementation is still 2.36 times faster. This is probably because when compiling the former no measures were taken to ensure vectorisation. When vectorisation is enabled, these implementations would perform similar to the numpy implementation.","title":"5. Comparison to Numpy's dot product implementation"},{"location":"exercise-1-solution/#the-moral-of-the-story","text":"You won't beat implementations of HPC libraries like Numpy. Specialists have gone great lengths to ensure they squeeze out the last bit of performance out of your cpu (avoiding slow Python loops, using the cache efficiently, using SIMD vectorisation and shared memory parallellization). Numpy arrays perform better than Python lists because they are contiguous blocks of data items of the same numeric type. In a Python list each entry is actually a pointer that may point to whatever object type. When the entry is accessed, Python must first find out the data type of the object and interpret it, before it can act on the object. Python itself is bad at number crunching, especially if raw Python loops are involved. You can transform Python code into low-level code using Numba and obtain significant speedups, without much effort. Sometimes the wins are close to optimal, sometimes not. Much depends on the data structures used in the Python implementation. You can also provide your own number-crunching routines in C++ or Modern Fortran. This gives you much more control over performance than Numba and allows you to squeeze the last bit of performance out of your cpu, but this requires expertise in C++ and/or Modern Fortran and performance programming principles. In principle both C++ and Modern Fortran are capable of achieving the same performance. Wip greatly facilitates the creation of binary extension modules for use with Python.","title":"The moral of the story"},{"location":"exercise-1-solution/#how-to-make-sure-all-implementations-pass-the-same-tests","text":"This is a useful but somewhat advanced topic, illustrating using methods as objects and the concept of generator functions. If you are a Python novice, you might want to skip this section and move on to section 6 . So far, all we provided separate tests and timings for all our implementations. As there is 5 of them, this implies a lot of code duplication, which we would like to avoid. Also we might forget to fully test all implementations, and miss bugs as a consequence. How can we subject all implementations to the same tests, and the same timing tests? In the example above, we have 5 different implementations which can be applied to three different types of arrays. Let's put them in a list , IMPLEMENTATIONS (we use capitalized naming to indicate it is a global variable). import numpy as np from dot import * IMPLS = [ dot # the initial Python implementation , jit_dot # numba.jit(dot) , cpp_dot # the C++ implementation , f90_dot # the Modern Fortran implementation , np.dot # the Numpy implementation ] ARRAY_TYPES = [ list, np.array ] We can iterate over the list and apply the implementations to some arguments: a = ... b = ... for dot_impl in IMPLEMENTATIONS: print(f'{dot_impl(a, b)}=') If all goes well, this will print 5 times the dot product of a and b . Because the implementations are basically function pointers, their value is not very informative. We create a dict that maps the values to more descriptive strings and print them: IMPLEMENTATION_DESCRIPTIONS = { dot : 'python dot version', jit_dot: 'numba.jit()dot version', cpp_dot: 'C++ dot version', f90_dot: 'f90 dot version', npy_dot: 'numpy.dot version', } print(\"\\nList of implementations:\") print(\"\\nImplementation descriptions:\") for dot_impl, desc in IMPLEMENTATION_DESCRIPTIONS.items(): print(f'{str(dot_impl) : <44} {desc:>20}') print() The print statements yield the following output: Implementation descriptions: <function dot at 0x1085efd30> python dot implementation CPUDispatcher(<function dot at 0x1085efd30>) numba.jit() dot implementation <nanobind.nb_func object at 0x10b957540> C++ dot implementation <fortran dot> f90 dot implementation <function dot at 0x10794e280> numpy.dot implementation When we run the tests with pytest , a failing test will be reported printing the values of all variables in the context, including the implementation that failed (the value of dot_impl ), and we can look up its value in the table above. Additionally, these implementations can be applied to both Python list s and Numpy ndarray s, except for the C++ implementation cpp_impl , which can only handle Numpy array s. This complicates things. Either the tests must know the implementation and do something sensible when an implementation is presented a non-supported array type, or the code generating the arrays must avoid generating unsupported array type for each implementation. Preferentially, this should happen in a generic way, rather than as an ad hoc treatment. A generic approach means that the code has some way of discovering which array types are valid for which implementations, without hard coding any exceptional cases. This can be implemented using a dict listing the acceptable array types for each implementation: IMPLEMENTATION_ARRAY_TYPES = { dot : [np.ndarray, list] , jit_dot : [np.ndarray, list] , cpp_dot : [np.ndarray,] , f90_dot : [np.ndarray, list] , np.dot : [np.ndarray, list] } This approach can be easily extended, when new implementations or array types are added. Let's look at how the code can use these data structures. We want to automate two different things, tests, and timings. A test needs to be executed only once, whereas for timings it is often useful to repeat many times to obtain meaningful timings. Test methods thus have this signature: def test_it(dot_impl, a, b): \"\"\"Test some property of the dot product for arrays `a` and `b`, admissible array types, using dot product implemtentation `dot_impl`.\"\"\" Using the framework of tests/dot/test_dot_impls.py tests are as simple as: def test_dot_commutative(): \"\"\"The test driver for commutativity of dot product implementations\"\"\" # a locally defined method testing commutativity def assert_dot_commutative(dot_impl, a, b): \"\"\"The test itself\"\"\" ab = dot_impl(a, b) ba = dot_impl(b, a) assert ab == ba # run the test for all implementations and all array pairs generated by # _random_array_pair_generator _test(assert_dot_commutative , implementations=IMPLEMENTATIONS , array_pair_generator=_random_array_pair_generator(n=2) ) _test is a helper function defined in the same file.","title":"How to make sure all implementations pass the same tests"},{"location":"exercise-1-solution/#6-report","text":"Report all timings in and explain your obsservations in the README.md file. See The moral of the story above. The extension .md implies the Markdown file format allowing for simple text formatting instructions . GitHub understands MarkDown, and displays the formatted version when you check .md files on https://github.com. This course text is written entirely in MarkDown.","title":"6. Report"},{"location":"exercise-1-solution/#7-storing-your-project-files-on-github","text":"Git is a Version Control system. One of the main advantages is that it is a backup of all the different versions of your project that you committed. In the terminal issue the command. > git status On branch main Your branch is up to date with 'origin/main'. Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git restore <file>...\" to discard changes in working directory) modified: README.md Untracked files: (use \"git add <file>...\" to include in what will be committed) docs/ mkdocs.yml no changes added to commit (use \"git add\" and/or \"git commit -a\") This tells us on which branch our local repo is, which files have changed, and which files are new, and therefor unknown to our git repo sofar. Files unknown to th git repo are called untracked . Note Which files are listed depends - obviously - on which files you modified since the last commit. When the project was created with wip init ... a first commit is automatically executed. Unless you explicitly requested no to create a remote repo, a push is also automatically executed. Tip It is useful to commit and push your work at every point where something was added and/or tested successfully. Tip If you are not the only developer on a project, or your project has many users, study the concept of git branches . Untracked files must be added before you can store them in the git repo. Adding a directory automatically add all files in it: > git add docs > git status On branch main Your branch is up to date with 'origin/main'. Changes to be committed: (use \"git restore --staged <file>...\" to unstage) new file: docs/api-reference.md new file: docs/index.md new file: docs/overview.md Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git restore <file>...\" to discard changes in working directory) modified: README.md Untracked files: (use \"git add <file>...\" to include in what will be committed) mkdocs.yml The docs/ files are now no longer untracked, but identified as new , but to be committed . > git add mkdocs.yml > git status On branch main Your branch is up to date with 'origin/main'. Changes to be committed: (use \"git restore --staged <file>...\" to unstage) new file: docs/api-reference.md new file: docs/index.md new file: docs/overview.md new file: mkdocs.yml Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git restore <file>...\" to discard changes in working directory) modified: README.md The file README.md ( modified ) can be added too, to make it to be committed , but since it is already known to the repo ( i.e. the previous version is already in the local repo), we can commit it right away using the -a flag. To store the to be committed files in the local repo we thus use the command: > git commit -a -m \"added documentation\" [main 11254ea] added documentation 5 files changed, 57 insertions(+) create mode 100644 docs/api-reference.md create mode 100644 docs/index.md create mode 100644 docs/overview.md create mode 100644 mkdocs.yml The -a flag tells git to add all modified files not staged for commit before actually committing. the -m \"added documentation\" flag specifies the commit message. In the remote GitHub repo each file will be labeled with the commit message of the latest commit in which the file was changed. When we ask for the status now, we see that there is nothing to commit, but that the local repo is one commit ahead (that we just executed) of the remote repo origin/main . That means that the changes of that last commit are in the local repo, but not yet in the remote GitHub repo. > git status On branch main Your branch is ahead of 'origin/main' by 1 commit. (use \"git push\" to publish your local commits) nothing to commit, working tree clean That is fixed by pushing: > git push Enumerating objects: 10, done. Counting objects: 100% (10/10), done. Delta compression using up to 8 threads Compressing objects: 100% (8/8), done. Writing objects: 100% (8/8), 1.44 KiB | 736.00 KiB/s, done. Total 8 (delta 1), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (1/1), completed with 1 local object. To https://github.com/etijskens/Dot.git 2024c98..11254ea main -> main > git status On branch main Your branch is up to date with 'origin/main'. nothing to commit, working tree clean","title":"7. Storing your project files on GitHub"},{"location":"exercise-1/","text":"Exercise 1 - Getting started with wip - Dot product implementations Preconditions You have Python 3.9 or higher on your system. You have a GitHub account . If you don't, create one here . You have a (classic) GitHub personal access token . If not, you can create a (classic) personal access token following these instructions Make sure you check at least the scopes repo and read:org . wiptools must be installed ( Installation instructions here ). Assignment This assignment must be carried out on your local system. Construct a Python module with function for the dot product \\sum_{i=1}^{n}{a_i}{b_i} of two arrays a and b . Start out with a Python implementation. Use a Python list for representing arrays. Time the Python implementation for a range of array lengths. Try improving the timings by using the numba @jit decorator. Tip: use Numpy's ndarray for storing arrays instead of a Python list . Try improving the code by providing your own C++ and/or Modern Fortran implementation. Again use Numpy arrays instead of Python list . Compare your implementations with numpy's own dot product numpy.dot . Report all timings and explain your observations in the README.md file. Store you project files on GitHub. Solution Although the dot product is an extremely simple concept, this exercise is more involved than one would expect. The solution also demonstrates our software development strategy developed in chapter 5 .","title":"Exercise 1 - Getting started with `wip` - Dot product implementations"},{"location":"exercise-1/#exercise-1-getting-started-with-wip-dot-product-implementations","text":"","title":"Exercise 1 - Getting started with wip - Dot product implementations"},{"location":"exercise-1/#preconditions","text":"You have Python 3.9 or higher on your system. You have a GitHub account . If you don't, create one here . You have a (classic) GitHub personal access token . If not, you can create a (classic) personal access token following these instructions Make sure you check at least the scopes repo and read:org . wiptools must be installed ( Installation instructions here ).","title":"Preconditions"},{"location":"exercise-1/#assignment","text":"This assignment must be carried out on your local system. Construct a Python module with function for the dot product \\sum_{i=1}^{n}{a_i}{b_i} of two arrays a and b . Start out with a Python implementation. Use a Python list for representing arrays. Time the Python implementation for a range of array lengths. Try improving the timings by using the numba @jit decorator. Tip: use Numpy's ndarray for storing arrays instead of a Python list . Try improving the code by providing your own C++ and/or Modern Fortran implementation. Again use Numpy arrays instead of Python list . Compare your implementations with numpy's own dot product numpy.dot . Report all timings and explain your observations in the README.md file. Store you project files on GitHub.","title":"Assignment"},{"location":"exercise-1/#solution","text":"Although the dot product is an extremely simple concept, this exercise is more involved than one would expect. The solution also demonstrates our software development strategy developed in chapter 5 .","title":"Solution"},{"location":"exercise-2-solution/","text":"Exercise 2 - Transfer your local project to a VSC cluster 1. Cloning a remote GitHub repo First, login on Vaughan (Check out the VSC documentation on this ). Logging in on Vaughan Open a terminal and execute the command: > ssh <your_vsc_id>@login-vaughan.hpc.uantwerpen.be Last login: Mon Aug 14 14:25:17 2023 from 81.164.139.192 -------------------------------------------------------------------- Welcome to VAUGHAN ! Vaughan specific links: https://docs.vscentrum.be/en/latest/antwerp/tier2_hardware/vaughan_hardware.html https://docs.vscentrum.be/en/latest/antwerp/SLURM_UAntwerp.html General links: https://docs.vscentrum.be/en/latest/antwerp/tier2_hardware.html https://docs.vscentrum.be https://www.uantwerpen.be/hpc Questions or problems? Do not hesitate and contact us: hpc@uantwerpen.be Happy computing! -------------------------------------------------------------------- Your quota is: file system /data/antwerpen using 95.77G of 250G space, 311062 of 500k files file system /user/antwerpen using 494.4M of 3G space, 2463 of 20k files file system /scratch/antwerpen using 382.1G of 2T space, 57803 of 250k files -------------------------------------------------------------------- > Which one of the (currently) two login nodes of Vaughan you will land on, is rather random. You can explicitly ask for a login node: > ssh <your_vsc_id>@login1-vaughan.hpc.uantwerpen.be or > ssh <your_vsc_id>@login2-vaughan.hpc.uantwerpen.be After logging in you will receive information about the last time you logged in some links with information information on your file quota on /data/antwerpen , /user/antwerpen and /scratch/antwerpen The latter is especially important. You have access to 3 file spaces: data, user (or home) and scratch. user file space is small and backed up. It is intended for configuration files. data file space is large, backed up, but has slower access than scratch. This is the place to: store your own Python projects install Python packages not installed on Vaughan by the VSC user support team scratch file space is large, fast, but not backed up. On each of these file spaces you can use a limited amount of disk space and a limited number of files. E.g. my data file space allows for 250G (97.55G occupied) and 500 000 files (I have already 311062 files). When you install your own software components the number of files rises quickly. When your file quota are exceeded for aa file space, you will still be able to login, but any proces that tries to create a file will fail. Sometimes the cause of failure is not immediately clear. E.g. a submitted job may fail when attempting to create an output file. Or, connecting to the cluster with vscode will fail because it installs a .vscode-server every time, and it cannot due to file quota eceeded . Tip Check the file quota message on every login. Cloning a GitHub repo As explained above, it is best to clone the remote repo on the data file space > cd $VSC_DATA > pwd /data/antwerpen/201/vsc20170 Make a workspace directory for all our projects > mkdir workspace > cd workspace > pwd /data/antwerpen/201/vsc20170/workspace Clone my Dot repo ( e.g. ): > git clone https://github.com/etijskens/Dot.git Cloning into 'Dot'... remote: Enumerating objects: 55, done. remote: Counting objects: 100% (55/55), done. remote: Compressing objects: 100% (34/34), done. remote: Total 55 (delta 15), reused 52 (delta 12), pack-reused 0 Receiving objects: 100% (55/55), 16.82 KiB | 1.29 MiB/s, done. Resolving deltas: 100% (15/15), done. To work with the project, cd into the project directory: > cd Dot > pwd /data/antwerpen/201/vsc20170/workspace/Dot As Dot is a wip project we must make Python available first: > module load Python > wip info Project : Dot: dot product implementations Version : 0.0.0 Package : dot GitHub repo: -- Home page : -- Location : /data/antwerpen/201/vsc20170/Dot docs format: Markdown 2. Repeat all the timings of Exercise 1 When you login on the cluster, you land on a login node , together with many other users that are logged in. That makes it a crowded place. Although you can run commands, commands taking longer than a minute, may need to share resources with other users, making it slower than on your personal machine. Timing tests are therefor not always reliable when run on a login node. The login node is your point of access for manipulating files and setting up the work, but the real number crunching is to happen on the compute nodes, of which there are far more than login nodes. Compute nodes must be reserved by you in a job script . The job script requests the resources needed for the job, sets up the system environment for the job, specifies the command(s) to be executed. Details are described in VSC infrastructure/Submitting jobs on Vaughan 3. Add the timings to the README.md file and explain your observations Compare the timings from the compute node with those on our own machine, Do you notice any differences?","title":"Exercise 2 - Transfer your local project to a VSC cluster"},{"location":"exercise-2-solution/#exercise-2-transfer-your-local-project-to-a-vsc-cluster","text":"","title":"Exercise 2 - Transfer your local project to a VSC cluster"},{"location":"exercise-2-solution/#1-cloning-a-remote-github-repo","text":"First, login on Vaughan (Check out the VSC documentation on this ).","title":"1. Cloning a remote GitHub repo"},{"location":"exercise-2-solution/#logging-in-on-vaughan","text":"Open a terminal and execute the command: > ssh <your_vsc_id>@login-vaughan.hpc.uantwerpen.be Last login: Mon Aug 14 14:25:17 2023 from 81.164.139.192 -------------------------------------------------------------------- Welcome to VAUGHAN ! Vaughan specific links: https://docs.vscentrum.be/en/latest/antwerp/tier2_hardware/vaughan_hardware.html https://docs.vscentrum.be/en/latest/antwerp/SLURM_UAntwerp.html General links: https://docs.vscentrum.be/en/latest/antwerp/tier2_hardware.html https://docs.vscentrum.be https://www.uantwerpen.be/hpc Questions or problems? Do not hesitate and contact us: hpc@uantwerpen.be Happy computing! -------------------------------------------------------------------- Your quota is: file system /data/antwerpen using 95.77G of 250G space, 311062 of 500k files file system /user/antwerpen using 494.4M of 3G space, 2463 of 20k files file system /scratch/antwerpen using 382.1G of 2T space, 57803 of 250k files -------------------------------------------------------------------- > Which one of the (currently) two login nodes of Vaughan you will land on, is rather random. You can explicitly ask for a login node: > ssh <your_vsc_id>@login1-vaughan.hpc.uantwerpen.be or > ssh <your_vsc_id>@login2-vaughan.hpc.uantwerpen.be After logging in you will receive information about the last time you logged in some links with information information on your file quota on /data/antwerpen , /user/antwerpen and /scratch/antwerpen The latter is especially important. You have access to 3 file spaces: data, user (or home) and scratch. user file space is small and backed up. It is intended for configuration files. data file space is large, backed up, but has slower access than scratch. This is the place to: store your own Python projects install Python packages not installed on Vaughan by the VSC user support team scratch file space is large, fast, but not backed up. On each of these file spaces you can use a limited amount of disk space and a limited number of files. E.g. my data file space allows for 250G (97.55G occupied) and 500 000 files (I have already 311062 files). When you install your own software components the number of files rises quickly. When your file quota are exceeded for aa file space, you will still be able to login, but any proces that tries to create a file will fail. Sometimes the cause of failure is not immediately clear. E.g. a submitted job may fail when attempting to create an output file. Or, connecting to the cluster with vscode will fail because it installs a .vscode-server every time, and it cannot due to file quota eceeded . Tip Check the file quota message on every login.","title":"Logging in on Vaughan"},{"location":"exercise-2-solution/#cloning-a-github-repo","text":"As explained above, it is best to clone the remote repo on the data file space > cd $VSC_DATA > pwd /data/antwerpen/201/vsc20170 Make a workspace directory for all our projects > mkdir workspace > cd workspace > pwd /data/antwerpen/201/vsc20170/workspace Clone my Dot repo ( e.g. ): > git clone https://github.com/etijskens/Dot.git Cloning into 'Dot'... remote: Enumerating objects: 55, done. remote: Counting objects: 100% (55/55), done. remote: Compressing objects: 100% (34/34), done. remote: Total 55 (delta 15), reused 52 (delta 12), pack-reused 0 Receiving objects: 100% (55/55), 16.82 KiB | 1.29 MiB/s, done. Resolving deltas: 100% (15/15), done. To work with the project, cd into the project directory: > cd Dot > pwd /data/antwerpen/201/vsc20170/workspace/Dot As Dot is a wip project we must make Python available first: > module load Python > wip info Project : Dot: dot product implementations Version : 0.0.0 Package : dot GitHub repo: -- Home page : -- Location : /data/antwerpen/201/vsc20170/Dot docs format: Markdown","title":"Cloning a GitHub repo"},{"location":"exercise-2-solution/#2-repeat-all-the-timings-of-exercise-1","text":"When you login on the cluster, you land on a login node , together with many other users that are logged in. That makes it a crowded place. Although you can run commands, commands taking longer than a minute, may need to share resources with other users, making it slower than on your personal machine. Timing tests are therefor not always reliable when run on a login node. The login node is your point of access for manipulating files and setting up the work, but the real number crunching is to happen on the compute nodes, of which there are far more than login nodes. Compute nodes must be reserved by you in a job script . The job script requests the resources needed for the job, sets up the system environment for the job, specifies the command(s) to be executed. Details are described in VSC infrastructure/Submitting jobs on Vaughan","title":"2. Repeat all the timings of Exercise 1"},{"location":"exercise-2-solution/#3-add-the-timings-to-the-readmemd-file-and-explain-your-observations","text":"Compare the timings from the compute node with those on our own machine, Do you notice any differences?","title":"3. Add the timings to the README.md file and explain your observations"},{"location":"exercise-2/","text":"Exercise 2 - Transfer your local project to a VSC cluster It is mandatory to make use of wip for this exercise. Make sure your worked is properly committed to your local git repo and pushed to your remote GitHub repo. make sure you give read and write access to the evaluator. (The write access is for feedback and for fixing problems) Preconditions You completed Exercise 1 - Getting started with wip . You have prepared your personal environment on the Vaughan VSC cluster as described in VSC infrastructure . Assignment Clone the remote GitHub repo created in Exercise 1 on the Vaughan cluster. Repeat all the timings of Exercise 1 on a compute node of Vaughan. Add the timings to the README.md file and explain your observations Solution See solution .","title":"Exercise 2 - Transfer your local project to a VSC cluster"},{"location":"exercise-2/#exercise-2-transfer-your-local-project-to-a-vsc-cluster","text":"It is mandatory to make use of wip for this exercise. Make sure your worked is properly committed to your local git repo and pushed to your remote GitHub repo. make sure you give read and write access to the evaluator. (The write access is for feedback and for fixing problems)","title":"Exercise 2 - Transfer your local project to a VSC cluster"},{"location":"exercise-2/#preconditions","text":"You completed Exercise 1 - Getting started with wip . You have prepared your personal environment on the Vaughan VSC cluster as described in VSC infrastructure .","title":"Preconditions"},{"location":"exercise-2/#assignment","text":"Clone the remote GitHub repo created in Exercise 1 on the Vaughan cluster. Repeat all the timings of Exercise 1 on a compute node of Vaughan. Add the timings to the README.md file and explain your observations","title":"Assignment"},{"location":"exercise-2/#solution","text":"See solution .","title":"Solution"},{"location":"exercise-3/","text":"Exercise 3 - The matrix-vector product This exercise is evaluated. It is mandatory to make use of wip for this exercise. Make sure that your worked is properly committed to your local git repo and pushed to your remote GitHub repo. that you give the evaluator read and write access to your project. (The write access is for feedback and for fixing problems). Preconditions You completed Exercise 1 - Getting started with wip . You have prepared your personal environment on the Vaughan VSC cluster as described in VSC infrastructure . Assignment Construct a Python module with a method for the matrix-vector product y = Ax , A_i = \\sum_{j=1}^{n}{A_{ij}x_j} . You can add the method to the Dot module package of Exercise 1 - Getting started with wip , or start a new wip package with the command wip init ... . Start out with a Python implementation and use Numpy array s for representing matrices and vectors throughout this exercise. Time the implementation for square matrices {n}\\cross{n} of double precision elements Let n vary from 16, 32, 64, ... until the total amount of data exceeds the size of L3 cache (which may depend on the processor of your loocal machine). A double precision number takes 16 bytes. Try improving the timings by using @numba.jit a C++ implementation a Modern Fortran implementation Compare your implementations with numpy's own matrix-vector product numpy.dot . Report all timings in the README.md file. Checkout this link for using MarkDown for simple text formatting. Explain your observations with respect to the timings in the README.md file.","title":"Exercise 3 - The matrix-vector product"},{"location":"exercise-3/#exercise-3-the-matrix-vector-product","text":"This exercise is evaluated. It is mandatory to make use of wip for this exercise. Make sure that your worked is properly committed to your local git repo and pushed to your remote GitHub repo. that you give the evaluator read and write access to your project. (The write access is for feedback and for fixing problems).","title":"Exercise 3 - The matrix-vector product"},{"location":"exercise-3/#preconditions","text":"You completed Exercise 1 - Getting started with wip . You have prepared your personal environment on the Vaughan VSC cluster as described in VSC infrastructure .","title":"Preconditions"},{"location":"exercise-3/#assignment","text":"Construct a Python module with a method for the matrix-vector product y = Ax , A_i = \\sum_{j=1}^{n}{A_{ij}x_j} . You can add the method to the Dot module package of Exercise 1 - Getting started with wip , or start a new wip package with the command wip init ... . Start out with a Python implementation and use Numpy array s for representing matrices and vectors throughout this exercise. Time the implementation for square matrices {n}\\cross{n} of double precision elements Let n vary from 16, 32, 64, ... until the total amount of data exceeds the size of L3 cache (which may depend on the processor of your loocal machine). A double precision number takes 16 bytes. Try improving the timings by using @numba.jit a C++ implementation a Modern Fortran implementation Compare your implementations with numpy's own matrix-vector product numpy.dot . Report all timings in the README.md file. Checkout this link for using MarkDown for simple text formatting. Explain your observations with respect to the timings in the README.md file.","title":"Assignment"},{"location":"exercises/","text":"Exercises Exercises 1-2 are meant learn the practical principles of setting up a project and developing code in Python, C++ and Modern Fortran, both on your local machine and the VSC clusters. Exercise 1 - Getting started with wip Exercise 2 - Deploying your project on a HPC cluster The remaining exercises are evaluated and must be delivered. Exercise 3 - The matrix-vector product","title":"Exercises"},{"location":"exercises/#exercises","text":"Exercises 1-2 are meant learn the practical principles of setting up a project and developing code in Python, C++ and Modern Fortran, both on your local machine and the VSC clusters. Exercise 1 - Getting started with wip Exercise 2 - Deploying your project on a HPC cluster The remaining exercises are evaluated and must be delivered. Exercise 3 - The matrix-vector product","title":"Exercises"},{"location":"glossary/","text":"Glossary Here is an alphabetical list of terms with links to where they are explained in the text. A address space anti-pattern array of structures B bandwidth limited bandwith saturated batch mode C Cache coherence cache coherent non-uniform memory architecture cache line cache-oblivious ccNUMA cell-based Verlet list construction code modernisation Code optimisations Common sense optimizations communication computational complexity computational intensity compute limited compute node compute nodes compute nodes D Debugging Distributed memory parallelization E embarrassingly parallel G GitHub account GitHub personal access token H hardware thread Hybrid memory parallelization I instruction pipelining interconnect J job script job script job script job L L1 cache L2 cache L3 cache LMOD modules local login nodes loop fusion M main memory master nodes memory bandwidth multi-core multi-processor N node NUMA P parallel program Partitioned Global Address Space pattern peak performance pipeline stalls process profiling Python 3.9 R registers remote resource manager S scheduler scheduler scheduler sequential serial shared memory parallelization short-ranged SIMD vectorisation software thread source Spatial locality structure of Arrays T Temporal locality thread tiling time to solution V Verlet list","title":"Glossary"},{"location":"glossary/#glossary","text":"Here is an alphabetical list of terms with links to where they are explained in the text.","title":"Glossary"},{"location":"glossary/#a","text":"address space anti-pattern array of structures","title":"A"},{"location":"glossary/#b","text":"bandwidth limited bandwith saturated batch mode","title":"B"},{"location":"glossary/#c","text":"Cache coherence cache coherent non-uniform memory architecture cache line cache-oblivious ccNUMA cell-based Verlet list construction code modernisation Code optimisations Common sense optimizations communication computational complexity computational intensity compute limited compute node compute nodes compute nodes","title":"C"},{"location":"glossary/#d","text":"Debugging Distributed memory parallelization","title":"D"},{"location":"glossary/#e","text":"embarrassingly parallel","title":"E"},{"location":"glossary/#g","text":"GitHub account GitHub personal access token","title":"G"},{"location":"glossary/#h","text":"hardware thread Hybrid memory parallelization","title":"H"},{"location":"glossary/#i","text":"instruction pipelining interconnect","title":"I"},{"location":"glossary/#j","text":"job script job script job script job","title":"J"},{"location":"glossary/#l","text":"L1 cache L2 cache L3 cache LMOD modules local login nodes loop fusion","title":"L"},{"location":"glossary/#m","text":"main memory master nodes memory bandwidth multi-core multi-processor","title":"M"},{"location":"glossary/#n","text":"node NUMA","title":"N"},{"location":"glossary/#p","text":"parallel program Partitioned Global Address Space pattern peak performance pipeline stalls process profiling Python 3.9","title":"P"},{"location":"glossary/#r","text":"registers remote resource manager","title":"R"},{"location":"glossary/#s","text":"scheduler scheduler scheduler sequential serial shared memory parallelization short-ranged SIMD vectorisation software thread source Spatial locality structure of Arrays","title":"S"},{"location":"glossary/#t","text":"Temporal locality thread tiling time to solution","title":"T"},{"location":"glossary/#v","text":"Verlet list","title":"V"},{"location":"links/","text":"Useful links C++ cplusplus.com cppreferencee.com Fortran fortran-lang.org Python python.org realpython.com Parallelization approaches OpenMP : C/C++/Fortran MPI : C/C++/Fortran mpi4py : Python, the documentation of mpi4py is not self-contained. It relies (silently) on the documentation of MPI. multiprocessing : Python concurrent.futures : Python dask : Python HPC Georg Hager's blog SC20 tutorial \u201cNode-Level Performance Engineering\u201d : This is all about \"When to parallelize, and what to do first \". Project management wiptools : managing your Python/C++/Fortran project. Wiptools facilitates creating new projects adding Python sub-modules, Python applications (CLIs), binary extension modules written in C++ and Fortran. automatically extracting documentation from the doc-strings of your files (html or pdf) (unit) testing ( pytest ) publishing the documentation on readthedocs publishing your code on the Python package index version management and control (on GitHub ). If you are not familiar with git, here is an excellent starting point. Other courses CodeRefinery teaches all the essential tools which are usually skipped in academic education . Especially useful is the Introduction to git . Hands-on Scientific Computing The Missing Semester of Your CS Education","title":"Useful links"},{"location":"links/#useful-links","text":"","title":"Useful links"},{"location":"links/#c","text":"cplusplus.com cppreferencee.com","title":"C++"},{"location":"links/#fortran","text":"fortran-lang.org","title":"Fortran"},{"location":"links/#python","text":"python.org realpython.com","title":"Python"},{"location":"links/#parallelization-approaches","text":"OpenMP : C/C++/Fortran MPI : C/C++/Fortran mpi4py : Python, the documentation of mpi4py is not self-contained. It relies (silently) on the documentation of MPI. multiprocessing : Python concurrent.futures : Python dask : Python","title":"Parallelization approaches"},{"location":"links/#hpc","text":"Georg Hager's blog SC20 tutorial \u201cNode-Level Performance Engineering\u201d : This is all about \"When to parallelize, and what to do first \".","title":"HPC"},{"location":"links/#project-management","text":"wiptools : managing your Python/C++/Fortran project. Wiptools facilitates creating new projects adding Python sub-modules, Python applications (CLIs), binary extension modules written in C++ and Fortran. automatically extracting documentation from the doc-strings of your files (html or pdf) (unit) testing ( pytest ) publishing the documentation on readthedocs publishing your code on the Python package index version management and control (on GitHub ). If you are not familiar with git, here is an excellent starting point.","title":"Project management"},{"location":"links/#other-courses","text":"CodeRefinery teaches all the essential tools which are usually skipped in academic education . Especially useful is the Introduction to git . Hands-on Scientific Computing The Missing Semester of Your CS Education","title":"Other courses"},{"location":"overview/","text":"Overview Goals of the course Parallel programming? What? Distribute work over many CPUs why? Shorter time to solution Bigger problems higher accuracy how? tools: OpenMP, MPI Principles and best practices A strategy for scientific software development Python + C++/Modern Fortran Performance aspects of modern CPU architecture and hierarchical memory A short introduction: Memory location matters for performance A very good talk about this topic, you need to see this: Scott Meyers on Cpu Caches and Why You Care","title":"Overview"},{"location":"overview/#overview","text":"","title":"Overview"},{"location":"overview/#goals-of-the-course","text":"Parallel programming? What? Distribute work over many CPUs why? Shorter time to solution Bigger problems higher accuracy how? tools: OpenMP, MPI Principles and best practices A strategy for scientific software development Python + C++/Modern Fortran","title":"Goals of the course"},{"location":"overview/#performance-aspects-of-modern-cpu-architecture-and-hierarchical-memory","text":"A short introduction: Memory location matters for performance A very good talk about this topic, you need to see this: Scott Meyers on Cpu Caches and Why You Care","title":"Performance aspects of modern CPU architecture and hierarchical memory"},{"location":"vsc-infrastructure/","text":"VSC infrastructure This document describes how to set up your environment for the project work of the 2000wetppr course on the Tier-2 cluster of the University of Antwerp, which is also a VSC cluster. With minor modifications this can be applied to all VSC clusters. In general, the text is also applicable to other HPC clusters, but the modifictions needed may be a bit more substantial. Note for students These topics are logically ordered. Make sure that you carry out all the tasks in the order described. A bit of terminology In this course we often use the terms local and remote . The term local refers to the physical machine you are working on, i.e. your desktop or laptop, or even a tablet or cell phone. On the other hand remote refers to a machine which is usually in some other place, and which you are accessing from your local machine through a network connection (usually the internet) with the remote machine. In this course the remote machine is the university's Tier-2 supercomputer, Vaughan , at the time of writing. The supercomputer, basically, consists of: One or more login nodes . When users logs in to the supercomputer, a connection is established between the user's local machine and a login node. This is where users execute simple, not computationally intensive tasks, e.g. : job preparation, organization of the workspace and projects, create input data, software development tasks, small tests, ... A collection of compute nodes , potentially many thousands. This is where computationally intensive tasks of the users are executed. Typically, the user has no direct connection to the compute nodes. One or more master nodes . The master node runs the resource manager and the scheduler > and is the connection between the login node and the compute nodes. The former keeps an eye on what the compute nodes are doing and whether they are ready to accept new computational tasks. The scheduler does the planning of that work. To carry out some computational work on the compute nodes, the user must send a request to the scheduler, describing the task, the environment in which that task must be executed, and the resources (number of nodes, memory, accelerators, ...) needed by the task. Such a request is called a job script and the task is referred to as a job . Preparing for the VSC infrastructure in 2000WETPPR Applying for a guest account Note This section is only for students of the course 2000wetppr . Students of the course 2000wetppr must apply for a guest account to access the university's HPC clusters, unless they already have a VSC account. The project work (see Evaluation ) requires access to one of the university's HPC clusters. To apply for a guest account, create a SSH public/private key pair (see below) and send it by e-mail to franky.backeljauw@uantwerpen.be with engelbert.tijskens@uantwerpen.be in cc. A guest account will subsequently be created for you. Applying for a VSC account Note This section is only for researchers of Flemish institutes . Researchers of Flemish research institutes can apply for a VSC account to get access to the VSC Tier-2 and Tier-1 supercomputers. See Getting access to VSC clusters . An ssh public/private key pair is also required. Creating an ssh public/private key pair An ssh public/private key pair in necessary for both a guest account (students) and a VSC account (researchers). A ssh public/private key pair is a way for secure access to a system through the Secure Shell protocol. They are basically two small files with matching numbers. You may think of the public key as a lock. Everyone is allowed to see the lock, but no one can open the lock without its key, which is the private part of the key pair. The public key (the lock) will be placed on a system you need access to, in this case the Tier-2 supercomputer of our university. To access to the supercomputer ( i.e. , to open the lock) from, say, your laptop, you need the private key to be stored on your laptop (or a USB memory stick) and pass it to the SSH protocol, which will verify that the private key and the public key match. If case they do, the SSH protocol will open the lock and grant you access to the machine. To create a ssh public/private key pair proceed as follows. Open a 'terminal'. On Windows The latest builds of Windows 10 and Windows 11 include a built-in SSH server and client that are based on OpenSSH. You can use the cmd prompt, powershell, or WSL (Windows subsystem for Linux) as a terminal. For older Windows versions, we recommend installing mobaxterm to generate a ssh public/private key pair. On Linux Most Linux distributions have a terminal application. MacOSX MacOSX comes with a build in Terminal.app . iTerm2 is a replacement for Terminal.app with many interesting extra features. Type the following command at the prompt: > ssh-keygen -t rsa -b 4096 You will then be prompted for a file location of the public and private key. You may accept the default location by entering. The default file location will look a bit different, depending on your OS. If the files already exist you can choose to overwrite them or to cancel the operation. You might want to change the filename of the key to a more meaningfull name, e.g. access_vaughan_rsa . Don't use blanks in the filename. Use hyphens ( - ) or underscores ( _ ) instead. Enter file in which to save the key (C:\\Users\\your_username/.ssh/id rsa) : C:\\Users\\your_username/.ssh/id rsa already exists. Overwrite (y/n)? y You will then be prompted for a passphrase (twice). A passphrase provides an extra level of protection in case somebody would steal your private key. Press enter for an empty passphrase. (Passphrases are a little annoying when using VS Code (see below) for remote development.) Enter passphrase (empty for no passphrase): Enter same passphrase again: Finally you will be notified of where the keys are stored: Your identification has been saved in C:\\Users\\your_username/.ssh/id rsa. Your public key has been saved in C:\\Users\\your_username/.ssh/id rsa.pub. For students of 2000wetppr To obtain a guest account, students must send their public key (and only the public key, the private key is, well, um, private ) to franky.backeljauw@uantwerpen.be with engelbert.tijskens@uantwerpen.be in cc . The public key is the one with the .pub extension. Accessing Vaughan Terminal based access Vaughan is (at the time of writing) the University of Antwerp's Tier-2 HPC cluster. For terminal based access you open a terminal (see above) and execute the command: > ssh -i path/to/my/private-ssh-key your-user-id@login1-vaughan.hpc.uantwerpen.be Last login: Mon Feb 27 12:40:32 2023 from 143.129.75.140 -------------------------------------------------------------------- Welcome to VAUGHAN ! ... Note If the key is in sub-directory .ssh of your home directory, the -i path/to/my/private-ssh-key can be omitted. The ssh command above, connects your terminal session to a login node (see above) of the Vaughan cluster. After the command is finished, you can use the terminal as if you were working on the login node. The current working directory will be a location in your file system on the cluster, rather than on your local machine. Vaughan has two login nodes, login1-vaughan.hpc.uantwerpen.be and login2-vaughan.hpc.uantwerpen.be . You can also use login-vaughan.hpc.uantwerpen.be . The system then will choose the login node with the highest availability. Ssh comes with a .ssh/config file that allows you to store the arguments of frequently used ssh commands. E.g. # file ~/.ssh/config Host vn1 HostName login1-vaughan.hpc.uantwerpen.be User vsc20170 IdentityFile /full/path/to/my/private-ssh-key IdentitiesOnly yes ForwardX11 yes ForwardX11Trusted yes ServerAliveInterval 60 which allows to abbreviate the above ssh command as ssh vn1 . The config file can contain several Host entries. IDE based access While editing files in terminal based access is very well possible using terminal editors, e.g. vim or nano , Many developers find code development using terminal based access rather cumbersome. IDEs (Integrated Development environment) provide a more user-friendly GUI based experience. Visual Studio Code provides a very reasonable user experience for both local aand remote development, providing a project directory tree, an editor pane, syntax highlighting, a debugging pane, a terminal, ... It is very well suited for our project work. So, install Visual Studio Code on your local machine. (It is available for Windows, Linux, and MacOSX). Here are some useful VS Code extensions that you should install. Click the Extensions icon in the activity bar on the left. You can search the Marketplace for interesting extensions. Necessary extensions Remote Development Highly recommended extensions Python extension for Visual Studio Code Python extension pack Recommended extensions for C++ C/C++ Better C++ syntax CMake CMake tools Recommended extensions for Fortran Modern Fortran There is a helpfull tutorial on Using VS Code for Remote Development , but before getting your hands dirty, please complete the steps below first. VS Code fails to connect due to file quota exceeded VS Code Remote installs some machinery in your home directory ( ~/.vscode-server ) of the remote machine you are using. As VS Code can easily create a lot of files remotely, this can easily cause file quota exceeded on your user file system. When accessing the cluster through VS Code, you will probably only notice that VS Code fails to connect with the cluster. Log in on the cluster from a terminal with ssh and run > ./path/to/wetppr/scripts/mv.vscode-server.sh This moves the .vscode-server directory from your home directory to $VSC_DATA where file quota are much larger and cannot cause problems. Setting up your remote environment LMOD modules A HPC cluster provides you with many installed software packages. However, none of them are immediately available. To make a package available, you must load the corresponding software module. (These modules, also known as LMOD modules , are a different module kind than the Python modules). In the project directory scripts/vaughan/ you find a script env-lmod.sh that can be sourced to load a set of commonly used LMOD modules in this course. Additional modules can be loaded as module load <module-name> , or the shorthand ml <module-name> . Sourcing a script differs from executing it in that it modifies the current environment ( see _e.g. this ). A script is sourced through the source command: > cd path/to/wetppr > source scripts/vaughan/env-lmod.sh Using toolchain_name foss/2022a module load calcua/2022a module load foss module load SciPy-bundle module load numba wip-tools dependencies: module load CMake module load git module load gh The env-lmod.sh script can be used to load the modules for different toolchains and versions: > source scripts/vaughan/env-lmod.sh intel # => intel/2022a > source scripts/vaughan/env-lmod.sh foss/2023a # => foss/2022a The foss toolchain uses the GCC compilers to build software, whereas the intel toolchain uses the Intel compilers. Tip The source command can be abbreviated as . . Tip Sourcing a script modifies your environment for the time of your shell session. Every time you start a new shell session, you must reload these modules. Initially, it might be tempting to source the script in your .bashrc . However, soon you will discover that such scripts depend on the project you are working on, and that it is better to have it somewhere in your project directory. Tip In order to be able to execute a shell script, it must be made executable ( chmod +x <path/to/script> ). Shell scripts that intend to modify the environment, like env-lmod.sh , and thus must be sourced, need not be executable. If you do not make the script executable, it can only be sourced. In this way you avoid the surprises from an unmodified environment after executing a script that expect to be sourced. (There are also [tricks to detect if a script is being sourced(https://stackoverflow.com/questions/2683279/how-to-detect-if-a-script-is-being-sourced)] or not.) Wiptools Wiptools is a Python package that can simplify your project management considerably. If you haven't already done so, source the env-lmod.sh script above and install wip-tools in a (remote) terminal as: > pip install --user wiptools ... If you are interested in building binary extension modules from C++, you should also install nanobind > pip install --user nanobind ... Note The --user flag instructs pip to install the package in the directory defined by the environment variable ${PYTHONUSERBASE} . The default install path of pip is a system location for which you do not have write permissions. Omitting --user would raise a PermissionError . When installing on your local machine, the --user flag may be omitted. Warning Before starting to use wiptools , checkout these prerequisites . Submitting jobs on Vaughan Unlike your personal computer, which you use mainly interactively , a supercomputer is mostly used in batch mode . To execute some computational work, you send a request to the scheduler specifying the work and how you want it to be executed. You must specify the resources you need, how the environment must be set up, i.e. necessary LMOD modules and environment variables, and the command(s) you want to execute. Such a request is called a job script . The scheduler decides which compute nodes will be used for your job and when it will be started, based on a fair share policy and the availability of the resources of the cluster. Most modern supercomputers - all VSC clusters - use Slurm for resource management and scheduling. An extensive description about using Slurm on Vaughan can be found here . Here is a simple job script, annotated with explanations for each line: <!-- BEGIN INCLUDE ../scripts/vaughan/hello-mpi4py/hello-mpi4py.slurm --> #!/bin/bash ### The line above is the shebang. It specifies the interpreter for the script. ### (see https://linuxhandbook.com/shebang/). Always /bin/bash for job scripts. ### Submit this job from the parent directory as: ### > cd path/to/wetppr/scripts/vaughan/hello-mpi4py ### > sbatch hello-mpi4py.slurm ### Slurm job script parameters ################################################ ### Accounting info. Specify the account from which credits will be taken to ### execute this job. At the time of writing (11/2023) only applicable on ### the VSC Tier-1 computer Hortense and the Tier-2 supercomputers at KU Leuven, ### soon also in Tier-2 supercomputers at UAntwerpen. ### SBATCH --account=<account-name> ### Specify the job name #SBATCH --job-name hello-mpi4py ### Redirect output written to stdout to file <job-name>.<job-id>.stdout ### Redirect output written to stderr to file <job-name>.<job-id>.stderr ### This groups your files nicely in a alphabetical listing. #SBATCH -o %x.%j.stdout #SBATCH -e %x.%j.stderr ### Slurm resource specifications ### Request 1 compute node #SBATCH --nodes=1 ### Request 16 MPI processes #SBATCH --ntasks=16 ### Request 1 CPU per MPI process (no hybrid OpenMP-MPI) #SBATCH --cpus-per-task=1 ### Tell Slurm that you expect the job to end in 5 minutes. ### (Abort the job if it takes longer). #SBATCH --time=00:05:00 ### Prepare the environment for the job ######################################## ### Source the env-lmod.sh script to loads the LMOD modules needed. ### We request it to use the foss toolchain. source ../env-lmod.sh foss ### Specify the computational work ############################################# ### Run the Python script hello-mpi4py.py on the requested resources by calling ### srun (this is a Slurm command) srun -n${SLURM_NTASKS} -c${SLURM_CPUS_PER_TASK} --exclusive --unbuffered python hello-mpi4py.py <!-- END INCLUDE --> This job script reqests the parallel execution of a simple Python script hello-mpi4py.py that prints some environment variables set by Slurm, and a line for each MPI process with some info about the MPI process: <!-- BEGIN INCLUDE ../scripts/vaughan/hello-mpi4py/hello-mpi4py.py --> import os jobid = os.getenv('SLURM_JOB_ID') stepid = os.getenv('SLURM_STEP_ID') pid = os.getpid() affinity = os.sched_getaffinity(0) # set of the CPUs used by the current MPI proces from mpi4py import MPI comm = MPI.COMM_WORLD def print_environment_variable(name): \"\"\"Print the name and the value of an environment value.\"\"\" value = os.getenv(name) print(f\"${{{name}}}={value}\") if __name__ == '__main__': # Print some environment variables (only if the MPI rank is 0) if comm.rank == 0: print_environment_variable('SLURM_NTASKS') print_environment_variable('SLURM_CPUS_PER_TASK') # Print a line with the job-id, job-step-id, MPI rank, total number of MPI # processes, process id, and the affinity (the id of the CPU on which # the MPI process is running). Every MPI process will print one line. print(f'jobid.stepid={jobid}.{stepid}, MPI_rank={comm.rank}/{comm.size}, {pid=} : affinity={affinity}') <!-- END INCLUDE --> The job is submitted from the login node as: > cd path/to/wetppr/scripts/vaughan/hello-mpi4py > sbatch mpi4py_hello_world.slurm Note that we first cd into the parent directory of the job script because the job script uses paths relative to the parent directory () ../env-lmod.sh and hello-mpi4py.py ). If all goes well, sbatch responds with something like: Submitted batch job 1186356 Where 1186356 is the job-id. Tip If something goes wrong with the job and you need help from the system administrator, always mention the job-id. The job is now in the job queue. You can check the status of all your submitted jobs with the squeue command: > squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1186356 zen2 hello-mp vsc20170 PD 0:00 2 (Priority) The ST column shows the status of your job. PD means 'pending', the job is waiting for resource allocation. It will eventually run. Once running, it will show R as a status code and the TIME column will show the walltime of the job. Once completed the status will be 'CD' and after some minutes, it will disappear from the output of the squeue command. The directory wetppr/scripts/vaughan_examples now contains two extra files: > ls -l -rw-rw-r-- 1 vsc20170 vsc20170 0 Oct 26 09:32 hello-mpi4py.1186356.stderr -rw-rw-r-- 1 vsc20170 vsc20170 1306 Oct 26 09:43 hello-mpi4py.1186356.stdout -rw-rw-r-- 1 vsc20170 vsc20170 973 Oct 26 09:42 hello-mpi4py.py -rw-rw-r-- 1 vsc20170 vsc20170 1826 Oct 26 08:56 hello-mpi4py.slurm The job has created two output files with the output written to stderr and stdout, respectively. As requested by the job script lines #SBATCH -o %x.%j.stdout and #SBATCH -e %x.%j.stderr , they are compose as <job-name>.<job-id>.stdout and <job-name>.<job-id>.stderr , respectively. Error messages are normally written to the stderr file, which is empty in this case, as indicated by the 0 file size. The output of the Python script hello-mpi4py.py appears in hello-mpi4py.1186356.stdout , as well as that of env-lmod.sh , which precedes that of hello-mpi4py.py . <!-- BEGIN INCLUDE ../scripts/vaughan/hello-mpi4py/hello-mpi4py.1186356.stdout_ok --> Using toolchain_name foss/2022a module load calcua/2022a module load foss module load SciPy-bundle module load numba wip-tools dependencies: module load CMake module load git module load gh ${SLURM_NTASKS}=16 ${SLURM_CPUS_PER_TASK}=1 jobid.stepid=1186356.0, MPI_rank=0/16, pid=1489956 : affinity={0} jobid.stepid=1186356.0, MPI_rank=1/16, pid=1489957 : affinity={1} jobid.stepid=1186356.0, MPI_rank=2/16, pid=1489958 : affinity={2} jobid.stepid=1186356.0, MPI_rank=3/16, pid=1489959 : affinity={3} jobid.stepid=1186356.0, MPI_rank=4/16, pid=1489960 : affinity={4} jobid.stepid=1186356.0, MPI_rank=5/16, pid=1489961 : affinity={5} jobid.stepid=1186356.0, MPI_rank=6/16, pid=1489962 : affinity={6} jobid.stepid=1186356.0, MPI_rank=7/16, pid=1489963 : affinity={7} jobid.stepid=1186356.0, MPI_rank=8/16, pid=1489964 : affinity={8} jobid.stepid=1186356.0, MPI_rank=9/16, pid=1489965 : affinity={9} jobid.stepid=1186356.0, MPI_rank=10/16, pid=1489966 : affinity={10} jobid.stepid=1186356.0, MPI_rank=11/16, pid=1489967 : affinity={11} jobid.stepid=1186356.0, MPI_rank=12/16, pid=1489968 : affinity={12} jobid.stepid=1186356.0, MPI_rank=13/16, pid=1489969 : affinity={13} jobid.stepid=1186356.0, MPI_rank=14/16, pid=1489970 : affinity={14} jobid.stepid=1186356.0, MPI_rank=15/16, pid=1489971 : affinity={15} <!-- END INCLUDE --> We already explained the job-id. The step-id is tied to an individual srun command. The job script has only one srun command, so the step-id is always 0 (counting starts at 0, in C/C++/Python tradition). The MPI rank is tied to he MPI process running. The job script requested #SBATCH --ntasks=16 , hence, the MPI rank ranges from 0 to 15. Pid is the process id. It refers to the Linux process of the given MPI rank. Finally, affinity is the set of CPUs on which the process runs. Since the job script requested #SBATCH --cpus-per-task=1 , each MPI rank uses only a single CPU. In hybrid MPI-OpenMP programs, an MPI rank can use OpenMP parallel sections over several CPUs. In that case one would request, e.g. , #SBATCH --cpus-per-task=4 and the afinity sets would show 4 different CPUs. The job above requested only a single compute node and a very short wall time (5'). Often, it will start executing allmost immediately after submitting. For larger jobs, requiring many nodes, the scheduler has to find a moment at which the number of nodes requested are available (that is, without work load). Typically, one has to wait a bit before the job starts. The waiting time depends on the total work load of the cluster, but rarely exceeds more than a day. To know when your job starts and ends, you can ask the scheduler to send you an e-mail, by adding these lines to your job script: ### Send mail when the job starts, ands and fails #SBATCH --mail-type=BEGIN,END,FAIL ### Send mail to this address: #SBATCH -\u2013mail-user=<your e-mail address> Note that the maximum wall time for a job is 72 hours. Longer jobs must be split into pieces shorter than 72 hours. Jobs of 1 hour or less have higher priority. Recommended steps from here A helpfull tutorial on Using VS Code for Remote Development The Getting started with wip","title":"VSC infrastructure"},{"location":"vsc-infrastructure/#vsc-infrastructure","text":"This document describes how to set up your environment for the project work of the 2000wetppr course on the Tier-2 cluster of the University of Antwerp, which is also a VSC cluster. With minor modifications this can be applied to all VSC clusters. In general, the text is also applicable to other HPC clusters, but the modifictions needed may be a bit more substantial. Note for students These topics are logically ordered. Make sure that you carry out all the tasks in the order described.","title":"VSC infrastructure"},{"location":"vsc-infrastructure/#a-bit-of-terminology","text":"In this course we often use the terms local and remote . The term local refers to the physical machine you are working on, i.e. your desktop or laptop, or even a tablet or cell phone. On the other hand remote refers to a machine which is usually in some other place, and which you are accessing from your local machine through a network connection (usually the internet) with the remote machine. In this course the remote machine is the university's Tier-2 supercomputer, Vaughan , at the time of writing. The supercomputer, basically, consists of: One or more login nodes . When users logs in to the supercomputer, a connection is established between the user's local machine and a login node. This is where users execute simple, not computationally intensive tasks, e.g. : job preparation, organization of the workspace and projects, create input data, software development tasks, small tests, ... A collection of compute nodes , potentially many thousands. This is where computationally intensive tasks of the users are executed. Typically, the user has no direct connection to the compute nodes. One or more master nodes . The master node runs the resource manager and the scheduler > and is the connection between the login node and the compute nodes. The former keeps an eye on what the compute nodes are doing and whether they are ready to accept new computational tasks. The scheduler does the planning of that work. To carry out some computational work on the compute nodes, the user must send a request to the scheduler, describing the task, the environment in which that task must be executed, and the resources (number of nodes, memory, accelerators, ...) needed by the task. Such a request is called a job script and the task is referred to as a job .","title":"A bit of terminology"},{"location":"vsc-infrastructure/#preparing-for-the-vsc-infrastructure-in-2000wetppr","text":"","title":"Preparing for the VSC infrastructure in 2000WETPPR"},{"location":"vsc-infrastructure/#applying-for-a-guest-account","text":"Note This section is only for students of the course 2000wetppr . Students of the course 2000wetppr must apply for a guest account to access the university's HPC clusters, unless they already have a VSC account. The project work (see Evaluation ) requires access to one of the university's HPC clusters. To apply for a guest account, create a SSH public/private key pair (see below) and send it by e-mail to franky.backeljauw@uantwerpen.be with engelbert.tijskens@uantwerpen.be in cc. A guest account will subsequently be created for you.","title":"Applying for a guest account"},{"location":"vsc-infrastructure/#applying-for-a-vsc-account","text":"Note This section is only for researchers of Flemish institutes . Researchers of Flemish research institutes can apply for a VSC account to get access to the VSC Tier-2 and Tier-1 supercomputers. See Getting access to VSC clusters . An ssh public/private key pair is also required.","title":"Applying for a VSC account"},{"location":"vsc-infrastructure/#creating-an-ssh-publicprivate-key-pair","text":"An ssh public/private key pair in necessary for both a guest account (students) and a VSC account (researchers). A ssh public/private key pair is a way for secure access to a system through the Secure Shell protocol. They are basically two small files with matching numbers. You may think of the public key as a lock. Everyone is allowed to see the lock, but no one can open the lock without its key, which is the private part of the key pair. The public key (the lock) will be placed on a system you need access to, in this case the Tier-2 supercomputer of our university. To access to the supercomputer ( i.e. , to open the lock) from, say, your laptop, you need the private key to be stored on your laptop (or a USB memory stick) and pass it to the SSH protocol, which will verify that the private key and the public key match. If case they do, the SSH protocol will open the lock and grant you access to the machine. To create a ssh public/private key pair proceed as follows. Open a 'terminal'. On Windows The latest builds of Windows 10 and Windows 11 include a built-in SSH server and client that are based on OpenSSH. You can use the cmd prompt, powershell, or WSL (Windows subsystem for Linux) as a terminal. For older Windows versions, we recommend installing mobaxterm to generate a ssh public/private key pair. On Linux Most Linux distributions have a terminal application. MacOSX MacOSX comes with a build in Terminal.app . iTerm2 is a replacement for Terminal.app with many interesting extra features. Type the following command at the prompt: > ssh-keygen -t rsa -b 4096 You will then be prompted for a file location of the public and private key. You may accept the default location by entering. The default file location will look a bit different, depending on your OS. If the files already exist you can choose to overwrite them or to cancel the operation. You might want to change the filename of the key to a more meaningfull name, e.g. access_vaughan_rsa . Don't use blanks in the filename. Use hyphens ( - ) or underscores ( _ ) instead. Enter file in which to save the key (C:\\Users\\your_username/.ssh/id rsa) : C:\\Users\\your_username/.ssh/id rsa already exists. Overwrite (y/n)? y You will then be prompted for a passphrase (twice). A passphrase provides an extra level of protection in case somebody would steal your private key. Press enter for an empty passphrase. (Passphrases are a little annoying when using VS Code (see below) for remote development.) Enter passphrase (empty for no passphrase): Enter same passphrase again: Finally you will be notified of where the keys are stored: Your identification has been saved in C:\\Users\\your_username/.ssh/id rsa. Your public key has been saved in C:\\Users\\your_username/.ssh/id rsa.pub. For students of 2000wetppr To obtain a guest account, students must send their public key (and only the public key, the private key is, well, um, private ) to franky.backeljauw@uantwerpen.be with engelbert.tijskens@uantwerpen.be in cc . The public key is the one with the .pub extension.","title":"Creating an ssh public/private key pair"},{"location":"vsc-infrastructure/#accessing-vaughan","text":"","title":"Accessing Vaughan"},{"location":"vsc-infrastructure/#terminal-based-access","text":"Vaughan is (at the time of writing) the University of Antwerp's Tier-2 HPC cluster. For terminal based access you open a terminal (see above) and execute the command: > ssh -i path/to/my/private-ssh-key your-user-id@login1-vaughan.hpc.uantwerpen.be Last login: Mon Feb 27 12:40:32 2023 from 143.129.75.140 -------------------------------------------------------------------- Welcome to VAUGHAN ! ... Note If the key is in sub-directory .ssh of your home directory, the -i path/to/my/private-ssh-key can be omitted. The ssh command above, connects your terminal session to a login node (see above) of the Vaughan cluster. After the command is finished, you can use the terminal as if you were working on the login node. The current working directory will be a location in your file system on the cluster, rather than on your local machine. Vaughan has two login nodes, login1-vaughan.hpc.uantwerpen.be and login2-vaughan.hpc.uantwerpen.be . You can also use login-vaughan.hpc.uantwerpen.be . The system then will choose the login node with the highest availability. Ssh comes with a .ssh/config file that allows you to store the arguments of frequently used ssh commands. E.g. # file ~/.ssh/config Host vn1 HostName login1-vaughan.hpc.uantwerpen.be User vsc20170 IdentityFile /full/path/to/my/private-ssh-key IdentitiesOnly yes ForwardX11 yes ForwardX11Trusted yes ServerAliveInterval 60 which allows to abbreviate the above ssh command as ssh vn1 . The config file can contain several Host entries.","title":"Terminal based access"},{"location":"vsc-infrastructure/#ide-based-access","text":"While editing files in terminal based access is very well possible using terminal editors, e.g. vim or nano , Many developers find code development using terminal based access rather cumbersome. IDEs (Integrated Development environment) provide a more user-friendly GUI based experience. Visual Studio Code provides a very reasonable user experience for both local aand remote development, providing a project directory tree, an editor pane, syntax highlighting, a debugging pane, a terminal, ... It is very well suited for our project work. So, install Visual Studio Code on your local machine. (It is available for Windows, Linux, and MacOSX). Here are some useful VS Code extensions that you should install. Click the Extensions icon in the activity bar on the left. You can search the Marketplace for interesting extensions. Necessary extensions Remote Development Highly recommended extensions Python extension for Visual Studio Code Python extension pack Recommended extensions for C++ C/C++ Better C++ syntax CMake CMake tools Recommended extensions for Fortran Modern Fortran There is a helpfull tutorial on Using VS Code for Remote Development , but before getting your hands dirty, please complete the steps below first.","title":"IDE based access"},{"location":"vsc-infrastructure/#vs-code-fails-to-connect-due-to-file-quota-exceeded","text":"VS Code Remote installs some machinery in your home directory ( ~/.vscode-server ) of the remote machine you are using. As VS Code can easily create a lot of files remotely, this can easily cause file quota exceeded on your user file system. When accessing the cluster through VS Code, you will probably only notice that VS Code fails to connect with the cluster. Log in on the cluster from a terminal with ssh and run > ./path/to/wetppr/scripts/mv.vscode-server.sh This moves the .vscode-server directory from your home directory to $VSC_DATA where file quota are much larger and cannot cause problems.","title":"VS Code fails to connect due to file quota exceeded"},{"location":"vsc-infrastructure/#setting-up-your-remote-environment","text":"","title":"Setting up your remote environment"},{"location":"vsc-infrastructure/#lmod-modules","text":"A HPC cluster provides you with many installed software packages. However, none of them are immediately available. To make a package available, you must load the corresponding software module. (These modules, also known as LMOD modules , are a different module kind than the Python modules). In the project directory scripts/vaughan/ you find a script env-lmod.sh that can be sourced to load a set of commonly used LMOD modules in this course. Additional modules can be loaded as module load <module-name> , or the shorthand ml <module-name> . Sourcing a script differs from executing it in that it modifies the current environment ( see _e.g. this ). A script is sourced through the source command: > cd path/to/wetppr > source scripts/vaughan/env-lmod.sh Using toolchain_name foss/2022a module load calcua/2022a module load foss module load SciPy-bundle module load numba wip-tools dependencies: module load CMake module load git module load gh The env-lmod.sh script can be used to load the modules for different toolchains and versions: > source scripts/vaughan/env-lmod.sh intel # => intel/2022a > source scripts/vaughan/env-lmod.sh foss/2023a # => foss/2022a The foss toolchain uses the GCC compilers to build software, whereas the intel toolchain uses the Intel compilers. Tip The source command can be abbreviated as . . Tip Sourcing a script modifies your environment for the time of your shell session. Every time you start a new shell session, you must reload these modules. Initially, it might be tempting to source the script in your .bashrc . However, soon you will discover that such scripts depend on the project you are working on, and that it is better to have it somewhere in your project directory. Tip In order to be able to execute a shell script, it must be made executable ( chmod +x <path/to/script> ). Shell scripts that intend to modify the environment, like env-lmod.sh , and thus must be sourced, need not be executable. If you do not make the script executable, it can only be sourced. In this way you avoid the surprises from an unmodified environment after executing a script that expect to be sourced. (There are also [tricks to detect if a script is being sourced(https://stackoverflow.com/questions/2683279/how-to-detect-if-a-script-is-being-sourced)] or not.)","title":"LMOD modules"},{"location":"vsc-infrastructure/#wiptools","text":"Wiptools is a Python package that can simplify your project management considerably. If you haven't already done so, source the env-lmod.sh script above and install wip-tools in a (remote) terminal as: > pip install --user wiptools ... If you are interested in building binary extension modules from C++, you should also install nanobind > pip install --user nanobind ... Note The --user flag instructs pip to install the package in the directory defined by the environment variable ${PYTHONUSERBASE} . The default install path of pip is a system location for which you do not have write permissions. Omitting --user would raise a PermissionError . When installing on your local machine, the --user flag may be omitted. Warning Before starting to use wiptools , checkout these prerequisites .","title":"Wiptools"},{"location":"vsc-infrastructure/#submitting-jobs-on-vaughan","text":"Unlike your personal computer, which you use mainly interactively , a supercomputer is mostly used in batch mode . To execute some computational work, you send a request to the scheduler specifying the work and how you want it to be executed. You must specify the resources you need, how the environment must be set up, i.e. necessary LMOD modules and environment variables, and the command(s) you want to execute. Such a request is called a job script . The scheduler decides which compute nodes will be used for your job and when it will be started, based on a fair share policy and the availability of the resources of the cluster. Most modern supercomputers - all VSC clusters - use Slurm for resource management and scheduling. An extensive description about using Slurm on Vaughan can be found here . Here is a simple job script, annotated with explanations for each line: <!-- BEGIN INCLUDE ../scripts/vaughan/hello-mpi4py/hello-mpi4py.slurm --> #!/bin/bash ### The line above is the shebang. It specifies the interpreter for the script. ### (see https://linuxhandbook.com/shebang/). Always /bin/bash for job scripts. ### Submit this job from the parent directory as: ### > cd path/to/wetppr/scripts/vaughan/hello-mpi4py ### > sbatch hello-mpi4py.slurm ### Slurm job script parameters ################################################ ### Accounting info. Specify the account from which credits will be taken to ### execute this job. At the time of writing (11/2023) only applicable on ### the VSC Tier-1 computer Hortense and the Tier-2 supercomputers at KU Leuven, ### soon also in Tier-2 supercomputers at UAntwerpen. ### SBATCH --account=<account-name> ### Specify the job name #SBATCH --job-name hello-mpi4py ### Redirect output written to stdout to file <job-name>.<job-id>.stdout ### Redirect output written to stderr to file <job-name>.<job-id>.stderr ### This groups your files nicely in a alphabetical listing. #SBATCH -o %x.%j.stdout #SBATCH -e %x.%j.stderr ### Slurm resource specifications ### Request 1 compute node #SBATCH --nodes=1 ### Request 16 MPI processes #SBATCH --ntasks=16 ### Request 1 CPU per MPI process (no hybrid OpenMP-MPI) #SBATCH --cpus-per-task=1 ### Tell Slurm that you expect the job to end in 5 minutes. ### (Abort the job if it takes longer). #SBATCH --time=00:05:00 ### Prepare the environment for the job ######################################## ### Source the env-lmod.sh script to loads the LMOD modules needed. ### We request it to use the foss toolchain. source ../env-lmod.sh foss ### Specify the computational work ############################################# ### Run the Python script hello-mpi4py.py on the requested resources by calling ### srun (this is a Slurm command) srun -n${SLURM_NTASKS} -c${SLURM_CPUS_PER_TASK} --exclusive --unbuffered python hello-mpi4py.py <!-- END INCLUDE --> This job script reqests the parallel execution of a simple Python script hello-mpi4py.py that prints some environment variables set by Slurm, and a line for each MPI process with some info about the MPI process: <!-- BEGIN INCLUDE ../scripts/vaughan/hello-mpi4py/hello-mpi4py.py --> import os jobid = os.getenv('SLURM_JOB_ID') stepid = os.getenv('SLURM_STEP_ID') pid = os.getpid() affinity = os.sched_getaffinity(0) # set of the CPUs used by the current MPI proces from mpi4py import MPI comm = MPI.COMM_WORLD def print_environment_variable(name): \"\"\"Print the name and the value of an environment value.\"\"\" value = os.getenv(name) print(f\"${{{name}}}={value}\") if __name__ == '__main__': # Print some environment variables (only if the MPI rank is 0) if comm.rank == 0: print_environment_variable('SLURM_NTASKS') print_environment_variable('SLURM_CPUS_PER_TASK') # Print a line with the job-id, job-step-id, MPI rank, total number of MPI # processes, process id, and the affinity (the id of the CPU on which # the MPI process is running). Every MPI process will print one line. print(f'jobid.stepid={jobid}.{stepid}, MPI_rank={comm.rank}/{comm.size}, {pid=} : affinity={affinity}') <!-- END INCLUDE --> The job is submitted from the login node as: > cd path/to/wetppr/scripts/vaughan/hello-mpi4py > sbatch mpi4py_hello_world.slurm Note that we first cd into the parent directory of the job script because the job script uses paths relative to the parent directory () ../env-lmod.sh and hello-mpi4py.py ). If all goes well, sbatch responds with something like: Submitted batch job 1186356 Where 1186356 is the job-id. Tip If something goes wrong with the job and you need help from the system administrator, always mention the job-id. The job is now in the job queue. You can check the status of all your submitted jobs with the squeue command: > squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1186356 zen2 hello-mp vsc20170 PD 0:00 2 (Priority) The ST column shows the status of your job. PD means 'pending', the job is waiting for resource allocation. It will eventually run. Once running, it will show R as a status code and the TIME column will show the walltime of the job. Once completed the status will be 'CD' and after some minutes, it will disappear from the output of the squeue command. The directory wetppr/scripts/vaughan_examples now contains two extra files: > ls -l -rw-rw-r-- 1 vsc20170 vsc20170 0 Oct 26 09:32 hello-mpi4py.1186356.stderr -rw-rw-r-- 1 vsc20170 vsc20170 1306 Oct 26 09:43 hello-mpi4py.1186356.stdout -rw-rw-r-- 1 vsc20170 vsc20170 973 Oct 26 09:42 hello-mpi4py.py -rw-rw-r-- 1 vsc20170 vsc20170 1826 Oct 26 08:56 hello-mpi4py.slurm The job has created two output files with the output written to stderr and stdout, respectively. As requested by the job script lines #SBATCH -o %x.%j.stdout and #SBATCH -e %x.%j.stderr , they are compose as <job-name>.<job-id>.stdout and <job-name>.<job-id>.stderr , respectively. Error messages are normally written to the stderr file, which is empty in this case, as indicated by the 0 file size. The output of the Python script hello-mpi4py.py appears in hello-mpi4py.1186356.stdout , as well as that of env-lmod.sh , which precedes that of hello-mpi4py.py . <!-- BEGIN INCLUDE ../scripts/vaughan/hello-mpi4py/hello-mpi4py.1186356.stdout_ok --> Using toolchain_name foss/2022a module load calcua/2022a module load foss module load SciPy-bundle module load numba wip-tools dependencies: module load CMake module load git module load gh ${SLURM_NTASKS}=16 ${SLURM_CPUS_PER_TASK}=1 jobid.stepid=1186356.0, MPI_rank=0/16, pid=1489956 : affinity={0} jobid.stepid=1186356.0, MPI_rank=1/16, pid=1489957 : affinity={1} jobid.stepid=1186356.0, MPI_rank=2/16, pid=1489958 : affinity={2} jobid.stepid=1186356.0, MPI_rank=3/16, pid=1489959 : affinity={3} jobid.stepid=1186356.0, MPI_rank=4/16, pid=1489960 : affinity={4} jobid.stepid=1186356.0, MPI_rank=5/16, pid=1489961 : affinity={5} jobid.stepid=1186356.0, MPI_rank=6/16, pid=1489962 : affinity={6} jobid.stepid=1186356.0, MPI_rank=7/16, pid=1489963 : affinity={7} jobid.stepid=1186356.0, MPI_rank=8/16, pid=1489964 : affinity={8} jobid.stepid=1186356.0, MPI_rank=9/16, pid=1489965 : affinity={9} jobid.stepid=1186356.0, MPI_rank=10/16, pid=1489966 : affinity={10} jobid.stepid=1186356.0, MPI_rank=11/16, pid=1489967 : affinity={11} jobid.stepid=1186356.0, MPI_rank=12/16, pid=1489968 : affinity={12} jobid.stepid=1186356.0, MPI_rank=13/16, pid=1489969 : affinity={13} jobid.stepid=1186356.0, MPI_rank=14/16, pid=1489970 : affinity={14} jobid.stepid=1186356.0, MPI_rank=15/16, pid=1489971 : affinity={15} <!-- END INCLUDE --> We already explained the job-id. The step-id is tied to an individual srun command. The job script has only one srun command, so the step-id is always 0 (counting starts at 0, in C/C++/Python tradition). The MPI rank is tied to he MPI process running. The job script requested #SBATCH --ntasks=16 , hence, the MPI rank ranges from 0 to 15. Pid is the process id. It refers to the Linux process of the given MPI rank. Finally, affinity is the set of CPUs on which the process runs. Since the job script requested #SBATCH --cpus-per-task=1 , each MPI rank uses only a single CPU. In hybrid MPI-OpenMP programs, an MPI rank can use OpenMP parallel sections over several CPUs. In that case one would request, e.g. , #SBATCH --cpus-per-task=4 and the afinity sets would show 4 different CPUs. The job above requested only a single compute node and a very short wall time (5'). Often, it will start executing allmost immediately after submitting. For larger jobs, requiring many nodes, the scheduler has to find a moment at which the number of nodes requested are available (that is, without work load). Typically, one has to wait a bit before the job starts. The waiting time depends on the total work load of the cluster, but rarely exceeds more than a day. To know when your job starts and ends, you can ask the scheduler to send you an e-mail, by adding these lines to your job script: ### Send mail when the job starts, ands and fails #SBATCH --mail-type=BEGIN,END,FAIL ### Send mail to this address: #SBATCH -\u2013mail-user=<your e-mail address> Note that the maximum wall time for a job is 72 hours. Longer jobs must be split into pieces shorter than 72 hours. Jobs of 1 hour or less have higher priority.","title":"Submitting jobs on Vaughan"},{"location":"vsc-infrastructure/#recommended-steps-from-here","text":"A helpfull tutorial on Using VS Code for Remote Development The Getting started with wip","title":"Recommended steps from here"}]}