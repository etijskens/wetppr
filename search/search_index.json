{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welkom bij Parallel programmeren","text":""},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Over de auteur</li> <li>Overview</li> <li>Glossary</li> <li>Chapter 1</li> <li>Chapter 2</li> <li>Chapter 3</li> <li>Chapter 4</li> <li>Evaluation</li> <li>Assignment</li> <li>Guide lines</li> </ol>"},{"location":"assignment/","title":"Assignment 2022-23","text":"<p>Note</p> <p>to be done ...</p>"},{"location":"chapter-1/","title":"Chapter 1 - Introduction","text":"MathJax.Hub.Config({     tex2jax: {       inlineMath: [['$','$'], ['\\\\(','\\\\)']],       processEscapes: true},       jax: [\"input/TeX\",\"input/MathML\",\"input/AsciiMath\",\"output/CommonHTML\"],       extensions: [\"tex2jax.js\",\"mml2jax.js\",\"asciimath2jax.js\",\"MathMenu.js\",\"MathZoom.js\",\"AssistiveMML.js\", \"[Contrib]/a11y/accessibility-menu.js\"],       TeX: {       extensions: [\"AMSmath.js\",\"AMSsymbols.js\",\"noErrors.js\",\"noUndefined.js\"],       equationNumbers: {       autoNumber: \"AMS\"       }     }   });  <p>Material:</p> <ul> <li>this text and</li> <li>this presentation.</li> </ul>"},{"location":"chapter-1/#overview","title":"Overview","text":"<ul> <li>What is a parallel program?</li> <li>Possible reasons to parallelize a program.</li> </ul>"},{"location":"chapter-1/#what-is-a-parallel-program","title":"What is a parallel program?","text":"<p>A parallel program is a program that distributes its work over different processing units such that parts of its  work load can be computed simultaneously. At the end the program gathers the partial results from the processing  units and combines them in a global result. If the tasks are independent of each other, the program is called  embarrassingly parallel. In general, the individual tasks are not independent and need to exchange information.  This is called communication. The opposite of a parallel program is a serial or sequential program,  executing all its instructions one after the other. </p>"},{"location":"chapter-1/#possible-reasons-to-parallelize-a-program","title":"Possible reasons to parallelize a program","text":""},{"location":"chapter-1/#1-reduce-the-time-to-solution","title":"1. Reduce the time to solution","text":"<p>The term time to solution in general means the time your machine needs to solve a computational problem. If the  problem can be divided in smaller tasks that can be computed simultaneously, the time to solution decreases. If a  company can solve a research or engineering question in a week or a day, that is an important difference. A  processing unit has a maximum number of instructions it can execute per second, this is called its peak  performance. Obviously, the peak performance is a machine limit puts a hard limit to what a processing unit can  achieve in a given amount of time. But instructions operate on data, and moving data from main memory takes time as  well. A program that must process lots of data but does little computation is limited by the speed at which the  processing unit can fetch data from the main memory. This is called the memory bandwidth (usually in Mbits/s). Programs that do a lot of computation and does not move a lot of data in or out of main memory is called compute  limited. A program that moves a lot of data and little computation is bandwidth limited. While in the past  programs used to be compute bound, today, most programs are memory bound, because the speed of the processing units  increased much faster than the speed of memory. As a consequence, efficient memory access patterns are crucial to the  performance of a program.  </p>"},{"location":"chapter-1/#2-solve-bigger-problems-in-the-same-time","title":"2. Solve bigger problems in the same time","text":"<p>There is a third machine limit that plays a role, namely the amount of main memory. This puts a limit on the size of  the problem that can be treated, e.g. the number of volume elements in a CFD simulation or the number of atoms in a  MD simulation. If the program can distribute the work over, say 10 machines, it has 10 times the amount of memory at  its disposition and thus can solve a 10 times bigger problem.  </p>"},{"location":"chapter-1/#3-produce-more-accurate-solutions","title":"3. Produce more accurate solutions","text":"<p>More accuracy can come from more complex physical models, or from using more basis functions to expand the solution. This leads to more computation and perhaps a prohibitively long time to solution. Problems involving discretisation  (the process of dividing the domain of a computational problem in small elements, as in computational fluid dynamics  and finite element modelling) the accuracy typically improves when the elements get smaller, as in approximating the  integral under a curve by rectangles. In both cases parallelization of the program may be necessary to obtain a  solution. </p>"},{"location":"chapter-1/#4-competition","title":"4 Competition","text":"<p>If a program that is in competition with other programs that solve the same problem, parallelization will allow it  to reduce the time to solution, to compute bigger problems and achieve more accurate solution. This is, obviously, a  competitive advantage.  </p>"},{"location":"chapter-1/#cant-i-just-by-a-faster-and-bigger-computer","title":"Can't I just by a faster and bigger computer?","text":"<p>Nope, that fairy tale ended approximately at the beginning of this century with the advent of the multi-processor  computer, also called multi-core computer. Increasing the peak performance by increasing the clock frequency was no  longer possible, because the power consumption of a processor increases as the third power of the clock frequency.  At a certain point it became impossible or too expensive to cool the processor. The only way to get a processor  execute more instructions per second was to put more processing units on it (cores). At that point serial program  became even slower on the new multi-processors because the clock frequency was reduced to remain inside the power  envelope. Moore's law predicts that the number of transistors in a processor doubles every 18 months due to  increasing miniaturization. With this the combined peak performance of the multi-processors increases as well, but  the peak performance of the individual processing units no longer does. This makes it necessary to parallelize  programs in order to keep up with Moore's law. It must be said that the increase of peak performance was not always  in line with Moore's law. At some point the peak performance of processing units was increased by adding  parallelization concept in single processing units like pipelining and SIMD vectorisation. We'll come to that later.</p>"},{"location":"chapter-2/","title":"Chapter 2 - Aspects of modern CPU architecture","text":"MathJax.Hub.Config({     tex2jax: {       inlineMath: [['$','$'], ['\\\\(','\\\\)']],       processEscapes: true},       jax: [\"input/TeX\",\"input/MathML\",\"input/AsciiMath\",\"output/CommonHTML\"],       extensions: [\"tex2jax.js\",\"mml2jax.js\",\"asciimath2jax.js\",\"MathMenu.js\",\"MathZoom.js\",\"AssistiveMML.js\", \"[Contrib]/a11y/accessibility-menu.js\"],       TeX: {       extensions: [\"AMSmath.js\",\"AMSsymbols.js\",\"noErrors.js\",\"noUndefined.js\"],       equationNumbers: {       autoNumber: \"AMS\"       }     }   });  <p>You do not have to be a CPU architecture specialist in order to be able to write efficient code. However, there are  a few aspects of CPU architecture that you should understand.</p>"},{"location":"chapter-2/#the-hierarchical-structure-of-cpu-memory","title":"The hierarchical structure of CPU Memory","text":"<p>CPU memory of modern CPUs is hierarchically organised. The memory levels close to the processor need to be fast, to  serve it with data so that it can continue its work. As fast memory is expensive, the levels close to the processor  are also smaller. The farther away from the processor the bigger they are, but also the slower.    </p> <ul> <li>Each processing unit (or core) has a number of registers (~1 kB) and vector registers on which instructions can    immediately operate (latency = 0 cycles). The registers are connected to  </li> <li>a dedicated L1 cache (~32 kB per core), with a latency of ~ 1 cycle. This is in turn    connected to:  </li> <li>a dedicated L2 cache, (~256 kB per core), with a latency of ~10 cycles. This is in turn connected to: </li> <li>the L3 cache, which is shared among a group of cores, (~2 MB per core), with a latency of ~50 cycles.    This is connected to:</li> <li>the main memory, which is shared by all cores,(256 GB - 2 TB), with a latency of ~200 cycles.</li> </ul> <p>The cores and the caches are on the same chip. For this reason they are considerably faster than the main memory.  Faster memory is more expensive and therefor smaller. The figure below illustrates the layout.</p> <p></p> <p>The I/O hub connects the cpu to the outside world, hard disk, network, ...</p> <p>When an instruction needs a data item in a register, the CPU  looks first in the L1 cache, if it is there it will it to the register that was requested. Otherwise, the CPU looks  in L2. If it is there, it is copied to L1 and the register. Otherwise, the CPU looks in L3. If it is there, it is  copied to L2, L1 and the register. Otherwise, the CPU looks copies the cache line surrounding the data item to  L3, L2, L1 and the data item itself to the register. A cache line is typically 64 bytes long and thus can contain 4  double precision floating point numbers or 8 single precision numbers. The main consequence of this strategy is that  if the data item is part of an array, the next elements of that array will also be copied to L1 so that when  processing the array the latency associated with main memory is amortized over 4 or 8 iterations. In addition, the  CPU will notice when it is processing an array and prefetch the next cache line of the array in order to avoid that  the processor has to wait for the data again. This strategy for loading data leads to two important best practices for  making optimal use of the cache.</p> <ol> <li>Exploit Spatial locality: Organize your data layout in main memory in a way that data in a cache line are mostly     needed together. </li> <li>Exploit Temporal locality: Organize your computations in a way that once a cache line is in L1 cache, as much as     possible computations on that data are carried out. This favors a high computational intensity (see below).     Common techniques for this are loop fusion and tiling. </li> </ol>"},{"location":"chapter-2/#loop-fusion","title":"Loop fusion","text":"<p>Here are two loops over an array <code>x</code>:</p> <pre><code>for xi in x:\n    do_something_with(xi)\nfor xi in x:\n    do_something_else_with(xi)\n</code></pre> <p>If the array <code>x</code> is big, too big to fit in the cache, the above code would start loading <code>x</code> elements into the cache, cach line by cache line. Since <code>x</code> is to large to fit in the cache, at some point, when the cache is full, the CPU  will start to evict the cache lines that were loaded long time a go ane are no more used to replace them with new  cache lines. By the time the first loop finishes, the entire beginning of the <code>x</code> array has been evicted and the  scond loop can start to transfer <code>x</code> again from the main memory to the registers, cache line by cache lina. this  violiate the temporal locality principle. So, it incurs twice the data traffic. Loop fusion fuses the two loops into  one and does all computations needed on <code>xi</code> when it is in the cache. </p> <pre><code>for xi in x:\n    do_something_with(xi)\n    do_something_else_with(xi)\n</code></pre> <p>The disadvantage of loop fusion is that the body of the loop may become too large and require more vector registers  than are available. At that point some computations may be done sequentially and performance may suffer. </p>"},{"location":"chapter-2/#tiling","title":"Tiling","text":"<p>Tiling is does the opposite. Ik keeps the loops separate but restricts them to chunks of <code>x</code> which fit in L1 cache.</p> <pre><code>for chunk in x: # chunk is a slice of x that fits in L1\n    for xi in chunk:\n        do_something_with(xi)\n    for xi in chunk:\n        do_something_else_with(xi)\n</code></pre> <p>Again all computations that need to be done to <code>xi</code> are done when it is in L1 cache. Again the entire <code>x</code> array is  transferred only once to the cache. A disadvantage of tiling is that the chunk size needs to be tuned to the size of  L1, which may differ on different machines. Thus, this approach is not cache-oblivious. Loop fusion, on the  other hand, is cache-oblivious.</p> <p>A good understanding of the workings of the hierarchical structure of processor memory is required to write  efficient programs. Although, at first sight, it may seem an overly complex solution for a simple problem, but it is  a good compromise to the many faces of a truly complex problem. There is an excellent presentation on this matter by  Scott Meyers: CPU Caches and Why You Care. It is an absolute must-see  for this course.</p>"},{"location":"chapter-2/#intra-core-parallellisation-features","title":"Intra-core parallellisation features","text":"<p>Modern CPUs are designed to (among other things) process loops as efficiently as possible, as loops typically  account for a large part of the work load of a program. To make that possible CPUs use two important concepts:  instruction pipelining (ILP) and SIMD vectorisation. </p>"},{"location":"chapter-2/#instruction-pipelining","title":"Instruction pipelining","text":"<p>Instruction pipelining is very well explained here.</p> <p>Basically, instructions are composed of micro-instructions (typically 5), each of which are executed in separate  hardware units of the CPU. By executing the instructions sequentially, only one of those units would be active at a  time: namely, the unit responsible for the current micro-instruction. By adding extra instruction registers, all  micro-instruction hardware units can work simultaneously, but on micro-instructions pertaining to different but  consecutive instructions. In this way, on average 5 (typically) instructions are being executed in parallel. This is  very useful for loops. There are a couple of problems that may lead to pipeline stalls, situations where the  pipeline comes to halt. </p> <ol> <li>A data element is requested that is not in the L1 cache. It must be fetched from deeper cache levels or even     from main memory. This is called a cache miss. A L1 cache miss means that the data is not found in L1, but is     found in L2. In a L2 cache miss it is not found in L2 but it is in L3, and a L3 cache miss, or a cache miss tout     court* de data is not found in L3 and has to be fetched from main memory. The pipeline stops executing for a     number of cycles corresponding to the latency of that cache miss. Data cache misses are the most important cause     of pipeline stalls and as the latency can be really high (~100 cycles).  </li> <li>A instruction is needed that is not in the L1 instruction cache. This may sometimes happen when a (large)     function is called that is not inlined. Just as for a data cache miss, the pipeline stalls for a number of cycles     corresponding to the latency of the cache miss, just as for a data cache miss. </li> <li>You might wonder how a pipeline proceeds when confronted with a branching instruction, a condition that has to be     tested, and must start executing different streams of instructions depending on the outcome (typically     if-then-else constructs). Here's the thing: it guesses the outcome of the test and starts executing the     corresponding branch. As soon as it notices that it guessed wrong, which is necessarily after the condition has been     tested, it stops, steps back and restarts at the correct branch. Obviously, the performance depends on how well     it guesses. The guesses are generally rather smart. It is able to recognize temporal patterns, and if it doesn't     find one, falls back on statistics. Random outcomes of the condition are thus detrimental to performance as its    guess will be wrong at least half the time.</li> </ol>"},{"location":"chapter-2/#consequences-of-computer-architecture-for-performance","title":"Consequences of computer architecture for performance","text":""},{"location":"chapter-2/#recommendations-for-array-processing","title":"Recommendations for array processing","text":"<p>The hierarchical organisation of computer memory has also important consequences for the layout of data arrays and  for loops over arrays in terms of performance (see below). </p> <ol> <li>Loops should be long. Typically, at the begin and end of the loop thee pipeline is not full. When the loop     is long, these sections can be amortized with respect to the inner section, where the pipeline is full. </li> <li>Branches in loops should be predictable. The outcome of unpredictable branches will be guessed wrongly,     causing pipeline stalls. Sometimes it may be worthwile to sort the array according to the probability of the     outcome if this work can be amortized over many loops.</li> <li>Loops should access data contiguously and with unit stride. This assures that<ul> <li>at the next iteration of the loop the data element needed is already in the L1 Cache and can be accessed    without delay,</li> <li>vector registers can be filled efficiently because they need contiguous elements from the input array.</li> </ul> </li> <li>Loops should have high computatonal intensity. The computational intensity $I_c$ is defined as $ I_c =     \\frac {n_{cc}}{n_{rw}} $, with $n_{cc}$ the number of compute cycles and $n_{rw}$ the total number of bytes read and     written. A high computational intensity means many compute cycles and little data traffic to/from memory and thus     implies that there will be no pipeline due to waiting for data to arrive. This is a compute bound loop. Low     computational intensity, on the other hand, will cause many pipeline stalls by waiting for data. This is a     memory bound loop. Here, it is the bandwidth (the speed at which data can be transported from main memory     to the registers) that is the culprit, rather than the latency. </li> </ol>"},{"location":"chapter-2/#recommendations-for-data-structures","title":"Recommendations for data structures","text":"<p>The unit stride for loops recommendation translates into a recommendation for data structures. Let's take Molecular  Dynamics as an example. Object Oriented Programming (OOP) would propose a Atom class with properties for mass $m$,  position $\\textbf{r}$, velocity $\\textbf{v}$, acceleration $\\textbf{a}$, and possibly others as well, but let's  ignore those for the time being. Next, the object oriented programmer would create an array of Atoms. This approach  is called an array of structures (AoS). The AoS approach  leads to a data layout in memory like | $m_0$, $r_{x0}$, $r_{y0}$, $r_{z0}$, $v_{x0}$, $v_{y0}$, $v_{z0}$, $a_{x0}$, | $a_{y0}$, $a_{z0}$, $m_1$, $r_{x1}$, $r_{y1}$, $r_{z1}$, $v_{x1}$, $v_{y1}$, | $v_{z1}$, $a_{x1}$, $a_{y1}$, $a_{z1} $, $m_2$, $r_{x2}$, $r_{y2}$, $r_{z2}$, | $v_{x2}$, $v_{y2}$, $v_{z2}$, $a_{x2}$, $a_{y2}$, $a_{z2}$, ... Assume we  store the properties as single precision floating point numbers, hence a cache line spans 8 values. We marked the cache  line boundaries in the list above with vertical bars. Suppose for some reason we need to find all atoms $j$ for  which $r_{xj}$ is between $x_{lwr}$ and $x_{upr}$. A loop over all atoms $j$ would test $r_{xj}$ and remember the  $j$ for which the test holds. Note that every cache line contains at most one single data item that we need in this  algorithm. some cache lines will even contain no data items that we need. For every data iten we need, a new cache  line must be loaded. This is terribly inefficient. There is a lot of data traffic, only 1/8 of which is useful and the  bandwidth will saturate quickly. Vectorisation would be completely useless. To fill the vector register we would  need 8 cache lines, most of which would correspond to cache misses and cost hundreds of cycles, before we can do 8  comparisons at once. The AoS, while intuitively very attractive, is clearly a disaster as it comes to performance.  The - much better - alternative data structure is the SoA, structure of Arrays. This creates an AtomContainer  class (to stay in the terminology of Object Oriented progrmming) containing an array of length $n_{atoms}$ for each  property. In this case there would be arrays for $m$, $r_x$, $r_y$, $r_z$, $v_x$, $v_y$, $v_z$, $a_x$, $a_y$, $a_z$.  Now all $r_x$ are stored contiguously in memory and every item in a cache would be used. Only one cache line would  be needed to fill a vector register. Prefetching would do a perfect job. The SoA data structure is much more  efficient, and once you get used to it, almost equally intuitive from an OOP viewpoint. Sometimes there is  discussion about storing the coordinates of a vector, e.g. $\\textbf{r}$ as per-coordinate arrays, as above, or as an  array of vectors. The latter makes is more practical to define vector functions like magnitude, distance, dot and  vector products, ... but they make it harder to SIMD vectorise those functions efficiently, because contiguous data  items need to be moved into different vector registers.  </p>"},{"location":"chapter-2/#selecting-algorithms-based-on-computational-complexity","title":"Selecting algorithms based on computational complexity","text":"<p>The computational complexity of an algorithm is an indication of how the number of instructions in an algorithms  scales with the problem size $N$. E.g. the work of an $O(N^2)$ algorithm scales quadratically with its problem size.  As an example consider brute force neighbour detection (Verlet list construction) of $N$ interacting atoms in Molecular  Dynamics:</p> <pre><code>    // C++ \n    for (int i=0; i&lt;N; ++i)\n        for (int j=i+1; j&lt;N; ++j) {\n            r2ij = squared_distance(i,j);\n            if (r2ij&lt;r2cutoff) \n               add_to_Verlet_list(i,j); \n        }\n</code></pre> <p>Note</p> <p>Note that we have avoided the computation of the square root by using the squared distance rather than the  distance.</p> <p>The body of the inner for loop is executed $N*(N-1)/2 = N^2/2 -N/2$ times. Hence, it is $O(N^2)$. Cell-based Verlet  list construction restricts the inner loop to the surrounding cells of atom <code>i</code> and is therefor $O(N)$.</p> <p>The computational complexity of an algorithm used to be a good criterion for algorithm selection: less work means  faster, not? Due to the workings of the hierarchical memory of modern computers the answer is not so clear-cut.  Consider two search algorithms for finding an element in a sorted array, linear search and binary search bisecting. Linear search simply loops over all elements until the element is found (or a larger element is found), and is thus  $O(N)$. Binary search compares the target value to the  middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the  search continues on the remaining half, again taking the middle element to compare to the target value, and  repeating this until the target value is found. If the search ends with the remaining half being empty, the target  is not in the array. The complexity of this algorithm is $O(log{N})$. Clearly, binary search finds the answer  by visiting far fewer elements in the array as indicated by its lower complexity. However, contrary to linear search it  visits the elements in the array non-contiguously, and it is very well possible that there will be a cache miss on  every access. Linear search, on the other hand, will have no cache misses: it loads a cache line, visits all the  elements in it and in the mean time the prefetching machinery takes care of loading the next cache line. It is only  limited by the bandwidth. For small arrays linear search will be faster than binary search. For large arrays the  situation is reversed. A clever approach would be to combine both methods: start with binary search and switch to  linear search as soon as the part of the array to search is small enough. This needs some tuning to find the $N$ at  which both algorithms perform equally well. The combined algorithm is thus not cache-oblivious. </p> <p>Tip</p> <p>There is no silver bullet. All approaches have advantages and disadvantages, some may appear in this situation  and others in another situation. The only valid reasoning is: numbers tell the tale (meten is weten):  measure the performance of your code. Measure it twice, then measure again.</p>"},{"location":"chapter-2/#supercomputer-architecture","title":"Supercomputer architecture","text":"<p>Note</p> <p>For a gentle but more detailed introduction about supercomputer architecture check out [this VSC course] (https://calcua. uantwerpen.be/courses/supercomputers-for-starters/Hardware-20221013-handouts.pdf). An updated  version will appear soon here  (look for 'Supercomputers for starters').</p> <p>We haven't talked about supercomputer architecture so far. In fact, supercomputers are not so very different from  ordinary computers. The basic building block of a supercomputer is a compute node, or a node tout court.  It can be seen as an ordinary computer but without peripheral devices (no screen, no keyboard, no mouse, ...).  A supercomputer consists of 100s to 1 000s of nodes (totalling up to 100 000s of cores), mutually connected to an  ultra-fast network, the interconnect. The interconnect allows the nodes to exchange information so that they can  work together on the same computational problem. It is the number of nodes and cores that makes a supercomputer a  supercomputer, not (!) the performance of the individual cores. Motherboards for supercomputer nodes typically have  2 sockets, each of which holds a CPU. Technically speaking they behave as a single CPU double the size, and double  the memory. Performance-wise, however, the latency across the two CPUs is typically a factor 2 larger. This goes by  the name ccNUMA, or cache coherent non-uniform memory architecture. Cache coherence means that if caches  of different copies hold copies of the same cache line, and one of them is modified, all copies are updated.  NUMA means that there are different domains in the global address space with different latency and/or bandwidth.  CPU0 can access data in DRAM1, but this is significantly slower (typically 2x). </p> <p></p>"},{"location":"chapter-3/","title":"Chapter 3 - Optimise first, then parallelize","text":"MathJax.Hub.Config({     tex2jax: {       inlineMath: [['$','$'], ['\\\\(','\\\\)']],       processEscapes: true},       jax: [\"input/TeX\",\"input/MathML\",\"input/AsciiMath\",\"output/CommonHTML\"],       extensions: [\"tex2jax.js\",\"mml2jax.js\",\"asciimath2jax.js\",\"MathMenu.js\",\"MathZoom.js\",\"AssistiveMML.js\", \"[Contrib]/a11y/accessibility-menu.js\"],       TeX: {       extensions: [\"AMSmath.js\",\"AMSsymbols.js\",\"noErrors.js\",\"noUndefined.js\"],       equationNumbers: {       autoNumber: \"AMS\"       }     }   });"},{"location":"chapter-3/#when-to-parallelize-and-what-to-do-first","title":"When to parallelize, and what to do first...","text":"<p>When your program takes too long, the memory of your machine is too small for your problem or the accuracy you need  cannot be met, you're hitting the wall. Parallelization seems necessary, and you feel in need of a supercomputer. However, supercomputers are expensive machines and resources are limited. It should come to no surprise that it is  expected that programs are allowed to run on supercomputers only if they make efficient use of their resources.  Often, serial programs provide possibilities to improve the performance. These come in two categories: common sense  optimisations (often completely overlooked by researchers) which rely on a good understanding of the mathematical  formulation of the problem and the algorithm, and code optimisations which rely on understanding processor  architecture and compilers. Lets first look at common sense optimisations. </p>"},{"location":"chapter-3/#common-sense-optimisations","title":"Common sense optimisations","text":"<p>Common sense optimizations come from a good understanding of the mathematical formulation of the problem and seeing  opportunities to reduce the amount of work. We give three examples. </p>"},{"location":"chapter-3/#1-magnetization-of-bulk-ferromagnets","title":"1. Magnetization of bulk ferromagnets","text":"<p>I was asked to speed up a program for computing the magnetisation $m(T)$ of bulk ferromagnets as a function of  temperature $T$. This is given by a self-consistent solution of the equations:</p> <p>$$ m = \\frac{1}{2 + 4\\Phi(m)} $$</p> <p>$$ \\Phi(m) = \\frac{1}{N} \\sum_{\\textbf{k}} \\frac{1}{e^{\\beta\\eta(\\textbf{k})m} - 1} $$</p> <p>with $ \\beta = 1/{k_B T} $. At $T=0$ we have $m(0) = m_0 = 0.5$, and at high $T$, $m(T)$ approaches zero.</p> <p>The solution is a curve like this:</p> <p></p> <p>The program compute this as follows: For any temperature $T$, set $m = m_0$ as an inital guess. Then iterate $ m_ {i+1} = 1/(2 + 4\\Phi(m_i)) $ until $ \\Delta m = m_{i+1} - m_i $ is small. Here,</p> <p>$$ \\Phi(m) = \\sum_{n=1}^\\infty \\frac{a}{\\pi} \\left(\\int_0^{\\pi/a} dq e^{-nm\\beta\\eta_1(q)}\\right)^3 $$</p> <p>and the integral is computed using Gauss-Legendre integration on 64 points.  </p> <p>The choice of $m=0.5$ as initial guess is obviously a good one close to $T=0$. However, looking at the graph above,  it becomes clear that as T increases the solution moves further and further away from $0.5$. Furthermore, if we  compute tempurature points at equidistant tempurature points, $T_j = \\delta j$ for some $\\delta$ and $j=0,1,2, . ..$, it is also clear that the solution of the previous temperature point, i.e. $m_{j-1}$, is a far better initial  initial guess  guess than $m_0$. This turns out to be 1.4x faster. Not a tremendous improvement, but as the graph above seems  continuous w e can take this idea a step further: using interpolation from solutions a lower temperature points to  predict the next solution and use that as an initial guess. Linear interpolation of $m_j$ from $m_{j-1}$ and  $m_{j-2}$ gives a speedup of 1.94x and quadratic interpolation from $m_{j-1}$, $m_{j-2}$ and $m_{j-3}$ a factor of 2.4x. That is a substantial speedup achieved without acttually modifying the code. This optimisation comes entirely from  understanding what your algorithm actually does. Investigation of the code itself demonstrated that it made suffered  from a lot of dynamic memory management and that it did not vectorize. After fixing these issues, the code ran an  additional 13.6x faster. In total the code was sped up by an impressive 32.6x.</p>"},{"location":"chapter-3/#2-transforming-the-problem-domain","title":"2. Transforming the problem domain","text":"<p>At another occasion I had to investigate a code for calculating a complicated sum of integrals in real space. After  fixing some bugs and some optimisation to improve the efficiency, it was still rather slow because the formula  converged slowly  As the code was running almost at peak performance, so there was little room for improvement.  However, at some point we tried to apply the Fourier transform to get an expression in frequency space. This  expression turned out to converge much faster and consequently far less terms had to be computed, yielding a speedup  of almost 2 orders of magnitude and was much more accurate. This is another example of common sense optimisation  originating in a good mathematical background. The natural formulation of a problem is not necessarily the best to  use for computation.    </p>"},{"location":"chapter-3/#3-transforming-data-to-reduce-their-memory-footprint","title":"3. Transforming data to reduce their memory footprint","text":"<p>I recently reviewed a Python code by the Vlaamse Milieumaatschappij for modelling the migration of invertebrate aquatic  species in response to improving (or deteriorating) water quality. The program read a lot data from .csv files. For  a project it was necessary to run a parameter optimisation. That is a procedure where model parameters are varied  until the outcome is satisfactory. If the number of model parameters is large the number of program runs required can  easily reach in the 100 000s. The program was parallellized on a single node. However, the program was using that many  data that 18 cores of the 128 cores available on a node already consumed all the available memory. By replacing the  data types of the columns of the datasets with datatypes with a smaller footprint, such as replacing categorical data  with integer IDs, replacing 32-bit integers with 16-bit or even 8-bit integers, float64 real numbers with float32 or  float16 numbers reduced the amount of data used by a factor 8. All of a sudden much more cores could be engaged in  the computation and the simulation sped up considerably.   </p> <p>Some of these \"common sense optimisations\" may seem obvious. Yet, of all the codes I reviewed during my career, few  of them were immune to common sense optimisation. Perhaps, developing (scientific) software takes a special mindset: </p> <p>Tip</p> <p>The scientific software developer mindset: Constantly ask yourself 'How can I improve this? How can I make  it faster, leaner, more readable, more flexible, more reusable, ... ?'</p> <p>Common sense optimisations are optimisations that in general don't require complex code analysis, require very little  code changes and thus little effort to implement them. Yet they can make a significant contribution.</p>"},{"location":"chapter-3/#code-optimisations","title":"Code optimisations","text":"<p>Code optimisations are optimisations aiming at making your solution method run as efficient as possible on the  machine(s) that you have at your disposal. This is sometimes referred as code modernisation, because code that  was optimised for the CPUs of two years a go may well need some revision for the latest CPU technology. These  optimisations must, necessarily, take in account the specific processor architecture of your machine(s). Important  topics are: </p> <ul> <li>Avoid pipeline stalls (due to impredictable branches, e.g.) </li> <li>Ensure SIMD vectorisation. On modern processors vector registers can contain 4 double precision floating point    numbers or 8 single precision numbers and vector instructions operate on these in the same number of cycles as    scalar instructions. Failing to vectorise can reduce the speed of your program by a factor up to 8x!   </li> <li>Smart data access patterns are indispensable for programs with a memory footprint that exceeds the size of the cache.    Transferring data from main memory (DRAM) to the processor's registers is slow: typical latencies are in the order    of 100 cycles (which potentially wastes ~800 single precision vectorised operations). Vector instructions are of    no help if the processing unit must wait for the data. </li> </ul> <p>This is clearly much more technical and complicated (in the sense that it requires knowledge from outside the  scientific domain of the problem you are trying to solve). Especially fixing memory access patterns can be difficult  and a lot ofwork, as you may have to change the data structures used by your program, which usually also means  rewriting a lot of code accessing the data. Such code optimisations can contribute significantly to the performance  of a program, typically around 5-10x, but possibly more. As supercomputers are expensive research infrastructure in  high demand, we cannot effort to waste resources. That being said, the lifetime of your program is also of  importance. If you are developing a code that will be run a few times during your Master project or PhD, using only  a hundred of node days, and to be forgotten afterwards, it is perhaps not worth to spend 3 months optimising it.</p> <p>Often, however, there is a way around these technicalities. If the scientific problem you are trying to solve, can  be expressed in the formalism of common mathematical domains, e.g. linear algebra, Fourier analysis, ..., there is  a good chance that there are good software libraries, designed with HPC in mind, that solved these problems for you.  In most cases there are even bindings available for your favorite progamming language (C/C++, Fortran, Python, ...). All you have to do is translate the mathematical formulation of your problem into library calls. </p> <p>Tip</p> <p>Use HPC libraries as much as possible. There is little chance that you will outperform them. Quite to the  contrary: your own code will probably do significantly worse. By using HPC libraries you gain three times:</p> <ul> <li>you gain performance,</li> <li>you gain development time as you will need a lot less code to solve your problem, less debugging, simpler    maintenance, ...</li> <li>your learn how to use the library which will get you at speed readily when you take on your next    scientific problem.</li> </ul> <p>Tip</p> <p>Don't reinvent the wheel. The wheel was invented ~8000 years ago. Many very clever people have put effort in  it and is pretty perfect by now. Reinventing it will unlikely result in an improvement. By extension: if you  need some code, spend some time google-ing around to learn what is already available and how other researchers  attack the problem. It can save you weeks of programming and debugging. Adapting someone else's code to your  needs will learn you more than coding it from scratch. You'll discover other approaches to coding problems than  yours, other language constructs, idioms, dependencies to build on, learn to read someone else's code, learn to  integrate pieces.    </p>"},{"location":"chapter-3/#when-is-code-optimized-enough","title":"When is code optimized enough?","text":"<p>Tip</p> <p>Premature Optimization Is the Root of All Evil Donald Knuth. </p> <p>This quote by a famous computer scientist in 1974 is often used to argue that you should only optimize if there is a  real need. If code is too slow, measurements (profiling) should tell in which part of the code most time is  spent. That part needs optimision. Iterate this a few times. Blind optimisation leads to useless and developer time  wasting micro-optimisations rendering the code hard to read and maintain. On the other hand, if we are writing code  for a supercomputer, it better be super-efficient. But even then, depending on the lifetime of the program we are  writing, there is a point at which the efforts spent optimising are outweighed by having to wait for the program  going in production.</p> <p>How can one judge wether a code needs further optimization or not? Obviously, there are no tricks for exposing  opportunities for common sense optimisations, nor for knowing wether better algorithms exist. That is domain  knowledgs, it comes with experience, and requires a lot of background. But for a given code and given input, can we  know wether improvements are possible? In Chapter 1 we mentioned the existence of  machine limits, the peak performance, $P_p$, the maximum number of floating point operstions that can be executed per  second, and the bandwidth, $B$, the maximum number of bytes that can be moved between main memory and the CPU's  registers per second. It is instructive to study how these machine limits govern the maximum performance $P_{max}$  as a function of the computational intensity $I_c$. If the CPU must not wait for data, $P_{max} = P_p$. In the  situation where the bandwidth is limiting the computation $ P_{max} = BI_c$. This leads to the formula: </p> <p>$$ P_{max} = min(P_p,BI_c) $$</p> <p>This is called the roofling model, since its graph looks like a roofliine.</p> <p></p> <p>We can measure the actual performance and computational intensity of the program and plot it on the graph. The point  must necessarily be under the roofline. For a micro-benchmark, such as a loop with a simple body, we can compute  $I_c$ by hand, count the Flops and time the benchmark to obtain the computational intensity. For an entire program  a performance analysis tool can construct the graph and measure where the program is in the graph. Let's discuss 4  different cases, corresponding to the four numbered spots in the graph above.</p> <ol> <li>Point 1 lies in the bandwidth limited region, but well below the roofline. Something prevents the program to go     at the maximum performance. There can be many causes: bad memory access causing cache misses, the code may fail     to vectorize, pipeline stalls, ... for a micro-benchmark you can perhaps spot the cause without help. For a     larger program a performance analyzer will highlight the problems. </li> <li>Point 2 lies in the peak performance limited region, also well below the roofline. Hence, cache misses are     unlikely the cause. </li> <li>Point 3 lies close to the roofline and the boundary between the bandwidth limited region and the peak performance     limited region. This the sweet spot. Both peak performance and bandwith are fully used.</li> <li>Point 4 lies close to the roofline in the peak performance limited region. This is an energy-efficient     computation at peak performance. It moves little data (high $I_c$). Moving data is by far the most energy     consuming part in a computation.</li> </ol>"},{"location":"chapter-3/#common-approaches-towards-parallelization","title":"Common approaches towards parallelization","text":"<p>Before we discuss common parallelization approaches, we need to explain some concepts:</p> <ul> <li> <p>process (wikipedia): \"In computing, a process is the    instance of a computer program that is being executed by one or more threads.\" A process has its own address    space, the region of main memory that can be addressed by the process. Normally, a process cannot go outside its    address space, nor can any other process go inside the process's own address space. In general, a process is    restricted to a node, and the number of parallel threads is at most equal to the nubmer of cores on the node. </p> </li> <li> <p>thread (wikipedia): \"In computer science, a thread of    execution is the smallest sequence of programmed instructions that can be managed ...\". The instructions in a    thread are thus by definition sequential. In the context of parallel computing, parallel threads are managed by the    parent process to run different tasks in parallel. Obviously, parallel threads need to run on distinct cores to be    truely concurrent. As threads belong to a process, they can in principle have access to the entire address space of    the process. </p> </li> </ul> <p>Now that we understand the concepts of processes and threads, we can explain three different types of  parallelization:</p>"},{"location":"chapter-3/#shared-memory-parallelization","title":"Shared memory parallelization","text":"<p>In shared memory parallelization there is one process with a number of threads to do work in parallel. As all  threads have in principle access to the entire memory address space, that is, they share memory, there is no need for  explicit communication. All exchange of information is doe by reading and writing to the shared memory. Typically,  shared memory parallellization involve only a single node. </p> <p>The most common framework for shared memory parallelization is OpenMP. Shared memory  parallel programs are relatively simple to construct, requiring relatively little changes to the sequentiol source  code. A limitation of shared memory parallel programsis that, in general, they are limited to a single machine  However, there exist software layers that can make a supercomputer behave as a single large machine with a single  process running and as many threads as there are cores in the set. Such systems allow to run a shared memory program  with much more threads and much more memory. This approach can be useful when distributed memory parallelization is  too expensive. </p>"},{"location":"chapter-3/#distributed-memory-parallelization","title":"Distributed memory parallelization","text":"<p>Distributed memory parallelization is the opposite of shared memory parallelization. There are many process,  each with only a single-thread. Every process has its own memory address space. These address spaces are not shared,  they are distributed. Therefor, explicity communication is necessary to exchange information. For processes on the  same machine (=node) this communication is intra-node, but for processes on distinct machines messages are sent  over the interconnect.</p> <p>Distributed memory programs are considerably more complex to write, as the communication must be explicitly handled  by the programmer, but may use as many processes as you want. Transformation of a sequential program into a  distributed memory program is often a big programming effort. The most common framework is MPI. </p>"},{"location":"chapter-3/#hybrid-memory-parallelization","title":"Hybrid memory parallelization","text":"<p>Hybrid memory parallelization combines both approaches. It has an unlimited number of processes, and a number of  threads per process, which run in parallel in a shared memory approach (OpenMP). The process communicate with each  other using MPI. Typically, the computation is organised as one proces per NUMA domain and one thread per core in  dat NUMA domain.  </p> <p>This approach uses shared memory parallelization where it is useful (on a NUMA domain), but removes the limitation  to a single machine. It has less processes, and thus less overhead in terms of memory footprint, and  communication overhead. It is also a bit more complex that pure distributed memory parallelization, and much more  complex that shared memory parallelization.</p>"},{"location":"chapter-4/","title":"chapter 4 - Case studies","text":""},{"location":"chapter-4/#_1","title":"chapter 4 - Case studies","text":""},{"location":"evaluation/","title":"Evaluation","text":"<p>In this course you will be learning by doing. You will be given an assignment, a (parallel)  programming task on which you will work for several weeks, under my supervision and with my support. The code that  you write must be regularly committed to a GitHub repository. This has many advantages:</p> <ul> <li>First, it serves as a backup. Every single commit can be retrieved at all times. So, you can't loose your code,    even not the older versions.</li> <li>Everyone with access to the repository can access the code. If you keep the repository public, that means everyone    with access to the internet. If you make it private, only the people you invite can access. </li> <li>It is important that you give me access. If you have problems, I can clone your repository and debug it to see    what is going wrong,</li> <li>If you cooperate with another student on the project you can exchange updates easily. You can make use of git    branches to avoid bother other people with your code changes before they are correct.  </li> </ul> <p>At our final session you will give a presentation about your work</p> <ul> <li>explaining the problems you encountered,</li> <li>explaining your approach,</li> <li>providing performance measurements for the different versions your code, and for different node counts,</li> <li>explaining the performance measurements,</li> <li>explaining what you found difficult during this course.</li> </ul> <p>After the presentation I will ask some questions, mainly because I am curious and eager to learn something, but also  to ensure that you understand what you present. </p> <p>The presentation must be added to your GitHub repository before you give your presentation. I will clone it and keep  it as a proof of your results.</p>"},{"location":"glossary/","title":"Glossary","text":"<p>Here is an alphabetical list of terms with links to where they are explained in the text.</p>"},{"location":"glossary/#a","title":"A","text":"<ul> <li>address space</li> <li>array of structures</li> </ul>"},{"location":"glossary/#b","title":"B","text":"<ul> <li>bandwidth limited</li> </ul>"},{"location":"glossary/#c","title":"C","text":"<ul> <li>Cache coherence</li> <li>cache coherent non-uniform memory architecture</li> <li>cache line</li> <li>cache-oblivious</li> <li>ccNUMA</li> <li>code modernisation</li> <li>Code optimisations</li> <li>Common sense optimizations</li> <li>communication</li> <li>computational complexity</li> <li>computational intensity</li> <li>compute limited</li> <li>compute node</li> </ul>"},{"location":"glossary/#d","title":"D","text":"<ul> <li>distributed memory parallelization</li> </ul>"},{"location":"glossary/#e","title":"E","text":"<ul> <li>embarrassingly parallel</li> </ul>"},{"location":"glossary/#i","title":"I","text":"<ul> <li>instruction pipelining</li> </ul>"},{"location":"glossary/#l","title":"L","text":"<ul> <li>L1 cache</li> <li>L2 cache</li> <li>L3 cache</li> <li>loop fusion</li> </ul>"},{"location":"glossary/#m","title":"M","text":"<ul> <li>main memory</li> <li>memory bandwidth</li> <li>multi-core</li> <li>multi-processor</li> </ul>"},{"location":"glossary/#n","title":"N","text":"<ul> <li>node</li> <li>NUMA</li> </ul>"},{"location":"glossary/#p","title":"P","text":"<ul> <li>parallel program</li> <li>peak performance</li> <li>pipeline stalls</li> <li>process</li> </ul>"},{"location":"glossary/#r","title":"R","text":"<ul> <li>registers</li> </ul>"},{"location":"glossary/#s","title":"S","text":"<ul> <li>sequential</li> <li>serial</li> <li>shared memory parallelization</li> <li>SIMD vectorisation</li> <li>Spatial locality</li> <li>structure of Arrays</li> </ul>"},{"location":"glossary/#t","title":"T","text":"<ul> <li>Temporal locality</li> <li>thread</li> <li>tiling</li> <li>time to solution</li> </ul>"},{"location":"guide-lines/","title":"Assignment guide lines","text":"<p>Note</p> <p>This is work in progress ...</p> <p>The assignment is there because imho programming is something you can only learn by doing. It  involves important skills that you should develop while working on the assignment:</p> <ul> <li>Understand the math of the problem and the algorithm to solve it,</li> <li>Do research on the problem, with respect to solution algorithms and implementation issues.</li> <li>Understand the working of a computer and a supercomputer.</li> <li>Reason about the implementation of the algorithm.</li> <li>Write and debug code in Python. </li> <li>Learn how slow functions can be sped up by converting them to either C++ or Fortran. </li> <li>Run your code on one of the UAntwerp HPC clusters.</li> </ul> <p>Learning is an incremental process. Especially for scientific software development the following is a good approach:</p> <ol> <li>Try, and test (We'll see what testing exactly means). </li> <li>Fail (often, the faster you fail, the faster you learn! ).   </li> <li>Think and do research (Google - or any other good search engine, for that matter - is your best friend), and     come up with an improvement. This is the hardest part, it requires intelligence and creativity.</li> <li>Iterate, i.e. restart at 1., until you no more fail and are satisfied with the solution.</li> <li>Document your itinerary. Document your classes, functions, variables, and keep track of the documents     that guided you to solving the problems encountered. When you will look at your work three months (only!) after     you left it as is, you will wonder what it was all about if you didn't document it. </li> </ol> <p>Although this approach may look as if you are supposed to find the solution to the problem in books or on the World  Wide Web, this does not at all exclude creativity. Learning about how other researchers approached a problem, can  easily spark new ideas that get you going. The fail fast, fail often principle also </p> <ul> <li>urges you to start as simple as possible and </li> <li>make incremental changes. </li> </ul> <p>Don't write a lot of code before you try and test. Typically, and this is corroborated by research, one bug is introduced with every new 10 lines. Finding 10 bugs in 100 lines is  a lot more difficult than finding one bug in 10 lines (although sometimes there is more than one bug :( ). </p> <p>A PhD student once asked me for support. He had written a 10 000 line Fortran program (without tests). When he ran it, the results were not what he expected and he suspected that there was a bug  'somewhere'. He asked if I could help him find the bug and - no kidding - by the end of next week because the  program had to go into production by then. I had to disappoint him and told him that he needed a true magician,  which I am not. Obviously, the program was flawed in some sense, but other than a bug it might just as well be:</p> <ul> <li>The program contains many bugs, which is very well possible in view of its size.</li> <li>The algorithm for solving the problem is inappropriate.</li> <li>There is an accuracy problem, related e.g. discretisation of time, space, or a insufficient basis function for    expanding the solution, or related to the finite precision of floating point numbers. (Floating point numbers are a    processor's approximation of the real numbers, but they do have different mathematical properties. E.g. floating    point addition is not commutative.)</li> <li>The mathematical formulation itself could be flawed or misunderstood.</li> <li>It is even possible that the program is correct but that the researchers expectations are wrong. </li> <li>...</li> </ul> <p>It could easily take weeks, if not months to write tests for the individual components to learn about the behaviour  of the program and narrow down to the source of the error. </p> <p>For this reason a sound strategy for scientific software development that makes sure that the code you write has a  sense of correctness is indispensable. Had the researcher come to me before he started programming this is the  advice he would have been given: </p> <p>Advice</p> <p>Write 5 lines of code and test them before you proceed (with the next 5 lines). Just 5, not 10! Your test  code is also code and will initially contain bugs as well. As you get more experienced you may increase that  number  to 6, even 7, ...  </p> <p>Admittedly, this advice is slightly biased to the conservative side, but I hope you get the point. You will be  surprised how many mistakes you make, being a novice. But as you will discover the source of error soon, your  progress will not come to halt. Instead you will learn fast and your progress will even speed up. I will give you  practical tools to accomplish this. </p>"},{"location":"over-de-auteur/","title":"Over de auteur","text":""},{"location":"over-de-auteur/#engelbert-tijskens","title":"[Engel]bert Tijskens","text":"<ul> <li>Lic. Aard- en delfstofkunde, Master in Physics of Microelectronics and Material Sciences, Doctor in de    Natuurwetenschappen - ik heb dus eigenlijk geen opleiding genoten in programmeren of wetenschappelijk rekenen ...    maar veel opleidingen bijgewoond, en vooral veel gelezen tijdens mijn voortdurende zoektocht naar betere manieren    om de opdrachten waar ik voor stond uit te voeren.  </li> <li>ik werk sinds 2012 voor CalcUA, de UA    kernfaciliteit voor supercomputing, en voor het VSC, het Vlaams Supercomputer Centrum.   Ik verzorg er opleiding en ondersteuning van onderzoekers rond wetenschappelijk programmeren voor HPC-omgevingen    en performantie-analyse. k ben gepassioneerd door Python, C++, Fortran en libraries en frameworks    waarmee hoog-performante en parallelle applicaties kunnen gebouwd worden.</li> <li>Sinds 2014 geef ik het vak \"Parallel programmeren\". Ik geef graag les en wil mijn ervaring van 30 jaar    wetenschappelijk programmeren delen met jonge onderzoekers.</li> <li>Voor 2012 leidde ik de DEM Research Group aan de KU Leuven. DEM staat voor Discrete Element Modelling.    Je kan het vergelijken met Molecular Dynamics, maar dan in de macroscopische wereld met atomen die een vorm    hebben, korrels dus, of grains in het Engels. Daarom wordt het ook Granular Dynamics genoemd.    Korrelstromen komen in heel wat industri\u00eble processen voor en het modelleren ervan is interessant, om inzicht te    verwerven in korrelige processen en om er goede procesinstallaties voor te ontwerpen. Dit is erg uitdagend omdat de    fysica van korrelige processen zo complex is. In tegenstelling tot MD zijn interacties tussen korrels dissipatief    en worden de contactkrachten bepaald door materiaaleigenschappen en oppervlakte-eigenschappen en zijn er zowel    normale als tangenti\u00eble contactkrachten (wrijving). Bovendien zijn de korrels - afhankelijk van het materiaal -    soms vervormbaar, of zelfs breekbaar. Omdat korrelige processen vaak over heel veel deeltjes gaan, zijn    performantie en parallellisatie essentieel. De simulatiesoftware waar we toen aan werkten, wordt nu    gecommercialiseerd door Mpacts. </li> </ul>"},{"location":"over-de-auteur/#enkele-voorbeelden","title":"Enkele voorbeelden","text":"<p>Kijk op Mpacts case studies voor meer voorbeelden. </p>"},{"location":"over-de-auteur/#cnh-maaidorser-ontwikkeld-met-mpacts","title":"CNH maaidorser - ontwikkeld met Mpacts","text":"<p>Hier zijn verscheidene korrelige processen aan de orde:</p> <ul> <li>de strohalmen maaien en binnen trekken in de machine,</li> <li>de graantjes losmaken van de aar en ze scheiden van het stro,</li> <li>stro afvoeren naar achter,</li> <li>het kaf van het koren scheiden (kaf weg blazen),</li> <li>het graan transporteren naar de verzamelbak bovenaan de machine,</li> <li>het graan transporteren van de verzamelbak naar de aanhangwagen achter de tractor,</li> </ul>"},{"location":"over-de-auteur/#spherische-korrels-die-op-een-trampoline-vallen","title":"Spherische korrels die op een trampoline vallen","text":""},{"location":"over-de-auteur/#mpacts-ijsbreker","title":"Mpacts: ijsbreker","text":"<p>Mpacts simulatie van een ijsbreker die door een ijslaag breekt.</p> <p></p>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#goals","title":"Goals","text":"<ul> <li>What is parallel programming?</li> <li>Why parallel programming?</li> <li>Performance is important</li> <li>How to parallelize a program?</li> <li>Tools</li> <li>Principles and best practices</li> <li>Strategy for scientific software development</li> </ul>"},{"location":"overview/#background-knowledge","title":"Background knowledge","text":"<ul> <li>The working of a modern processor</li> <li>CPU architectory and hierarchical memory architecture<ul> <li>A short introduction: Memory location matters for performance</li> <li>A very good talk about this topic, you need to see this: Scott Meyers on Cpu Caches    and Why You Care </li> </ul> </li> <li>Accelerators (GPU), increasingly important topic, but we cannot treate everything in this course. </li> <li>The architecture of a supercomputer</li> <li>Nodes</li> <li>Interconnect</li> <li>Accelerators (GPU), increasingly important topic, but we cannot treate everything in this course. </li> </ul>"},{"location":"public/enable-latex/","title":"Enable latex","text":"MathJax.Hub.Config({     tex2jax: {       inlineMath: [['$','$'], ['\\\\(','\\\\)']],       processEscapes: true},       jax: [\"input/TeX\",\"input/MathML\",\"input/AsciiMath\",\"output/CommonHTML\"],       extensions: [\"tex2jax.js\",\"mml2jax.js\",\"asciimath2jax.js\",\"MathMenu.js\",\"MathZoom.js\",\"AssistiveMML.js\", \"[Contrib]/a11y/accessibility-menu.js\"],       TeX: {       extensions: [\"AMSmath.js\",\"AMSsymbols.js\",\"noErrors.js\",\"noUndefined.js\"],       equationNumbers: {       autoNumber: \"AMS\"       }     }   });"}]}