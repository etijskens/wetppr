{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome at Parallel programming","text":""},{"location":"#welcome-at-parallel-programming","title":"Welcome at Parallel programming","text":""},{"location":"#welkom-bij-parallel-programmeren","title":"Welkom bij Parallel programmeren","text":"<p>Material for the course 2000wetppr of the University of Antwerp</p> <p></p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Over de auteur</li> <li>Overview</li> <li>Glossary</li> <li>Links</li> <li>(Un)sorted links</li> <li> Programming quotes :-)</li> <li>Chapter 1 - Introduction</li> <li>Chapter 2 - Aspects of modern CPU architecture</li> <li>Chapter 3 - Optimise first, then parallelize</li> <li>Chapter 4 - Case studies</li> <li>Chapter 5 - A strategy for the development research software</li> <li>Chapter 6 - Tools for parallellization</li> <li>Evaluation</li> <li>VSC infrastructure</li> </ol>"},{"location":"assignment-2022-23/","title":"Assignment 2022-23","text":""},{"location":"assignment-2022-23/#assignment-2022-23","title":"Assignment 2022-23","text":""},{"location":"assignment-2022-23/#the-mandelbrot-set","title":"The Mandelbrot set","text":"<p>The Mandelbrot set  is defined as the set of all complex numbers , for which the  iterative scheme </p> <p> </p> <p>is bounded, i.e. </p> <p> </p> <p>It can be demonstrated that, if for some , , the iteration is  guaranteed to be unbounded, and thus . This yields a practical criterion to compute an approximate Mandelbrot set , containing as all complex numbers for which the first  iterations remain inside the circle with radius 2 (centered at the origin), i.e. for which  for all  .</p> <p>The Mandelbrot set , or its approximation , is traditionally coloured black, while the  other points, the points escaping outside the circle with radius 2 towards infinity, get a colour that indicates how  fast they escape. Here is an example (from wikipedia):</p> <p></p> <p>Here is the challenge for you.</p>"},{"location":"assignment-2022-23/#the-mandelbrot-challenge","title":"The Mandelbrot challenge","text":"<p>Take the complex numbers  for which  and , i.e. about the upper half of the figure above (the lower half is symmetric). Compute the approximate  mandelbrot sets ,  and  with pixel densities  of 100,  500, and 2500 pixels per unit length. You may use the center of the pixels for the points . Time each of these 9  cases. The rectangle measures  by , so you get images of  pixels,  pixels, and  pixels, resp. </p> <p>It is best to choose the array of pixels such that the coordinate axes pass through the center of pixels. Thus, the  complex number corresponding to a pixel, that is the pixel's center, is always given by </p> <p> </p> <p>with  and  integer constants.   </p> walltime[s] d=50 d=500 d=5000 sum N=100 N=1000 N=10000 sum Total <p>This a competition! The winner, that is the one with the smallest total compute time Total over the 9 cases,  will get 5 points, the second one 4 points, and so on. The remaining 15 points are to be earned on the presentation.</p> <p>As in every competition, there are a few rules:</p> <ul> <li>Your code must be a Python program, but you may use C++ or Fortran to build your own Python modules for speed.</li> <li>The timings must be run on a single compute node of Vaughan,    the Tier-2 VSC-cluster of the University of Antwerp. Every compute node on Vaughan has 64 cores, all of which you can    use to    parallellise the work and increase the througput. </li> <li>Every case must be computed in at most one job. If you like, you may run all cases together in one job, but    splitting a case over more jobs is not allowed.</li> <li>Obviously, the github repo that you will use to store the different versions of your code, the presentation and    the results, must contain all necessary files to rerun your program, as to verify the correctness of the results and    the timings. </li> <li>You must save the image to disk. However, the timing must only include computing the image, not saving it to disk.  </li> </ul> <p>Success! </p>"},{"location":"assignment-2022-23/#plotting-the-images","title":"Plotting the images","text":"<p>Getting the nice images of the Mandelbrot set you may encounter on the web isn't as easy as it seems. Although this is  not part of the challenge, I can understand your dissappointment if after spending all the effort to optimise and  parallelise your codes, the images do not match your expectations. However, your get a long way by applying a  transformation to the escape count (the pixel value), and mapping that to a color map. Here are some pointers:</p> <ul> <li>Plotting algorithms for the Mandelbrot set</li> <li>A related question on stackoverflow</li> <li>How to plot the mandelbrot set - adding some colors</li> </ul>"},{"location":"assignment-ideas/","title":"Assignment ideas","text":"<ul> <li>Compute N terms of the sum of inverse squares</li> <li>The Mandelbrot set</li> <li>How Mojo\ud83d\udd25 gets a 35,000x speedup over Python \u2013 Part 1</li> <li>How Mojo\ud83d\udd25 gets a 35,000x speedup over Python \u2013 Part 1</li> <li>Compute pi using the Saha-Sinha formula</li> <li>String theorists accidentally find a new formula for pi</li> <li>New Recipe for Pi - Numberphile</li> <li>An amazing thing about 276 - Numberphile</li> </ul>"},{"location":"chapter-1/","title":"Chapter 1 - Introduction","text":""},{"location":"chapter-1/#chapter-1-introduction","title":"Chapter 1 - Introduction","text":"<p>Material:</p> <ul> <li>this website,</li> <li>which is built with <code>mkdocs</code> from this GitHub repo. The repo contains also   some example code and code for some of the case studies of chapter 4 in directories <code>wetppr</code> and <code>tests/wetppr</code>. - some presentations found here. (Most of these   need some reworking, especially those not in VSC format).</li> </ul>"},{"location":"chapter-1/#overview","title":"Overview","text":"<ul> <li>What is a parallel program?</li> <li>Possible reasons to parallelize a program.</li> </ul>"},{"location":"chapter-1/#what-is-a-parallel-program","title":"What is a parallel program?","text":"<p>A parallel program is a program that distributes its work over different processing units such that parts of its work load can be computed simultaneously. At the end the program gathers the partial results from the processing units and combines them in a global result. If the tasks are independent of each other, the program is called embarrassingly parallel. In general, the individual tasks are not independent and need to exchange information. This is called communication. The opposite of a parallel program is a serial or sequential program, executing all its instructions one after the other. </p>"},{"location":"chapter-1/#possible-reasons-to-parallelize-a-program","title":"Possible reasons to parallelize a program","text":""},{"location":"chapter-1/#1-reduce-the-time-to-solution","title":"1. Reduce the time to solution","text":"<p>The term time to solution in general means the time your machine needs to solve a computational problem. If the problem can be divided in smaller tasks that can be computed simultaneously, the time to solution decreases. If a company can solve a research or engineering question in a week or a day, that is an important difference. A processing unit has a maximum number of instructions it can execute per second, this is called its peak performance. Obviously, the peak performance is a machine limit puts a hard limit to what a processing unit can achieve in a given amount of time. But instructions operate on data, and moving data from main memory takes time as well. A program that must process lots of data but does little computation is limited by the speed at which the processing unit can fetch data from the main memory. This is called the memory bandwidth (usually in Mbits/s).</p> <p>Programs that do a lot of computation and does not move a lot of data in or out of main memory is called compute limited. A program that moves a lot of data and little computation is bandwidth limited. While in the past programs used to be compute bound, today, most programs are memory bound, because the speed of the processing units increased much faster than the speed of memory. As a consequence, efficient memory access patterns are crucial to the performance of a program.  </p>"},{"location":"chapter-1/#2-solve-bigger-problems-in-the-same-time","title":"2. Solve bigger problems in the same time","text":"<p>There is a third machine limit that plays a role, namely the amount of main memory. This puts a limit on the size of the problem that can be treated, e.g. the number of volume elements in a CFD simulation or the number of atoms in a MD simulation. If the program can distribute the work over, say 10 machines, it has 10 times the amount of memory at its disposition and thus can solve a 10 times bigger problem.  </p>"},{"location":"chapter-1/#3-produce-more-accurate-solutions","title":"3. Produce more accurate solutions","text":"<p>More accuracy can come from more complex physical models, or from using more basis functions to expand the solution. This leads to more computation and perhaps a prohibitively long time to solution. Problems involving discretisation (the process of dividing the domain of a computational problem in small elements, as in computational fluid dynamics and finite element modelling) the accuracy typically improves when the elements get smaller, as in approximating the integral under a curve by rectangles. In both cases parallelization of the program may be necessary to obtain a solution. </p>"},{"location":"chapter-1/#4-competition","title":"4. Competition","text":"<p>If a program that is in competition with other programs that solve the same problem, parallelization will allow it to reduce the time to solution, to compute bigger problems and achieve more accurate solution. This is, obviously, a competitive advantage.</p>"},{"location":"chapter-1/#why-can-i","title":"Why can I?","text":"<p>Can't I just buy a bigger and faster computer?</p> <p>Nope, that fairy tale ended approximately at the beginning of this century with the advent of the multi-processor computer, also called multi-core computer. Increasing the peak performance by increasing the clock frequency was no longer possible, because the power consumption of a processor increases as the third power of the clock frequency. At a certain point it became impossible or too expensive to cool the processor. The only way to get a processor execute more instructions per second was to put more processing units on it (cores). At that point serial program became even slower on the new multi-processors because the clock frequency was reduced to remain inside the power envelope. Moore's law predicts that the number of transistors in a processor doubles every 18 months due to increasing miniaturization. With this the combined peak performance of the multi-processors increases as well, but the peak performance of the individual processing units no longer does. This makes it necessary to parallelize programs in order to keep up with Moore's law. It must be said that the increase of peak performance was not always in line with Moore's law. At some point the peak performance of processing units was increased by adding parallelization concept in single processing units like pipelining and SIMD vectorisation. We'll come to that later.</p>"},{"location":"chapter-2/","title":"Chapter 2 - Aspects of modern CPU architecture impacting performance","text":""},{"location":"chapter-2/#chapter-2-aspects-of-modern-cpu-architecture-impacting-performance","title":"Chapter 2 - Aspects of modern CPU architecture impacting performance","text":"<p>Modern CPUs deliver an enormous amount of FLOPs (floating point operations per second). This is the peak performance of the machine. The importance of peak performance, in general however, is rather limited. In practice, very little codes, or algorithms, can achieve the peak performance of modern CPUs. The reason for this astonishing fact is that instuctions must operate on data. That data must first be moved from the main memory (DRAM) to the registers of the ALU (Arithmetic and Logic Unit) that executes the instructions, and moving data takes time. In fact, generally much more than executing floating point operations. This is extremely well explained in this presentation by Stephen Jones from NVIDIA: How GPU computing works. Despite the title, it also explains how CPU computing works, contrasting the architectural design of CPUs and GPUs to achieve two different things. We will not deal with GPUs in this course, but for examining wether a code can profit from being run on a GPU an understanding of this presentation is highly beneficial.</p>"},{"location":"chapter-2/#summary-of-how-gpu-computing-works","title":"Summary of How GPU computing works","text":"<ul> <li> <p>[0:00-5:00] The compute intensity of a device is the peak performance (FLOPs) divided by the data rate. The data rate is the bandwidth of the device (GBytes/s) divided by the size of a single data item. A single precision (SP) floating point number is 4 Bytes and a double precision (SP) floating point number is 8 Bytes. The bandwidth of a device is the amount of data that can be transferred between the CPU and the main memory (DRAM) of the device. As the presentation explains, the compute intensity for double precision data of modern CPUs is 80 (40 for single precision). This basically means that, on average, a DP data item coming from main memory should be used by 80 floating point operations in order to keep the CPU busy. That is a pritty tall number. In fact there aren't many algorithms that have to do that much work to do on each data item. </p> </li> <li> <p>[5:30] \"Notice I count loads and not stores, because I don't care about stores, because I don't have to wait for them.\" Obviously, the loads of <code>x[i]</code> and <code>y[i]</code> must precede the computation, and the store of <code>y[i]</code> can, in principle be executed any time after the result is computed. This is a bit of a simplification. The store must at least be executed before the register can be reused, which is, in view of the limited number of registers pritty soon. Furthermore, stores consume bandwidth too. Roughly, two loads and one store use 1.5 times the bandwidth of a two loads. So, while you don't have to wait for the store, you wait a bit longer for the two loads.  </p> </li> <li> <p>[6:10] Memory latency the time between sending a load request and receiving the data item requested.</p> </li> <li> <p>[6:40] pipelining. See Instruction pipelining</p> </li> <li> <p>[15:10] The focus of the presentation is now on GPUs. </p> </li> </ul> <p>Although this presentation explains why memory bandwidth and memory latency matter more than FLOPs and how the GPU solves the latency problem, it does not talk about how the CPU solves it (probably intentionally). This is explained in another presentation by Scott Meyers CPU Caches and Why You Care. Nothwithstanding the presentation is from 2013 and uses a CPU from 2010, the principles are fully valid today, although the numbers will be different for today's CPUs.</p>"},{"location":"chapter-2/#summary-of-cpu-caches-and-why-you-care","title":"Summary of CPU Caches and Why You Care","text":"<ul> <li> <p>[0:01:10-0:04:05] example 1: matrix traversal,traversal order matters.</p> </li> <li> <p>[0:04:05-0:10:30] example 2: parallel threads,thread memory access matters.</p> </li> <li> <p>[0:10:30-] understanding CPU caches (see The hierarchical structure of CPU Memory below).</p> </li> <li> <p>[0:20:20] 'Non-cache access can slow down things by orders of magnitude' cache misses.</p> </li> <li> <p>[0:20:20] 'compact, well localized code that fits in cache is fastest'</p> </li> <li> <p>[0:20:20] 'compact data structures that fit in cache are fastest'.</p> </li> <li> <p>[0:20:20] 'data structure traversals touching only cached data are fastest'.</p> </li> <li> <p>[0:21:40] The concept of cache lines. Cache favor linear traversal of memory. The concept of prefetching. A linear array, being traversed linearly, is the fastest data structure you can possibly have. CPU hardware is designed with this purpose in mind. </p> </li> <li> <p>[0:24:30] Implications</p> <ul> <li> <p>Locality counts: reads/writes at address A =&gt; contents near A are already cached</p> <ul> <li>e.g. on the same cache line</li> <li>e.g. on a nearby cache line that was prefetched</li> </ul> </li> <li> <p>Predictable access patterns count. </p> <ul> <li>predictable ~ forward or backward traversal.</li> </ul> </li> <li> <p>linear traversals very cache-friendly</p> <ul> <li>excellent locality, predictable traversal pattern</li> <li>linear array search can beat  searches of heap-based binary search trees</li> <li>  binary search of a sorted array can beat  searches of heap-based hash tables.</li> <li>Big-Oh wins for large , but hardware caching takes early lead</li> </ul> </li> </ul> </li> <li> <p>[0:27:30] cache coherency explains the behavior of the multi-threaded example 2. It's called false sharing.</p> </li> <li> <p>[0:45:00] Guidance</p> <ul> <li> <p>for data </p> <ul> <li>Where practical employ linear array traversals (if needed sort)</li> <li>Use as much of a cache line as possible</li> <li>Be alert for false sharing in multi-threaded systems</li> </ul> </li> <li> <p>for code</p> <ul> <li>Fit working set in cache. Avoid iteration over heterogeneous sequences with virtual calls, i.e. sort sequences by type.</li> <li>make 'fast-path' branch-free sequences, use up-front conditionals to screen out slow cases</li> <li>inline cautiously. Inlining reduces branching and facilitates code-reducing optimizations, but code duplications reduces the effective cache size.</li> </ul> </li> <li> <p>Take advantage of profile-guided optimization (PGO) and whole program optimization (WPO)</p> </li> </ul> </li> <li> <p>[1:01:00] Cache associativity</p> </li> </ul> <p>Below, we explain some details of modern CPU architecture with an impact on performance. We start out with the memory of a CPU because, as the presentation explains, that is key to performance. Then, we explain some tricks of the CPU to achieve the peak performance delivered by modern CPUs.</p>"},{"location":"chapter-2/#the-hierarchical-structure-of-cpu-memory","title":"The hierarchical structure of CPU Memory","text":"<p>CPU memory of modern CPUs is hierarchically organised. The memory levels close to the processor need to be fast, to  serve it with data so that it can continue its work. As fast memory is expensive, the levels close to the processor  are also smaller. The farther away from the processor the bigger they are, but also the slower.    </p> <ul> <li>Each processing unit (or core) has a number of registers (~1 kB) and vector registers on which instructions can    immediately operate (latency = 0 cycles). The registers are connected to  </li> <li>a dedicated L1 cache (~32 kB per core), with a latency of ~ 1 cycle. This is in turn    connected to:  </li> <li>a dedicated L2 cache, (~256 kB per core), with a latency of ~10 cycles. This is in turn connected to: </li> <li>the L3 cache, which is shared among a group of cores, (~2 MB per core), with a latency of ~50 cycles.    This is connected to:</li> <li>the main memory, which is shared by all cores,(256 GB - 2 TB), with a latency of ~200 cycles.</li> </ul> <p>The cores and the caches are on the same chip. For this reason they are considerably faster than the main memory.  Faster memory is more expensive and therefor smaller. The figure below illustrates the layout.</p> <p></p> <p>The I/O hub connects the cpu to the outside world, hard disk, network, ...</p> <p>When an instruction needs a data item in a register, the CPU looks first in the L1 cache, if it is there it will it to the register that was requested. Otherwise, the CPU looks in L2. If it is there, it is copied to L1 and the register. Otherwise, the CPU looks in L3. If it is there, it is  copied to L2, L1 and the register. Otherwise, the CPU looks copies the cache line surrounding the data item to  L3, L2, L1 and the data item itself to the register. A cache line is typically 64 bytes long and thus can contain 4  double precision floating point numbers or 8 single precision numbers. The main consequence of this strategy is that  if the data item is part of an array, the elements of that array surrounding the requested element will also be copied to L1 so that when  processing the array the latency associated with main memory is amortized over 4 or 8 iterations. In addition, the  CPU will notice when it is processing an array and prefetch the next cache line of the array in order to avoid that  the processor has to wait for the data again. This strategy for loading data leads to two important best practices for  making optimal use of the cache.</p> <ol> <li>Exploit Spatial locality: Organize your data layout in main memory in a way that data in a cache line are mostly     needed together. </li> <li>Exploit Temporal locality: Organize your computations in a way that once a cache line is in L1 cache, as much as     possible computations on that data are carried out. This favors a high computational intensity (see below).     Common techniques for this are loop fusion and tiling. </li> </ol>"},{"location":"chapter-2/#loop-fusion","title":"Loop fusion","text":"<p>Here are two loops over an array <code>x</code>:</p> <pre><code>for xi in x:\n    do_something_with(xi)\nfor xi in x:\n    do_something_else_with(xi)\n</code></pre> <p>If the array <code>x</code> is big, too big to fit in the cache, the above code would start loading <code>x</code> elements into the cache, cach line by cache line. Since <code>x</code> is to large to fit in the cache, at some point, when the cache is full, the CPU  will start to evict the cache lines that were loaded long time a go ane are no more used to replace them with new  cache lines. By the time the first loop finishes, the entire beginning of the <code>x</code> array has been evicted and the  scond loop can start to transfer <code>x</code> again from the main memory to the registers, cache line by cache lina. this  violiate the temporal locality principle. So, it incurs twice the data traffic. Loop fusion fuses the two loops into  one and does all computations needed on <code>xi</code> when it is in the cache. </p> <pre><code>for xi in x:\n    do_something_with(xi)\n    do_something_else_with(xi)\n</code></pre> <p>The disadvantage of loop fusion is that the body of the loop may become too large and require more vector registers  than are available. At that point some computations may be done sequentially and performance may suffer. </p>"},{"location":"chapter-2/#tiling","title":"Tiling","text":"<p>Tiling is does the opposite. Ik keeps the loops separate but restricts them to chunks of <code>x</code> which fit in L1 cache.</p> <pre><code>for chunk in x: # chunk is a slice of x that fits in L1\n    for xi in chunk:\n        do_something_with(xi)\n    for xi in chunk:\n        do_something_else_with(xi)\n</code></pre> <p>Again all computations that need to be done to <code>xi</code> are done when it is in L1 cache. Again the entire <code>x</code> array is  transferred only once to the cache. A disadvantage of tiling is that the chunk size needs to be tuned to the size of  L1, which may differ on different machines. Thus, this approach is not cache-oblivious. Loop fusion, on the  other hand, is cache-oblivious.</p> <p>A good understanding of the workings of the hierarchical structure of processor memory is required to write  efficient programs. Although, at first sight, it may seem an overly complex solution for a simple problem, but it is  a good compromise to the many faces of a truly complex problem.\\</p> <p>Tip</p> <p>An absolute must-see for this course is the excellent presentation on this matter by Scott Meyers:  CPU Caches and Why You Care.  (We can also recommend all his  books on C++). </p> <p>This, in fact, holds for GPUs as well. An equally interesting presentation is How GPU computing works by Stephen Jones from NVIDIA.</p>"},{"location":"chapter-2/#intra-core-parallellisation-features","title":"Intra-core parallellisation features","text":"<p>Modern CPUs are designed to (among other things) process loops as efficiently as possible, as loops typically  account for a large part of the work load of a program. To make that possible CPUs use two important concepts:  instruction pipelining (ILP) and SIMD vectorisation. </p>"},{"location":"chapter-2/#instruction-pipelining","title":"Instruction pipelining","text":"<p>Instruction pipelining is very well explained here.</p> <p>Basically, instructions are composed of micro-instructions (typically 5: instruction Fetch (IF), instruction decode  (ID), execute (EX), memory access (MEM), write back (WB), details here), each of which are executed in separate  hardware units of the CPU. By executing the instructions sequentially, only one of those units would be active at a  time: namely, the unit responsible for the current micro-instruction. By adding extra instruction registers, all  micro-instruction hardware units can work simultaneously, but on micro-instructions pertaining to different but  consecutive instructions. In this way, on average 5 (typically) instructions are being executed in parallel. This is  very useful for loops.</p> <p>Executing micro-instructions serially, without pipelining:</p> <p></p> <p>Pipelined execution of micro-instructions, in the middle part 5 micro-instructions are executed simultaneously,  which means that on average 5 instructions are executed simultaneously:</p> <p></p> <p>There are a couple of problems that may lead to pipeline stalls, situations where the  pipeline comes to halt. </p> <ol> <li>A data element is requested that is not in the L1 cache. It must be fetched from deeper cache levels or even     from main memory. This is called a cache miss. A L1 cache miss means that the data is not found in L1, but is     found in L2. In a L2 cache miss it is not found in L2 but it is in L3, and a L3 cache miss, or a cache miss tout     court* de data is not found in L3 and has to be fetched from main memory. The pipeline stops executing for a     number of cycles corresponding to the latency of that cache miss. Data cache misses are the most important cause     of pipeline stalls and as the latency can be really high (~100 cycles).  </li> <li>A instruction is needed that is not in the L1 instruction cache. This may sometimes happen when a (large)     function is called that is not inlined. Just as for a data cache miss, the pipeline stalls for a number of cycles     corresponding to the latency of the cache miss, just as for a data cache miss. </li> <li>You might wonder how a pipeline proceeds when confronted with a branching instruction, a condition that has to be     tested, and must start executing different streams of instructions depending on the outcome (typically     if-then-else constructs). Here's the thing: it guesses the outcome of the test and starts executing the     corresponding branch. As soon as it notices that it guessed wrong, which is necessarily after the condition has been     tested, it stops, steps back and restarts at the correct branch. Obviously, the performance depends on how well     it guesses. The guesses are generally rather smart. It is able to recognize temporal patterns, and if it doesn't     find one, falls back on statistics. Random outcomes of the condition are thus detrimental to performance as its    guess will be wrong at least half the time.</li> </ol>"},{"location":"chapter-2/#simd-vectorisation","title":"SIMD vectorisation","text":"<p>Scalar arithemetic, e.g. the addition, in CPUs operates as follows: the two operands are loaded in two (scalar)  registers, the add instruction will add them and put the result in a third register. </p> <p></p> <p>In modern CPUs the registers have been widened to contain more than one operand and the corresponding vector  addition can compute and store the result in the same number of cycles. Typically, a vector register is now 512 bits wide, or 64 bytes, the same as the lenght of a cache line.</p> <p></p> <p>SIMD vectorisation can in principle speed up loops by a factor of 2, 4, 8, 16, depending on the number of bytes the  data elements use. Howecer, when the data being processed is not in the cache it does not help. </p> <p>Tip</p> <p>If your code does not vectorize, first find out if the data is in the cache, If not is does not help. </p>"},{"location":"chapter-2/#the-cost-of-floating-point-instructions","title":"The cost of floating point instructions","text":"<p>Note</p> <p>All animals are equal, but some animals are more equal than others. Animal farm, George Orwell.</p> <p>Not all mathematical operations are equally fast. Here's a table listing their relative cost: </p> cost operations cheap addition, subtraction, multipication rather expeesive division expensive square root very expensive trigonometric, exponential, logarithmic functions <p>As an example, let's write a functon for the Lennard-Jones potential:</p> <p></p> <p>Here's a first C++ translation of the mathematical expression of the Lennard-jones potential:</p> <pre><code>double VLJ0( double r ) {\n    return 1./pow(r,12) - 1./pow(r,6);  \n} \n</code></pre> <p>We measured the cost of <code>VLJ0</code> by timing its application to a long array and express it relative to the best  implementation we could come up with. The cost of <code>VLJ0</code> is 18.0, so it is a really expensive implemenation. In view  of the table above, that should come to no surprise: it has two divisions and two <code>pow</code> calls which raise a real  number to a real power. <code>pow</code> is implemented using an exponential and a logarithm. Let's try to improve that.</p> <p>We can get rid of the divisions using : </p> <pre><code>function \ndouble VLJ1( double r ) {\n    return std::pow(r,-12) - std::pow(r,-6);\n}\n</code></pre> <p>This scores a bit better: 14.9, but the two <code>pow</code> calls remain expensive. The expression for the Lennard-Jones  potential can be rewritten as . Using a temporary to store  we are left with only one  <code>pow</code> call: </p> <pre><code>double VLJ2( double r ) {\n    double tmp = std::pow(r,-6);\n    return tmp*(tmp-1.0);\n}\n</code></pre> <p>This has a performance score of 7.8, still far away from 1. Realizing that we don't need to use <code>pow</code> because the  expression has in fact integer powers,</p> <pre><code>double VLJ3( double r ) {\n    double tmp = 1.0/(r*r*r*r*r*r);\n    return tmp*(tmp-1.0);\n}\n</code></pre> <p>This has one division, 6 multiplications and subtraction. We can still reduce the number of multiplications a bit:</p> <pre><code>double VLJ( Real_t r ) {\n    double rr = 1./r;\n    rr *= rr;\n    double rr6 = rr*rr*rr;\n    return rr6*(rr6-1);\n}\n</code></pre> <p>Both these implementation have a performance score of 1. The optimum has been reached. The effect of two  multiplications less in the last implementation doesn't show, because in fact the compiler optimizes them away anyway. </p> <p>Note</p> <p>Compilers are smart, but it will not do the math for you. </p> <p>There is yet a common sense optimisation that can be applied. The standard formulation of the Lennard-Jones  potential is expressed as a function of . Since it has only even powers of we can as well express it as a  function of :</p> <p> </p> <p>At first sight, this may not immediately seem an optimisation, but in Molecular Dynamics the Lennard-Jones potential is  embedded in a loop over all interacting pairs for which distance between the interacting atoms is computed:</p> <pre><code>double interaction_energy = 0;\nfor(int i=0; i&lt;n_atosm; ++i)\n    std::vector&lt;int&gt;&amp; verlet_list_i = get_verlet_list(i); \n    for(int j : verlet_list_i) {\n        r_ij = std::sqrt( (x[j] - x[i])^2 + (y[j] - y[i])^2 + (z[j] - z[i])^2 )\n        if( r_ij &lt; r_cutoff)\n            interaction_energy += VLJ(r_ij);\n    }\n</code></pre> <p>Using  this loop can be implemented as: </p> <pre><code>double interaction_energy = 0;\ndouble r2_cutoff = r_cutoff^2;\nfor(int i=0; i&lt;n_atosm; ++i)\n    std::vector&lt;int&gt;&amp; verlet_list_i = get_verlet_list(i); \n    for(int j : verlet_list_i) {\n        s_ij = (x[j] - x[i])^2 + (y[j] - y[i])^2 + (z[j] - z[i])^2\n        if( s_ij &lt; r2_cutoff)\n            interaction_energy += V_2(s_ij);\n    }\n</code></pre> <p>This avoid the evaluation of a <code>sqrt</code> for every interacting pair of atoms. </p> <p>Homework</p> <p>Write a program in C++ or Fortran to time the above implementations of the Lennard-Jones potential. Since  timers are not accurate enough to measure a single call, apply it to an array and divide the time for  processing the array by the number of array elements.</p> <ul> <li>Think about the length of the array in relation to the size of the cache (L1/L2/L3).</li> <li>Think about vectorisation. </li> </ul>"},{"location":"chapter-2/#consequences-of-computer-architecture-for-performance","title":"Consequences of computer architecture for performance","text":""},{"location":"chapter-2/#recommendations-for-array-processing","title":"Recommendations for array processing","text":"<p>The hierarchical organisation of computer memory has also important consequences for the layout of data arrays and  for loops over arrays in terms of performance (see below). </p> <ol> <li>Loops should be long. Typically, at the begin and end of the loop thee pipeline is not full. When the loop     is long, these sections can be amortized with respect to the inner section, where the pipeline is full. </li> <li>Branches in loops should be predictable. The outcome of unpredictable branches will be guessed wrongly,     causing pipeline stalls. Sometimes it may be worthwile to sort the array according to the probability of the     outcome if this work can be amortized over many loops.</li> <li>Loops should access data contiguously and with unit stride. This assures that<ul> <li>at the next iteration of the loop the data element needed is already in the L1 Cache and can be accessed    without delay,</li> <li>vector registers can be filled efficiently because they need contiguous elements from the input array.</li> </ul> </li> <li>Loops should have high computatonal intensity. The computational intensity  is defined as , with  the number of compute cycles and  the total number of bytes read and     written. A high computational intensity means many compute cycles and little data traffic to/from memory and thus     implies that there will be no pipeline due to waiting for data to arrive. This is a compute bound loop. Low     computational intensity, on the other hand, will cause many pipeline stalls by waiting for data. This is a     memory bound loop. Here, it is the bandwidth (the speed at which data can be transported from main memory     to the registers) that is the culprit, rather than the latency. </li> </ol>"},{"location":"chapter-2/#recommendations-for-data-structures","title":"Recommendations for data structures","text":"<p>The unit stride for loops recommendation translates into a recommendation for data structures. Let's take Molecular  Dynamics as an example. Object Oriented Programming (OOP) would propose a Atom class with properties for mass ,  position , velocity , acceleration , and possibly others as well, but let's  ignore those for the time being. Next, the object oriented programmer would create an array of Atoms. This approach  is called an array of structures (AoS). The AoS approach  leads to a data layout in memory like | , , , , , , , , | , , , , , , , , | , , , , , , , , | , , , , , , ... Assume we  store the properties as single precision floating point numbers, hence a cache line spans 8 values. We marked the cache  line boundaries in the list above with vertical bars. Suppose for some reason we need to find all atoms  for  which  is between  and . A loop over all atoms  would test  and remember the   for which the test holds. Note that every cache line contains at most one single data item that we need in this  algorithm. some cache lines will even contain no data items that we need. For every data iten we need, a new cache  line must be loaded. This is terribly inefficient. There is a lot of data traffic, only 1/8 of which is useful and the  bandwidth will saturate quickly. Vectorisation would be completely useless. To fill the vector register we would  need 8 cache lines, most of which would correspond to cache misses and cost hundreds of cycles, before we can do 8  comparisons at once. The AoS, while intuitively very attractive, is clearly a disaster as it comes to performance.  The - much better - alternative data structure is the SoA, structure of Arrays. This creates an AtomContainer  class (to stay in the terminology of Object Oriented progrmming) containing an array of length  for each  property. In this case there would be arrays for , , , , , , , , , .  Now all  are stored contiguously in memory and every item in a cache would be used. Only one cache line would  be needed to fill a vector register. Prefetching would do a perfect job. The SoA data structure is much more  efficient, and once you get used to it, almost equally intuitive from an OOP viewpoint. Sometimes there is  discussion about storing the coordinates of a vector, e.g.  as per-coordinate arrays, as above, or as an  array of vectors. The latter makes is more practical to define vector functions like magnitude, distance, dot and  vector products, ... but they make it harder to SIMD vectorise those functions efficiently, because contiguous data  items need to be moved into different vector registers.  </p>"},{"location":"chapter-2/#selecting-algorithms-based-on-computational-complexity","title":"Selecting algorithms based on computational complexity","text":"<p>The computational complexity of an algorithm is an indication of how the number of instructions in an algorithms  scales with the problem size . E.g. the work of an  algorithm scales quadratically with its problem size.  As an example consider brute force neighbour detection (Verlet list construction) of  interacting atoms in Molecular  Dynamics:</p> <pre><code>    // C++ \n    for (int i=0; i&lt;N; ++i)\n        for (int j=i+1; j&lt;N; ++j) {\n            r2ij = squared_distance(i,j);\n            if (r2ij&lt;r2cutoff) \n               add_to_Verlet_list(i,j); \n        }\n</code></pre> <p>Note</p> <p>Note that we have avoided the computation of the square root by using the squared distance rather than the  distance.</p> <p>The body of the inner for loop is executed  times. Hence, it is . Cell-based Verlet  list construction restricts the inner loop to the surrounding cells of atom <code>i</code> and is therefor .</p> <p>The computational complexity of an algorithm used to be a good criterion for algorithm selection: less work means  faster, not? Due to the workings of the hierarchical memory of modern computers the answer is not so clear-cut.  Consider two search algorithms for finding an element in a sorted array, linear search and binary search bisecting. Linear search simply loops over all elements until the element is found (or a larger element is found), and is thus  . Binary search compares the target value to the  middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the  search continues on the remaining half, again taking the middle element to compare to the target value, and  repeating this until the target value is found. If the search ends with the remaining half being empty, the target  is not in the array. The complexity of this algorithm is . Clearly, binary search finds the answer  by visiting far fewer elements in the array as indicated by its lower complexity. However, contrary to linear search it  visits the elements in the array non-contiguously, and it is very well possible that there will be a cache miss on  every access. Linear search, on the other hand, will have no cache misses: it loads a cache line, visits all the  elements in it and in the mean time the prefetching machinery takes care of loading the next cache line. It is only  limited by the bandwidth. For small arrays linear search will be faster than binary search. For large arrays the  situation is reversed. A clever approach would be to combine both methods: start with binary search and switch to  linear search as soon as the part of the array to search is small enough. This needs some tuning to find the  at  which both algorithms perform equally well. The combined algorithm is thus not cache-oblivious. </p> <p>Tip</p> <p>There is no silver bullet. All approaches have advantages and disadvantages, some may appear in this situation  and others in another situation. The only valid reasoning is: numbers tell the tale (meten is weten):  measure the performance of your code. Measure it twice, then measure again.</p>"},{"location":"chapter-2/#supercomputer-architecture","title":"Supercomputer architecture","text":"<p>Note</p> <p>For a gentle but more detailed introduction about supercomputer architecture check out [this VSC course] (https://calcua. uantwerpen.be/courses/supercomputers-for-starters/Hardware-20221013-handouts.pdf). An updated  version will appear soon here  (look for 'Supercomputers for starters').</p> <p>We haven't talked about supercomputer architecture so far. In fact, supercomputers are not so very different from  ordinary computers. The basic building block of a supercomputer is a compute node, or a node tout court.  It can be seen as an ordinary computer but without peripheral devices (no screen, no keyboard, no mouse, ...).  A supercomputer consists of 100s to 1 000s of nodes (totalling up to 100 000s of cores), mutually connected to an  ultra-fast network, the interconnect. The interconnect allows the nodes to exchange information so that they can  work together on the same computational problem. It is the number of nodes and cores that makes a supercomputer a  supercomputer, not (!) the performance of the individual cores. Motherboards for supercomputer nodes typically have  2 sockets, each of which holds a CPU. Technically speaking they behave as a single CPU double the size, and double  the memory. Performance-wise, however, the latency across the two CPUs is typically a factor 2 larger. This goes by  the name ccNUMA, or cache coherent non-uniform memory architecture. Cache coherence means that if caches  of different copies hold copies of the same cache line, and one of them is modified, all copies are updated.  NUMA means that there are different domains in the global address space of the node with different latency and/or  bandwidth. CPU0 can access data in DRAM1, but this is significantly slower (typically 2x). </p> <p></p>"},{"location":"chapter-3/","title":"Chapter 3 - Optimize first, then parallelize","text":""},{"location":"chapter-3/#chapter-3-optimize-first-then-parallelize","title":"Chapter 3 - Optimize first, then parallelize","text":""},{"location":"chapter-3/#when-to-parallelize-and-what-to-do-first","title":"When to parallelize, and what to do first...","text":"<p>When your program takes too long, the memory of your machine is too small for your problem or the accuracy you need cannot be met, you're hitting the wall. Parallelization seems necessary, and you feel in need of a supercomputer. However, supercomputers are expensive machines and resources are limited. It should come to no surprise that it is expected that programs are allowed to run on supercomputers only if they make efficient use of their resources. Often, serial programs provide possibilities to improve the performance. These come in two categories: common sense optimizations (often completely overlooked by researchers) which rely on a good understanding of the mathematical formulation of the problem and the algorithm, and code optimizations which rely on understanding processor architecture and compilers. Let's first look at common sense optimizations. </p>"},{"location":"chapter-3/#common-sense-optimizations","title":"Common sense optimizations","text":"<p>Common sense optimizations come from a good understanding of the mathematical formulation of the problem and seeing opportunities to reduce the amount of work. We give three examples. </p>"},{"location":"chapter-3/#1-magnetization-of-bulk-ferro-magnets","title":"1. Magnetization of bulk ferro-magnets","text":"<p>I was asked to speed up a program for computing the magnetization  of bulk ferro-magnets as a function of temperature . This is given by a self-consistent solution of the equations:</p> <p> </p> <p> </p> <p>with . At  we have , and at high ,  approaches zero.</p> <p>The solution is a curve like this:</p> <p></p> <p>The program compute this as follows: For any temperature , set  as an inital guess. Then iterate  until  is small. Here,</p> <p> </p> <p>and the integral is computed using Gauss-Legendre integration on 64 points.</p> <p>The choice of  as initial guess is obviously a good one close to . However, looking at the graph above, it becomes clear that as T increases the solution moves further and further away from . Furthermore, if we compute temperature points at equidistant temperature points,  for some  and , it is also clear that the solution of the previous temperature point, i.e. , is a far better initial initial guess guess than . This turns out to be 1.4x faster. Not a tremendous improvement, but as the graph above seems continuous w e can take this idea a step further: using interpolation from solutions a lower temperature points to predict the next solution and use that as an initial guess. Linear interpolation of  from  and  gives a speedup of 1.94x and quadratic interpolation from ,  and  a factor of 2.4x.  That is a substantial speedup achieved without actually modifying the code. This optimization comes entirely from understanding what your algorithm actually does. Investigation of the code itself demonstrated that it made suffered from a lot of dynamic memory management and that it did not vectorize. After fixing these issues, the code ran an additional 13.6x faster. In total the code was sped up by an impressive 32.6x.</p>"},{"location":"chapter-3/#2-transforming-the-problem-domain","title":"2. Transforming the problem domain","text":"<p>At another occasion I had to investigate a code for calculating a complicated sum of integrals in real space. After fixing some bugs and some optimization to improve the efficiency, it was still rather slow because the formula converged slowly  As the code was running almost at peak performance, so there was little room for improvement. However, at some point we tried to apply the Fourier transform to get an expression in frequency space. This expression turned out to converge much faster and consequently fewer terms had to be computed, yielding a speedup of almost 2 orders of magnitude and was much more accurate. This is another example of common sense optimization originating in a good mathematical background. The natural formulation of a problem is not necessarily the best to use for computation.</p>"},{"location":"chapter-3/#3-transforming-data-to-reduce-their-memory-footprint","title":"3. Transforming data to reduce their memory footprint","text":"<p>I recently reviewed a Python code by the Vlaamse Milieumaatschappij for modelling the migration of invertebrate aquatic species in response to improving (or deteriorating) water quality. The program read a lot of data from .csv files. For a project it was necessary to run a parameter optimization. That is a procedure where model parameters are varied until the outcome is satisfactory. If the number of model parameters is large the number of program runs required can easily reach in the 100 000s. The program was parallelized on a single node. However, the program was using that many data that 18 cores of the 128 cores available on a node already consumed all the available memory. By replacing the data types of the columns of the datasets with datatypes with a smaller footprint, such as replacing categorical data with integer IDs, replacing 32-bit integers with 16-bit or even 8-bit integers, float64 real numbers with float32 or float16 numbers reduced the amount of data used by a factor 8. All of a sudden much more cores could be engaged in the computation and the simulation sped up considerably.</p> <p>Some of these \"common sense optimizations\" may seem obvious. Yet, of all the codes I reviewed during my career, few of them were immune to common sense optimization. Perhaps, developing (scientific) software takes a special mindset:</p> <p>!!! tip     The scientific software developer mindset: Constantly ask yourself 'How can I improve this? How can I make     it faster, leaner, more readable, more flexible, more reusable, ... ?'</p> <p>Common sense optimizations are optimizations that in general don't require complex code analysis, require very little code changes and thus little effort to implement them. Yet they can make a significant contribution.</p>"},{"location":"chapter-3/#code-optimizations","title":"Code optimizations","text":"<p>Code optimizations are optimizations aiming at making your solution method run as efficient as possible on the machine(s) that you have at your disposal. This is sometimes referred as code modernization, because code that was optimized for the CPUs of two years a go may well need some revision for the latest CPU technology. These optimizations must, necessarily, take in account the specific processor architecture of your machine(s). Important topics are:</p> <ul> <li>Avoid pipeline stalls (due to unpredictable branches, e.g.) - Ensure SIMD vectorization. On modern processors vector registers can contain 4 double precision floating point   numbers or 8 single precision numbers and vector instructions operate on these in the same number of cycles as   scalar instructions. Failing to vectorize can reduce the speed of your program by a factor up to 8x!   - Smart data access patterns are indispensable for programs with a memory footprint that exceeds the size of the cache.   Transferring data from main memory (DRAM) to the processor's registers is slow: typical latencies are in the order   of 100 cycles (which potentially wastes ~800 single precision vectorized operations). Vector instructions are of   no help if the processing unit must wait for the data.</li> </ul> <p>This is clearly much more technical and complicated (in the sense that it requires knowledge from outside the scientific domain of the problem you are trying to solve). Especially fixing memory access patterns can be difficult and a lot of work, as you may have to change the data structures used by your program, which usually also means rewriting a lot of code accessing the data. Such code optimizations can contribute significantly to the performance of a program, typically around 5-10x, but possibly more. As supercomputers are expensive research infrastructure in high demand, we cannot afford to waste resources. That being said, the lifetime of your program is also of importance. If you are developing a code that will be run a few times during your Master project or PhD, using only a hundred of node days, and to be forgotten afterwards, it is perhaps not worth to spend 3 months optimising it.</p> <p>Often, however, there is a way around these technicalities. If the scientific problem you are trying to solve, can be expressed in the formalism of common mathematical domains, e.g. linear algebra, Fourier analysis, ..., there is a good chance that there are good software libraries, designed with HPC in mind, that solved these problems for you. In most cases there are even bindings available for your favorite programming language (C/C++, Fortran, Python, ...).  All you have to do is translate the mathematical formulation of your problem into library calls. </p> <p>Tip</p> <p>Use HPC libraries as much as possible. There is little chance that you will outperform them. Quite to the     contrary: your own code will probably do significantly worse. By using HPC libraries you gain three times:     - you gain performance, - you gain development time as you will need a lot less code to solve your problem, less debugging, simpler       maintenance, ... - your learn how to use the library which will get you at speed readily when you take on your next       scientific problem.</p> <p>Tip</p> <p>Don't reinvent the wheel. The wheel was invented ~8000 years ago. Many very clever people have put effort in     it and is pretty perfect by now. Reinventing it will unlikely result in an improvement. By extension: if you     need some code, spend some time google-ing around to learn what is already available and how other researchers     attack the problem. It can save you weeks of programming and debugging. Adapting someone else's code to your     needs will learn you more than coding it from scratch. You'll discover other approaches to coding problems than     yours, other language constructs, idioms, dependencies to build on, learn to read someone else's code, learn to     integrate pieces.</p>"},{"location":"chapter-3/#when-is-code-optimized-well-enough","title":"When is code optimized well enough?","text":"<p>Tip</p> <p>Premature optimization is the root of all evil Donald Knuth.</p> <p>This quote by a famous computer scientist in 1974 is often used to argue that you should only optimize if there is a real need. If code is too slow, measurements (profiling) should tell in which part of the code most time is spent. That part needs optimization. Iterate this a few times. Blind optimization leads to useless and developer time-wasting micro-optimizations rendering the code hard to read and maintain. On the other hand, if we are writing code for a supercomputer, it better be super-efficient. But even then, depending on the lifetime of the program we are writing, there is a point at which the efforts spent optimising are outweighed by having to wait for the program going in production.</p> <p>How can one judge whether a code needs further optimization or not? Obviously, there are no tricks for exposing opportunities for common sense optimizations, nor for knowing whether better algorithms exist. That is domain knowledge, it comes with experience, and requires a lot of background. But for a given code and given input, can we know whether improvements are possible? In Chapter 1 we mentioned the existence of machine limits, the peak performance, , the maximum number of floating point operations that can be executed per second, and the bandwidth, , the maximum number of bytes that can be moved between main memory and the CPU's registers per second. It is instructive to study how these machine limits govern the maximum performance  as a function of the computational intensity . If the CPU must not wait for data, . In the situation where the bandwidth is limiting the computation . This leads to the formula: </p> <p> </p> <p>This is called the roof-line model, since its graph looks like a roof-line.</p> <p></p> <p>We can measure the actual performance and computational intensity of the program and plot it on the graph. The point must necessarily be under the roof-line. For a micro-benchmark, such as a loop with a simple body, we can compute  by hand, count the Flops and time the benchmark to obtain the computational intensity. For an entire program a performance analysis tool can construct the graph and measure where the program is in the graph. Let's discuss 4 different cases, corresponding to the four numbered spots in the graph above.</p> <ol> <li>Point 1 lies in the bandwidth limited region, but well below the roof-line. Something prevents the program to go at the maximum performance. There can be many causes: bad memory access causing cache misses, the code may fail to vectorize, pipeline stalls, ... for a micro-benchmark you can perhaps spot the cause without help. For a larger program a performance analyzer will highlight the problems. </li> <li>Point 2 lies in the peak performance limited region, also well below the roof-line. Hence, cache misses are unlikely the cause. </li> <li>Point 3 lies close to the roof-line and the boundary between the bandwidth limited region and the peak performance limited region. This the sweet spot. Both peak performance and bandwith are fully used.</li> <li>Point 4 lies close to the roof-line in the peak performance limited region. This is an energy-efficient computation at peak performance. It moves little data (high ). Moving data is by far the most energy consuming part in a computation.</li> </ol>"},{"location":"chapter-3/#common-approaches-towards-parallelization","title":"Common approaches towards parallelization","text":"<p>Before we discuss common parallelization approaches, we need to explain some concepts:</p> <ul> <li>process (wikipedia): \"In computing, a process is the  instance of a computer program that is being executed by one or more threads.\" A process has its own address   space, the region of main memory that can be addressed by the process. Normally, a process cannot go outside its address space, nor can any other process go inside the process's own address space. An exception is when both   processes agree to communicate, which is the basis of distributed memory parallelism (see below). In general, a   process is restricted to a single node, and the maximum number of parallel threads is equal to the number of cores   on that node. </li> <li>thread (wikipedia): \"In computer science, a thread of   execution is the smallest sequence of programmed instructions that can be managed ...\". The instructions in a   thread are thus by definition sequential. In the context of parallel computing, parallel threads are managed by the parent process to run different tasks in parallel. Obviously, parallel threads need to run on distinct cores to be truly concurrent. As threads belong to a process, they can in principle have access to the entire address space of the process. Sometimes a distinction is made between hardware threads and software threads. A software thread is a set of sequential instructions for a computational task that is scheduled by the program to execute. When its   execution starts, it is assigned to a core. software threads can be interrupted, in order to let the core do other, more urgent work, e.g., and restarted. There can be many more software threads in a program than it has cores available, but, obviously, they cannot all run in parallel. Software threads are very useful in personal computers with many interactive applications opened simultaneously, where are many non-urgent tasks. For HPC applications they are a bit heavy-weight. A hardware thread is a lightweight software thread that is exclusively tied to a core. When given work, it can start immediately and runs to completion without interruption. That is certainly useful in HPC where not loosing compute cycles is more important than flexibility.</li> </ul> <p>Now that we understand the concepts of processes and threads, we can explain three different types of parallelization:</p>"},{"location":"chapter-3/#shared-memory-parallelization","title":"Shared memory parallelization","text":"<p>In shared memory parallelization there is one process managing a number of threads to do work in parallel. As all the threads belong to the same process, they have access to the entire memory address space, that is, they share memory. To avoid problems, as well as for performance reasons, the variables inside a thread are private by default, i.e. they can only be accessed by the thread itself, and must be declared shared if other threads should have access too. Cooperating threads must exchange information by reading and writing to shared variables. The fact that all the threads belong to the same process, implies that shared memory programs are limited to a single node, because a process cannot span several nodes. However, there exist shared memory machines larger than a typical supercomputer node, e.g. SuperDome at KU leuven. Such systems allow to run a shared memory program with much more threads and much more memory. This approach can be useful when distributed memory parallelization is not feasible for some reason. </p> <p>The most common framework for shared memory parallelization is OpenMP. OpenMP parallelization of a sequential program is relatively simple, requiring little changes to the source code in the form of directives. A good starting point for OpenMP parallelization is this video. An important limitation of OpenMP is that it is only available in C/C++/Fortran, and not Python. Fortunately, Python has other options, e.g. multiprocessing, concurrent.futures and dask. In addition, it is possible to build your own Python modules from C++ or Fortran code, in which OpenMP is used to parallelize some tasks. The popular numpy is a good example.</p>"},{"location":"chapter-3/#pgas","title":"PGAS","text":"<p>PGAS, or Partitioned Global Address Space, is a parallel programming paradigm that provide communication operations involving a global memory address space on top of an otherwise distributed memory system. it therefore acts as a shared memory parallelization.  </p>"},{"location":"chapter-3/#distributed-memory-parallelization","title":"Distributed memory parallelization","text":"<p>Distributed memory parallelization is the opposite of shared memory parallelization. There are many process, each with only a single-thread. Every process has its own memory address space. These address spaces are not shared, they are distributed. Therefor, explicit communication is necessary to exchange information. For processes on the same machine (=node) this communication is intra-node, but for processes on distinct machines messages are sent over the interconnect.</p> <p>Distributed memory programs are considerably more complex to write, as the communication must be explicitly handled by the programmer, but may use as many processes as you want. Transformation of a sequential program into a distributed memory program is often a big programming effort. The most common framework is MPI. MPI is available in C/C++/Fortran and also in Python by the mpi4py module. </p>"},{"location":"chapter-3/#hybrid-memory-parallelization","title":"Hybrid memory parallelization","text":"<p>Hybrid memory parallelization combines both approaches. It has an unlimited number of processes, and a number of threads per process, which run in parallel in a shared memory approach (OpenMP). The process communicate with each other using MPI. Typically, the computation is organized as one proces per NUMA domain and one thread per core in dat NUMA domain. This approach uses shared memory parallelization where it is useful (on a NUMA domain), but removes the limitation to a single machine. It has fewer processes, and thus less overhead in terms of memory footprint, and communication overhead. It is also a bit more complex that pure distributed memory parallelization, and much more complex than shared memory parallelization.</p> <p>Hybrid memory parallelization is usually implemented with OpenMP at the shared memory level and MPI at the distributed level.</p>"},{"location":"chapter-4/","title":"chapter 4 - Case studies","text":""},{"location":"chapter-4/#chapter-4-case-studies","title":"chapter 4 - Case studies","text":""},{"location":"chapter-4/#monte-carlo-ground-state-energy-calculation-of-a-small-atom-cluster","title":"Monte Carlo ground state energy calculation of a small atom cluster","text":""},{"location":"chapter-4/#introduction","title":"Introduction","text":"<p>The code for this benchmark was Kindly provided by Jesus Eduardo Galvan Moya, former PhD student of the Physics Department, Condensed Matter Theory.</p> <p>It is a small molecular dynamics code which happens to serve many didactic purposes. It is simple code, not too big and full of issues you should learn to pay attention to ;-)</p> <p>The goal of the program is to calculate the ground state energy of a small atomistic system of 10-150 atoms. The system is at 0K, so there are no velocities, and the total energy of the system consist of the interaction energy only. Interactions are described by a pair-wise interaction potential, without cutoff radius (brute force). A Monte Carlo approach is used to find the configuration with the lowest energy, 1000 separate runs with different initial configuration are run. Each run comprises 200 000 random atom moves. Finally, the run with the lowest energy is kept and subjected to Quasi-Newton iteration in order to find a local energy minimum.</p>"},{"location":"chapter-4/#implementation","title":"Implementation","text":"<p>Here is how this algorithm goes (C++ pseudocode):</p> <pre><code>n_atoms = 50; // (for example)\nstd::vector&lt;double&gt; x, y, z, xmin, ymin, zmin;\ndouble Emin = std::numeric_limits&lt;double&gt;::max();\nfor(int ic=0; ic&lt;1000; ++ic)\n{// loop over initial configurations\n // generate initial configuration\n    initialize(x,y,z);\n    for(int ip=0; ip&lt;200000; ++ip)     {// loop over random perturbations\n     // perturb the current configuration            x += small_perturbation();\n        y += small_perturbation();\n        z += small_perturbation();\n        E = 0;\n     // double loop over all interactions\n        for(int i=0; i&lt;n_atoms; ++i)\n            for(int j=0; j&lt;i; ++j) {\n                double rij = std::sqrt((x[j]-x[j])^2 + (y[j]-y[j])^2 + (z[j]-z[j])^2);\n                E += V(rij);\n            }\n        }\n    }\n    if( E &lt; Emin )\n    {// remember the current (perturbed) configuration\n        xmin = x;\n        ymin = y;\n        zmin = z;\n        Emin = E\n    }\n} // Perform a Newton-Raphsom iteration on E(x,y,z) with x0 = xmin, y0 = ymin, z = zmin.   ...\n</code></pre> <p>The memory footprint of this problem is (<code>n_atoms</code> x 3) <code>doubles</code> x 8 bytes/<code>double</code>. For <code>n_atoms = 150</code>, that is 3600 bytes, which is far less than the size of L1 cache (32KB). Hence, the problem fits easily in the L1 cache. As soon as the entire problem is loaded in the cache, the code will run without needing to wait for data. Furthermore, the interaction potential</p> <p> </p> <p>is rather compute intensive, as it uses several expensive operations: two exponentials and two divisions, plus the square root for the distance which here cannot be avoided:</p> <p> </p> <p>Consequently, the code is certainly compute bound.</p>"},{"location":"chapter-4/#optimization","title":"Optimization","text":"<p>Most of the work is carried out in the inner double loop over the interactions. Let's see if we can optimise this.</p> <p>Initially, both expressions for the interatomic distance  and the interaction potential  were implemented as functions called in the double loop. The first timing for the double loop with 50 atoms is 144 s. By checking the vectorization report of the compiler, we learned that the two function calls prohibited vectorization. After inlining the functions,  the timing was reduced to 93 s.  The inner loop contains a lot of short loops. This is bad for pipelining and vectorization (many loops end with incompletely filled vector registers.) If we split the loop in a double loop for calculating the interatomic distances and storing them in a long array, and a long loop over that array to compute the interactions, the situation might improve.</p> <pre><code>        E = 0;\n        int n_interactions = n_atoms*(n_atoms-1)/2;\n        std::vector&lt;double&gt; rij(n_interactions);          // (in C++ std::vector is actually a contiguous array)\n     // double loop over all interactions\n        for(int i=0; i&lt;n_atoms; ++i)\n            for(int j=0; j&lt;i; ++j)                 rij = std::sqrt((x[j] - x[j])^2 + (y[j] - y[j])^2 + (z[j] - z[j])^2);\n        }\n        for(int ij=0; ij&lt;n_interactions; ++ij)\n            E += V(rij[ij]);\n</code></pre> <p>This reduces the time from 93 to 86 s. Not much, but since we must run this loop 1 000 x 200 000 times it nevertheless represents a substantial gain.</p> <p>Note</p> <p>We implemented this both in C++ and Fortran. The results were almost identical. Some say that C++ is an inefficient programming language and that the opposite holds for Fortran. This is not true. Both C++ and Fortran compilers are capable of building optimally performant programs for the CPU at hand. We'll come to this subject later.</p> <p>At this point, we seem to be done optimising the inner loops. Maybe there is something we can do to the surrounding loops? The perturbation loop adds a small perturbation to every coordinate of every atom in the list to see if the perturbation results in a lower energy. The perturbation involves  random numbers and generation random numbers is also rather expensive. We might wonder if it is really necessary to perturb all atoms. What if we perturbed only one atom? That reduces the number of random number generations by a factor . In addition, most of the interactions remain the same, only the  interactions with the perturbed atom change. Hence, our program now has a complexity . In the original formulation the number of interaction to be computed was . As the program is compute-bound, changing the computational complexity from  to  will have a big impact. This optimization falls under the common sense optimizations.</p> <p>It is important to realize that this optimization changes the nature of the algorithm. It remains to be seen whether 200 000 configurations is still sufficient to find the minimum. We might need more, or maybe less. This up to the researcher to investigate.</p> <p>Let's see how we can implement this modification and how that effects the performance. We start with depicting the relation between  as a (lower triangular) matrix and as the linear <code>rij</code> array in the split loop above.</p> <p></p> <p>The linear array stores the rows of the lower triangular matrix: , , , , , , , , , ... . The matrix elements show the value or the index into the linear array. Let's do something similar for the interaction energy:</p> <p></p> <p>We have added a column to compute the row sums and the total sum of the interaction energies . Let's now visualize the changes when an atom, say atom 4, is perturbed.</p> <p></p> <p>The items changing due to perturbing  are marked in orange. The row sum for row 4 has to be computed from scratch and in row 5 and 6 the elements corresponding to column 4 change as well. The next figure shows how the perturbed result can be computed from the previous result by first subtracting the previous result and then adding the new result.</p> <p></p> <p>Here is a comparison of the timings:</p> speedup 50 86 s 5.7 15.1 150 (x3) 747 s (x9) 17.3 s (x3) 43.3 500 (x10) 8616 s (x100) 57.0 s (x10) 115.2 <p>Clearly, the timings for the  algorithm increase quadratically, while those for the  algorithm increase only linearly and the speedups are substantial. The  algorithm for 500 atoms - a number that our researcher considered unattainable because it would take too long to compute - is still faster than the  algorithm.</p> <p>!!! Tip     Look for algorithms of low computational complexity. However, The best algorithme may also depend on the problem as we saw in Selecting algorithms based on computational complexity.</p> <p>Despite the considerable performance improvement, there are a few disadvantages to it too. The  algorithm has more code, is more difficult to understand and thus harder to maintain. Moreover, its loops are more complex, making it harder for the compiler to optimize. Auto-vectorization doesn't work. If it needs further optimization, it is certainly no low-hanging fruit.</p>"},{"location":"chapter-4/#parallelization","title":"Parallelization","text":"<p>If the time of solution for this sofar sequential program is still too large, we might opt for parallelization. The interaction loop is now doing relatively little work, and hard to parallelize. On the other hand the perturbation loop can be easily distributed over more threads as this loop is embarrassingly parallel. As long as every thread generates a different series of random numbers they can run their share of the perturbation iterations completely independent. This is very easy to achieve with OpenMP. In the end every thread would have its own minimum energy configuration, and the overall minimum energy configuration is simply found as the minimum of per thread minima. Since every core has its own L1 cache, the problem for each thread also fits in L1. </p>"},{"location":"chapter-4/#project-mcgse","title":"Project mcgse","text":"<p>The <code>wetppr/mcgse</code> folder repeats this case study for the Morse potential (I lost the original code :-( )</p> <p> </p> <p>We will assume that all parameters are unity.   </p> <p>Here is its graph:</p> <p></p> <p>Using our research software development strategy, we start in Python, implement both algorithms and test. A good test is the case of a cluster of 4 atoms. Energy minimum then consists of a tetrahedron with unit sides. Every pair is then at equilibrium distance and . The vertices of the tetrahedron are on a sphere of radius . Let us randomly distribute 4 points on a sphere of radius  and see how well close we get to . </p> <pre><code>    import numpy as np\n    import mcgse # our module for this project: wetppr/mcgse\n    sample = mcgse.sample_unit_sphere(4) * np.sqrt(3/8)\n    config = (sample[0], sample[1], sample[2]) # initial coordinates of the atoms (x,y,z)\n    dist = mcgse.LogNormal(mean=-5, sigma=.4) # distribution to draw the length of the displacements from\n    #   the distribution and its parameters were selected using quite some trial and error to obtain     #   useful results...\n    Emin_ON2, *config_min_ON2 = mcgse.execute_perturbation_loop(config=config, n_iterations=20000, dist=dist, algo='ON2', verbosity=1)  Emin_ON , *config_min_ON  = mcgse.execute_perturbation_loop(config=config, n_iterations=20000, dist=dist, algo='ON' , verbosity=1)\n</code></pre> <p>Here are the results for 5 runs:</p> <pre><code>ON2 iteration 0: Emin=1.8642580817361518\nON2 iteration 200000: Emin=0.343375960680797, last improvement: iteration = 2044\nON iteration 0: Emin=1.8642580817361518\nON iteration 200000: Emin=0.1318184548419835, last improvement: iteration = 30162\n\nON2 iteration 0: Emin=1.0114013021541974\nON2 iteration 200000: Emin=0.368488427516059, last improvement: iteration = 32701\nON iteration 0: Emin=1.0114013021541974\nON iteration 200000: Emin=0.058861153165589014, last improvement: iteration = 5168\n\nON2 iteration 0: Emin=3.69912617914294\nON2 iteration 200000: Emin=0.3819530373342961, last improvement: iteration = 4580\nON iteration 0: Emin=3.69912617914294\nON iteration 200000: Emin=0.3297933435887894, last improvement: iteration = 65216\n\nON2 iteration 0: Emin=3.299140128625619\nON2 iteration 200000: Emin=0.5323556068840862, last improvement: iteration = 12505\nON iteration 0: Emin=3.299140128625619\nON iteration 200000: Emin=0.5270227273967558, last improvement: iteration = 16929\n\nON2 iteration 0: Emin=1.2894488159651718\nON2 iteration 200000: Emin=0.40188231571036437, last improvement: iteration = 2621\nON iteration 0: Emin=1.2894488159651718\nON iteration 200000: Emin=0.07936811573814093, last improvement: iteration = 25806\n</code></pre> <p>We can draw some interesting observations from these runs:</p> <ul> <li>Neither of the algorithms seem to get close to the minimum,</li> <li>In terms of closeness to the minimum there no clear winner, although <code>ON</code> got rather close twice,</li> <li>The higher the initial energy, the worse the solution, which is acceptable, as the average displacement magnitude   is fixed.</li> <li>None of the algorithms seems to converge. In the first and the last run <code>ON2</code> found its best guess at 2044 and   2621 iterations. None of the approximately 198_000 later attempts could reduce the energy. This seems to be the   case for <code>ON</code> as well, although the numbers are a bit higher. Despite being far from the minimum, improvements   seem to involve progressively more work.  Especially the last conclusion is rather worrying. Our algorithms don't seem to sample the configuration space very efficiently.  Perhaps, rather than displacing the atoms randomly, it might be more efficient to move them in the direction of the steepest descent of the energy surface. Since we have an analytical expression, we can compute it. The interaction  exerts a force   </li> </ul> <p> </p> <p> </p> <p>Here,   </p> <p>and</p> <p> </p> <p> </p> <p>Thus,  Hence:</p> <p> </p> <p>Finally (setting all parameters to unity), </p> <p>Now that we have the forces on the atoms in the current configuration, we should be able to move the atoms in the direction of the force, rather than in a random direction, as before. In fact, we have a true minimization problem now.</p> <p>to be continued...</p>"},{"location":"chapter-4/#study-of-data-access-patterns-in-a-large-lennard-jones-systems","title":"Study of data access patterns in a large Lennard-Jones systems","text":""},{"location":"chapter-4/#introduction_1","title":"Introduction","text":"<p>In this case study we consider a large system of atoms whose interaction is described by a Lennard-Jones potential. By large we mean a system that does not fit in the cache. Consequently, the effect of caches will be noticeable in the results. We will consider two different settings. A Monte Carlo setting, as above, in which the interaction energy is computed as a sum of pairwise interactions. It is of little physical significance, but is useful to demonstrate the effect of the caches on the computations. The second setting is a true molecular dynamics setting in which the time evolution of a collection of atoms is computed by time integration of the interaction forces which are computed as the gradient of the interaction potential. This gives rise to time dependent accelerations, velocities and positions of the atoms. </p>"},{"location":"chapter-4/#monte-carlo-setting","title":"Monte Carlo setting","text":"<p>The interaction energy is given by:</p> <p> </p> <p>Since our system is large, say billions of atoms, computing this sum considering all pairs, is computationally unfeasible because it has  computational complexity. We will discuss approaches to reduce the computational complexity to . To study the effect of the cache we will compute the partial sum </p> <p>for , that is   </p> <p>Because our system is translationally invariant, we can put atom  at the origin, in which case . Thus, we end up with:</p> <p> </p> <p>We will use the best implementation for the Lennard-Jones potential that we discussed in The cost of floating point instructions, expressed as a function of , as to avoid the square root needed to compute . We consider three different cases:</p> <ol> <li>A contiguous loop over arrays <code>x[1:N]</code>, <code>y[1:N]</code>, <code>z[1:N]</code>. This is a structure of arrays (SoA) approach.</li> <li>A contiguous loop over a single array <code>xyz[1:3N</code>, in which the ,  and  coordinates of the -th atom come after each other followed by the  ,  and  coordinates of the -th atom. This is an array of structures approach (AoS).</li> <li>A contiguous loop over arrays <code>x[1:N]</code>, <code>y[1:N]</code>, <code>z[1:N]</code> in which the atoms are picked by random permutation of . So, all atoms are visited, but in a random order. </li> </ol> <p>For each case  is computed for  repeating the loop over   times. In this way the amount of interaction potential evaluations is exactly  irrespective of the length of the array, and the timings can be compared. The smallest arrays fit in L1, while the longest arrays () do not even fit in L3. Here are the timings:</p> <p></p> <p>It is clearly visible that the behaviour of the random case above is very different from the two contiguous cases. For the longest arrays, the performance is a whopping 15x worse on the random loop, yet every case performs exactly the same work. There burning question is of course: \"what is causing the performance breakdown of the randomized loop\"? The second question, certainly less burning, but nevertheless important, is: \"is the lowest curve (the AoS case) the best we can get?\". If you are really curious, you might wonder about the small difference between the AoS case and the SoA case at larger . To help your understanding of the problem, here is a different representation of the same graph, this time the number of bytes used by the arrays on the x-axis instead of the array size . With this x-axis it is easy to draw the boundaries of the L1, L2 and L3 caches.  </p> <p>Surprisingly enough, the changes in the curves coincide with the cache boundaries. As soon as the problem is too large for a cache, cache misses cause pipeline stalls, and the CPU has to wait for the data needed. The latency increases at every cache boundary and the slowdown becomes more pronounced each time. This also explains the slight advantage for the AoS case over the SoA case for problems not fitting in L3. As x, y, and z follow contiguously in memory in the AoS case, when it needs new data from memory, it has to wait for only a single cache line, while the SoA needs three. If you have difficulties to grasp, revisit the talk by Scott Meyers CPU Caches and Why You Care.</p> <p>The second question is a bit harder to answer. Let us analyze the performance of the (Fortran) loop:</p> <pre><code>! Contiguous access, SoA: p=[xxx\u2026yyy\u2026zzz\u2026]\ndo ik=1,k\n    do im=1,m                           !  FLOPS\n        r2 = (p(im)-x0)**2              !\n            +(p(m+im)-y0)**2            !\n            +(p(2*m+im)-z0)**2          ! 3-, 2+, 3*\n!       r = lj_pot2(r)                  !\n        r2i = 1.0d0/r2                  ! 1/\n        rr6i = r2i*r2i*r2i;             ! 2*\n        V0j = 4.0d0*rr6*(rr6-1.0d0);    ! 2*, 1-\n    enddo                               !------------\nenddo                                   ! 14 flops\n</code></pre> <p>The loop has 14 floating point operations. It is executed  times in 1.2s. That makes  flops/s. The peak performance of the machine is 1 core x 1 instruction per cycle x 4 SIMD registers per instruction x 2.8 GHz = 11.2 Gcycles/s = 11.2 Gflops/s. Consequently, we are running at 56% of the peak performance. So it looks as if we could still do better. </p> <p>Let us analyze the data traffic of the same loop:</p> <pre><code>! Contiguous access, SoA: p=[xxx\u2026yyy\u2026zzz\u2026]\ndo ik=1,k\n    do im=1,m                           ! FLOPS         ! DATA\n        r2 = (p(im)-x0)**2              !               !\n            +(p(m+im)-y0)**2            !               !\n            +(p(2*m+im)-z0)**2          ! 3-, 2+, 3*    ! 3DP\n!       r = lj_pot2(r)                  !               !\n        r2i = 1.0d0/r2                  ! 1/            !\n        rr6i = r2i*r2i*r2i;             ! 2*            !\n        V0j = 4.0d0*rr6*(rr6-1.0d0);    ! 2*, 1-        !\n    enddo                               !---------------!-----\nenddo                                   ! 14 flops      ! 24B\n</code></pre> <p>The loop reads 24 bytes x  iterations in 1.2 s. That makes 10.7 GB/s. The bandwidth of the machine is 109 GB/s for 10 cores, that is 10.9 GB for 1 core. Our loop runs at the maximum bandwidth. It is bandwidth saturated. This is a machine limit. It can simply not feed the CPU with data faster than this. It is instructive to draw a roof-line model for this.  </p> <p>The above loop, that is the contiguous cases, plot on the bandwidth part of the roof-line indicating that the machine limit (bandwidth) is reached, the random case sits close to the bottom far away from all machine limits. The conclusion is that the loop as it is runs at its maximum speed, being bandwidth limited. However, 44% of the time the CPU is not doing useful work, because it is waiting for data. That means that if we replaced the Lennard-Jones potential with another one that is about twice as compute intensive, and for that reason more accurate, we would still finish the computation in 1.2s and have a more accurate solution, because we are using the cycles that the CPU was waiting for data to do the extra computations. </p>"},{"location":"chapter-4/#molecular-dynamics-setting","title":"Molecular Dynamics setting","text":"<p>We consider the same system, a large collection of atoms interacting through a Lennard-Jones potential. In a Molecular Dynamics setting the time evolution of th system is computed by time integration of the classical equation of motion:</p> <p> </p> <p> </p> <p> </p> <p>The forces are computed as the gradient of the interaction energy:</p> <p> </p> <p>We assume a system size of  atoms. The number of terms in the sum above is then . That will keep us busy, won't it? However, when you start evaluating all these contributions, you very soon realize that most of them are really small, so small that they don't actually contribute to the result. They are short-ranged. Mathematically, a force is short-ranged if it decays faster than . This is because the area of a sphere with radius  is  and hence the number of particles at distance grows as . Consequently, in order for the force exerted by those particle to be negligible it has to decay faster than .</p> <p>The derivative of the Lennard-Jones potential is:</p> <p> </p> <p>Hence,</p> <p> </p> <p> </p> <p>Note that the force factor , that is the factor in front of , can also be expressed in terms of :</p> <p> </p> <p> </p> <p>So, we can avoid the square root in computing . Clearly, we can compute the interaction energy and the interaction force in one go with little extra effort:</p> <p> </p> <p> </p> <p>The fact that the interaction force is short-ranged, allows us to neglect the interaction forces beyond a cutoff distance , thus offering a possibility to avoid the cost of an  algorithm. </p>"},{"location":"chapter-4/#implementing-cutoff","title":"Implementing cutoff","text":"<p>As a first step we can avoid the computation of the interaction energy and the interaction force if , or :</p> <pre><code># (python pseudocode)\nfor i in range(N):\n    for j in range(i):\n        x_ij = x[j]-x[i]\n        y_ij = y[j]-y[i]\n        z_ij = z[j]-z[i]\n        s_ij = x_ij*x_ij + y_ij*y_ij + z_ij*z_ij\n        if s_ij &lt;= s_c:\n            t = 1/s_ij\n            t3 = t*t*t\n            E += t3*(t3-1)\n            f = -6*t*t3*(2*t3-1)\n            Fx[i] += f*x_ij\n            Fy[i] += f*y_ij\n            Fz[i] += f*z_ij\n            Fx[j] -= f*x_ij\n            Fy[j] -= f*y_ij\n            Fz[j] -= f*z_ij\n</code></pre> <p>Although this loop only computes the interactions wheen , it still visits every pair to compute . The corresponding amount of work is still . Some improvement is possible by using Verlet lists. The Verlet list of an atom  is the set of atoms  for which , where  is typically a bit larger than . The loop is now witten as:</p> <pre><code># (python pseudocode)\nfor i in range(N):\n    for j in verlet_list(i):\n        # as above\n</code></pre> <p>The loop over  is now much shorter, its length is bounded, typically in the range . Hence, the double loop is effectively . The construction of the Verlet list, however, is still , but the cost of it is amortised over a number of timesteps. Because atoms move only a little bit over a time step and , the Verlet list can indeed be reused a number of timesteps, before it needs to be updated.  Algorithms for constructing the Verlet list with  complexity do exist. Here's a 2-D version of cell-based Verlet list construction. It can be easily extended to 3-D, but that is harder to visualise. In the left figure below, atom  (the orange dot) is surrounded by a blue circle of radius . Atoms inside the blue circle are in the Verlet list of atom . We now overlay the domain with a square grid, of grid size  (middle figure). Atom pairs in  the same cell or in nearest neighbour cells are Verlet list candidates, but not pairs in second-nearest neighbours or further. To construct the Verlet list of atom , we only have to test atoms in the same cell, or in its 8 nearest neighbours, all coloured light-blue. By iterating over all cells and over the atoms it contains, the Verlet lists of all atoms can be constructed with  complexity. In fact, by looking for pairs in all nearest neighbours, all candidate pairs are visited twice ( and ). Hence, only half of the nearest neighbours needs to be visited (right figure).  </p> <p>The algorithm requires that the grid implements:  - a cell list: a list of all the atoms that are in the cell, in order to iterate over all atoms in a cell. The cell   lists can be constructed with  complexity, and - a method to find the neighbour cells of a cell.  This is a good example for demonstrating the effectiveness of our strategy for research software development. Here are the steps you should take</p> <ol> <li>Start out in Python.</li> <li>Take a small system, e.g. , use Numpy arrays for the positions, velocities, ...</li> <li>Implement brute force computation of interactions and interaction forces ().</li> <li>Implement brute force computation of interactions and interaction forces with cutoff (). 4. Implement brute force construction of Verlet lists (). (You might need a larger system for testing this). 5. Implement Verlet list computation of interactions and interaction forces ().</li> <li>Implement cell-based Verlet list construction (). (You might need a larger system for testing this).</li> <li>Optimise, try using Numba, or by taking the compute intensive parts to C++. </li> <li>Of course test and validate every step, e.g. by comparing to previous steps. </li> </ol> <p>Tip</p> <p>Remember that, for performance, you should avoid using loops in Python. When I implemented the     cell-based Verlet list construction in Python, it turned out to be terribly slow, mainly because of 5 levels of     nesting Python loops. The C++ version turned out to be 1200x faster (twelve hundred indeed, no typo!). </p>"},{"location":"chapter-4/#moving-atoms","title":"Moving atoms","text":"<p>The initialization of a physically consistent system of atoms is a non-trivial task in itself. Because molecular motion conserves energy, random positions and velocities at time  may pos a lot of trouble for time integration. When two atoms happen to be very close they experience very high repulsive force and thus are accelerated vigorously. This can easily make the simulation explode. A practical way is to put atoms on a lattice with interatomic distances close to the equilibrium distance of the Lennard-Jones potential, e.g. primitive cubic, body-centred cubic (BCC), face-centred cubic (FCC), hexagonal closest packing (HCP). then slowly increase random velocities to increase the kinetic energy and hence the temperature.</p> <p>When initializing the system on a lattice, often the performance is rather good because the regular arrangement allows for a good data access pattern. However, as (simulation) time proceeds the atoms move and diffusion kicks in. Every timestep, some atoms will move in and out of some other atom's Verlet sphere. Gradually, the atoms will move further and further from their original positions, but their location in memory does not change, and, consequentially, the data access pattern approaches the random array access we discussed above, leading to considerable performance degradation.</p>"},{"location":"chapter-5/","title":"Chapter 5 - Developing Research Software","text":""},{"location":"chapter-5/#chapter-5-developing-research-software","title":"Chapter 5 - Developing Research Software","text":""},{"location":"chapter-5/#a-strategy-for-the-development-research-software","title":"A strategy for the development research software","text":"<p>You are facing a new research question, to be solved computationally and on your shelf of computational tools nothing useful is found. You start with an empty sheet of paper on you desk. or rather with an empty screen on your laptop. How you take on such a challenge? This chapter is about a strategy for (research) code development that</p> <ol> <li>minimizes coding efforts,</li> <li>allows for high performance,</li> <li>provides flexible and reusable software components.</li> </ol> <p>Coding efforts is more than just the time to type your program in an editor or IDE. It is also the time you spend making sure that your code is correct, and stays correct while you are working on it, restructuring its components, improving performance, parallelizing it, and ensuring that it solves the problem you need to solve.</p> <p>High performance is essential when we run our problem on a supercomputer, but perhaps that is not necessary. We want to postpone performance optimization until it is really needed. This principle was set in stone in a quote by Donald Knuth in 1974 already: \"Premature optimization is the root of all evil\". Spending time on optimization before it is needed is wasting time, and stopping progress. Initially, we want to focus on simplicity and correctness, and on understanding  the characteristics of the problem at hand. If the algorithm we choose to solve is inadequate, we want to know that as soon as possible. On the other hand, when it is needed, we want our project to be in a state that facilitates performance optimization where it is needed. </p> <p>Finally, we want an approach that builds experience. Flexible and reusable software components are materializing the experience that we build up. They enable us to proceed faster when pieces of a last year's problem may be useful today. Two important aspects of reusable code are writing simple functions and classes with a single functionality, and documentation. Well documenting your code increases the chance that when you check out your code three months later, you will not be staring baffled at your screen wondering what it was all about. It happens to me, it can certainly happen to you.</p> <p>None of these features come without effort. It is also not a bucket list of checkboxes to make sure that you did not overlook something. They comprise a learning process, require attention, discipline and research.</p> <p>Here is a story that demonstrates an anti-pattern for research code development.</p> <p>Note</p> <p>The term pattern refers to a set problems with common features that can be efficiently solved with the same approach, by applying the pattern. The term comes from a very useful book \"Design Patterns - Elements of reusable object-oriented software\", by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, 1995. Several \"pattern\" books later transferred the approach to other domains. An anti-pattern is a pattern that perhaps solves the problem, or not, but if it does, in pessimal way. It is a pattern NOT to follow.</p> <p>A PhD student once asked me for support. He had written a 10 000 line Fortran program. When he ran it, the results were not what he expected, and he suspected that there was a bug 'somewhere'. He asked if I could help him find the bug and - no kidding - by the end of next week, because the program had to go into production by then. I had to disappoint him and told him that he needed a 'true' magician, which I, unfortunately, was not. Obviously, the program was flawed in some sense, but other than a bug it might just as well be one or more of the situations below:</p> <ul> <li>The program may contain many bugs, which is very well possible in view of its size. On average a programmer introduces about 1 bug in every 10 lines of code! Check this link for some amazing facts</li> <li>The algorithm for solving the problem is inappropriate.</li> <li>There is an accuracy problem, related to space/time discretization, or a insufficient basis function for expanding the solution, or related to the finite precision of floating point numbers. (Floating point numbers are a processor's approximation of the real numbers, but they do have different mathematical properties. E.g. floating point addition is not commutative.)</li> <li>The mathematical formulation itself could be flawed or misunderstood.</li> <li>The program is correct, but the researchers expectations are wrong. - ...</li> </ul> <p>It could easily take weeks, if not months to write tests for the individual components to learn about the behaviour of the program and narrow down to the source of the error. This anti-pattern is a disaster waiting to happy.</p> <p>For this reason a sound strategy that makes sure that the code you write is correct, that does what you want, and builds understanding of the problem at hand as you proceed, is indispensable. The strategy proposed below has been shaped by a lifetime of research software developement. I use it in nearly every software project I take on, whether it is small or big, simple or complex, starting from scratch or from someone else code.  The proposed strategy builds on five principles:</p> <ol> <li>Start out in a high level programming language. Python is an excellent choice.</li> <li>Start out simple, as simple as possible, in order to build up understanding of your problem and how it should    be solved fast. 3. Test and validate your code, continuously, and for small code fragments, to discover bug and mistakes as soon as    possible. 4. Improve your code, adding better algorithms (which usually are more complex), pay attention to data structure    facilitating good data access patterns, think of common sense optimizations,    gradually increase the complexity of the problem. Keep principle 3. in mind and continue testing and validating the improvements.</li> <li>If neccessary, optimise your code. Necessary is when the time to solution is too long.</li> <li>If necessary, parallelise your code. Necessary is when the time to solution is still too long after paying    attention to principle 5., or if the problem does not fit in the memory of a single machine. In that case it it    advisable to optimise anyway to not waste cycles on an expensive supercomputer. </li> </ol> <p>Tip for researchers of the University of Antwerp and institutes affiliated with the VSC</p> <p>Researcher of the University of Antwerp and institutes affiliated with the VSC are wellcome to contact me for support when developing research software. The best time to do that would be before having written any line of code at all for the problem, in order to follow all principles.    </p> <p>The ordering of the list is important and reflects the successive steps in working on a project.  Below, we explain these principles in depth.</p> <p>Much of the wisdom of this strategy is integrated into a Python application micc2.</p>"},{"location":"chapter-5/#principle-1","title":"Principle 1","text":""},{"location":"chapter-5/#start-out-in-a-high-level-language","title":"Start out in a high level language","text":"<p>Python is an excellent choice:</p> <ul> <li>Python is a   high-level general-purpose programming language   that can be applied to many different classes of problems. - Python is easy to learn. It is an interpreted and interactive language and programming in Python is intuitive,   producing very readable code, and typically significantly more productive than in low-level languages as   C/C++/Fortran. Scripting provides a very flexible approach to formulating a research problem, as compared to an   input file of a low-level language program. - It comes with a large standard Library</li> <li>There is wide variety of third-party extensions, the Python Package Index(PyPI). Many packages   are built with HPC in mind, on top of high quality HPC libraries. - The functionality of standard library and extension packages is enabled easily as <code>import module_name</code>, and   installing packages is as easy as: <code>pip install numpy</code>. The use of modules is so practical and natural to Python   that researchers do not so often feel the need to reinvent wheels.</li> <li>Availability of High quality Python distributions (   Intel, Anaconda), cross-platform Windows/Linux/MACOS</li> <li>Python is open source. In itself that is not necessarily an advantage, but its large community guarantees an   very good documentation, and for many problems high-quality solutions are found readily on user forums.  - Python is used in probably any scientific domain, and may have many third party extension freely available in that   domain. It is available with a lot scientific Python packages on all VSC-clusters.</li> <li>Several high-quality Integrated Development Environments (IDEs) are freely available for Python: e.g. PyCharm, VS Code, which at the same time   provide support for C/C++/Fortran.</li> <li>Although Python in itself is a rather slow language, as we will see, there are many ways to cope with performance   bottlenecks in Python.   The quality of Python is embodied it its design principles \"The Zen of Python\" which are printed when you <code>import this</code>. Many of these also apply to our strategy for developing research software.</li> </ul> <pre><code>&gt; python\n&gt;&gt;&gt; import this\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n</code></pre> <p>In many ways, Python gently pushes you in the right direction, providing a pleasant programming experience.</p> <p></p> <p>As an alternative on could consider Julia. Following a different programming paradigm, called multiple dispatch, its learning curve is probably a bit steeper and it has a smaller community. However, it is more performant than Python. On the other hand its programming productivity might not be that of Python.  An other alternative could be Matlab. Although it has a long history of scientific computing, from as a language is it not as well designed as Python, and it is not fit for HPC. Moreover, it is a commercial product, and the community using it is not as willing to share its solutions as for Python, and there are much less extensions available. Often, you will need to solve the problem yourself, and, as a consequence, proceed at a slower pace.</p> <p>Starting out in a low-level programming language (C/C++/Fortran) is certainly a bad idea. Even if you are an experienced programmer you will proceed slower. The productivity of Python is easily 10 times larger than for C/C++/Fortran because:</p> <ul> <li>the advantages coming with an interpreted language, vs. a compiled language,</li> <li>the fact that Python is a very well designed, expressive and readable language with a rather flat learning curve,</li> <li>the availability of a large standard library and a wide range of domain-specific third party extensions, as well   as the ease with which these are enabled in your code.</li> </ul> <p>Hence, this course sticks to Python. </p>"},{"location":"chapter-5/#principle-2","title":"Principle 2","text":""},{"location":"chapter-5/#start-out-as-simple-as-possible","title":"Start out as simple as possible","text":"<p>Take a small toy problem, the smallest you can think of that still represents the problem that you want to solve. There is a famous quote, attributed to Einstein (although it seems he formulated it differently) \"Make everything as simple as possible, but not simpler\". That applies very well here. The simpler the problem you start with, the faster you will build experience and understanding of the problem. Ideally, take a problem with a known analytical solution. Look for something you can easily visualise. Four atoms are easier to visualise than a hundred. Visualization is a perfect way for obtaining insight (pun intended!) in your problem. Choose the simplest algorithm that will do the trick, don't bother about performance. Once it works correctly, use it as a reference case for validating improvements. Here are some interesting Python modules for visualization:</p> <ul> <li>matplotlib</li> <li>bokeh</li> <li>plotly</li> <li>seaborn</li> </ul>"},{"location":"chapter-5/#principle-3","title":"Principle 3","text":""},{"location":"chapter-5/#test-and-validate-your-code-changes","title":"Test and validate your code (changes)","text":"<p>In view of these amazing facts on the abundance of bugs in code there seems to be little chance that a programmer writes 100 lines of code without bugs. On average 7 bugs are to be expected in every 100 lines of code. Probably not all these bugs affect the outcome of the program, but in research code the outcome  is of course crucial. How can we ensure that our code is correct, and remains so as we continue to work on it? The answer is unit-tests. Unit-testing are pieces of test-code together with verified outcomes. In view of the abundancy of bus it is best to test small pieces of code, in the order of 10 lines. Test code is also code and thus can contain bugs as well. The amount of test code for a system can be large. An example is probaly the best way to demonstrate the concept. In chapter 4 we discussed the case study Monte Carlo ground state energy calculation of a small atom cluster and a small project wetppr/mcgse where the original study is repeated with a Morse potential, described by the formula:</p> <p> </p> <p>The code for this function is found in wetppr/mcgse/init.py. Note that we provided default unit values for all the parameters: </p> <pre><code>import numpy as np\n\ndef morse_potential(r: float, D_e: float = 1, alpha: float = 1, r_e: float = 1) -&gt; float:\n    \"\"\"Compute the Morse potential for interatomic distance r.\n\n    This is better than it looks, we can pass a numpy array for r, and it will\n    use numpy array arithmetic to evaluate the expression for the array.\n\n    Args:\n        r: interatomic distance\n        D_e: depth of the potential well, default = 1\n        alpha: width of the potential well, default = 1\n        r_e: location of the potential well, default = 1\n    \"\"\"\n    return D_e * (1 - np.exp(-alpha*(r - r_e)))**2\n</code></pre> <p>The implementation of the function comprises only one line. How can we test its correctness? One approach would be to list a few r-values for which we know the outcome. E.g. . Here is a test funtion for it. Note that </p> <pre><code>from math import isclose\n\ndef test_morse_potential_at_r_e():\n    # the value at r_e=1 is 0\n    r = 1\n    Vr = mcgse.morse_potential(x)\n    Vr_expected = 0\n    assert isclose(Vr, Vr_expected, rel_tol=1e-15)\n</code></pre> <p>Because the function is using floating point arithmetic, the outcome could be subject to roundoff error. We account for a relative error of <code>1e-15</code>. When running the test, an AssertionError will be raised whenever the relative error is larger than <code>1e-15</code>. When it comes to testing functions it is practical to focus on mathematical properties of the function (fixing parameters to unity). E.g.</p> <ul> <li>  for ,</li> <li>  for ,</li> <li>  is monotonously decreasing on , -  is monotonously increasing on , -  is positive on ,</li> <li>  is positive on ,  being the inflection point,</li> <li>  is negative on ,</li> <li>...</li> </ul> <p>See <code>tests/wetppr/mcgse/test_mcgse.py</code> for details. The file contains many more tests for other functions in the file <code>wetppr/mcgse/__init__.py</code>. </p>"},{"location":"chapter-5/#automating-tests","title":"Automating tests","text":"<p>As soon as you have a few tests, running them manually after every code change, becomes impractical. We need to automate running the tests. Several automated test runners are available. Pytest is a good choice. When given a directory as a parameter, it will import all <code>test_*.py</code> files under it, look for methods starting with <code>test</code>, execute them and produce a concise report about which tests pass and which tests fail. Details about test discovery are found here.</p> <pre><code>&gt; cd path/to/wetppr\n&gt; pytest tests\n==================================== test session starts =====================================\nplatform darwin -- Python 3.9.5, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\nrootdir: /Users/etijskens/software/dev/workspace/wetppr\nplugins: typeguard-2.13.3, mpi-0.5, anyio-3.6.2\ncollected 6 items\n\ntests/wetppr/mcgse/test_mcgse.py .....                                                 [100%]\n\n===================================== 5 passed in 1.65s ======================================\n</code></pre> <p>As is clear from the output above it found 5 tests in <code>tests/wetppr/mcgse/test_mcgse.py</code>, all executed successfully. In the presence of errors it is instructive to run <code>pytest</code> with <code>-v -s</code> options.</p> <p>For every new method added to your code, add some tests and run <code>pytest</code>. For every code change run all the tests again. Make sure they pass before you continue to improve or extend the code.</p>"},{"location":"chapter-5/#debugging-a-failing-test","title":"Debugging a failing test","text":"<p>Debugging is the process of stepping through a program, executing it line by line, and examining the results in order to find the source of the error. IDEs like PyCharm and VS Code provide a most friendly debugging experience, but you can also use the Python debugging module pdb:</p> <p>Debugging a failing test is often the best way to investigate the source of the error.  Python has an interesting idiom that allows a module to be run as a script, i.e. execute a module. To that end you put this code snippet at the end of a module:</p> <pre><code>#\n# module code here\n#\nif __name__ == \"__main__\":\n    #\n    # script code here.\n    #\n    print(\"-*# finished #*-\") # if this line doesn't show up, something went wrong.\n</code></pre> <p>The idiom has no common name :-(, we will refer to it as the <code>if __name__ == \"__main__\":</code> idiom. If a module file is imported as in <code>import module_name</code>, only the module code is executed, and the body of the <code>if</code> statement <code>if __name__ == \"__main__\":</code> is ignored, because the <code>import</code> statement sets the value of the <code>__name__</code> variable to the module name. Thus, the condition evaluates to <code>False</code>. Most of the module code will consist of <code>def</code> and <code>class</code> statements, defining methods (the Python term for a function) and classes. When Python executes a <code>def</code> or a <code>class</code> statement, it interprets the code of the method or class and registers them under their respective names, so that they can be called by the module or script that imported the module.</p> <p>If the module file is executed on the command line, as in the command <code>python module_name.py</code> or in an IDE, the <code>python</code> executable sets the <code>__name__</code> variable to <code>\"__main__\"</code>. Thus, the condition evaluates to <code>True</code> and its body is executed.  This Python idiom provides us with a practical approach for executing a failing test. Assume that by running <code>pytest tests</code> we find out that there is an error in the test function <code>test_morse_potential_mathematical_properties</code>:</p> <pre><code>&gt; pytest tests\n==================================== test session starts =====================================\nplatform darwin -- Python 3.9.5, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\nrootdir: /Users/etijskens/software/dev/workspace/wetppr\nplugins: typeguard-2.13.3, mpi-0.5, anyio-3.6.2\ncollected 6 items\n\ntests/wetppr/test_wetppr.py .                                                          [ 16%]\ntests/wetppr/mcgse/test_mcgse.py .F...                                                 [100%]\n\n========================================== FAILURES ==========================================\n________________________ test_morse_potential_mathematical_properties ________________________\n...\n================================== short test summary info ===================================\nFAILED tests/wetppr/mcgse/test_mcgse.py::test_morse_potential_mathematical_properties - ass...\n================================ 1 failed, 5 passed in 4.27s =================================\n\n</code></pre> <p>At the end of the test module <code>tests/wetppr/mcgse/test_mcgse.py</code>, which contains the failing test <code>test_morse_potential_mathematical_properties</code> you will see this code snippet:</p> <pre><code>#\n# test functions here\n#\nif __name__ == \"__main__\":\n    the_test_you_want_to_debug = test_energy\n\n    print(\"__main__ running\", the_test_you_want_to_debug)\n    the_test_you_want_to_debug()\n    print(\"-*# finished #*-\")\n</code></pre> <p>When we run <code>pytest tests</code>, pytest imports the test module, setting <code>__name__</code> to <code>\"test_mcgse\"</code>. Hence the condition <code>__name__ == \"__main__\"</code> evaluates to <code>False</code> and its body is not executed. Only when the module is executed the body will be executed too. Since we want to run the <code>test_morse_potential_mathematical_properties</code> function, change the first line in the body to:</p> <pre><code>    the_test_you_want_to_debug = test_morse_potential_mathematical_properties\n</code></pre> <p>The variable <code>the_test_you_want_to_debug</code> now is an alias for the test function that we want to debug, <code>test_morse_potential_mathematical_properties</code>. The next statement is a print statement producing something like:</p> <pre><code>__main__ running &lt;function test_morse_potential_mathematical_properties at 0x11098c310&gt;\n</code></pre> <p>which ensures us that we called the right test function. The test function is then called in the next statement through its alias <code>the_test_you_want_to_debug</code>:</p> <pre><code>    the_test_you_want_to_debug()\n</code></pre> <p>To debug the test using pdb execute:</p> <pre><code>&gt; python -m pdb tests/wetppr/mcgse/test_mcgse.py\n</code></pre>"},{"location":"chapter-5/#quick-and-dirty-testing","title":"Quick and dirty testing","text":"<p>The <code>if __name__ == \"__main__\":</code> idiom has another interesting application. When working on small projects, with only a few functions, sometimes we don't want to set up a <code>tests</code> directory with a <code>test_small_module.py</code> file. Instead we can use the <code>if __name__ == \"__main__\":</code> idiom to write a quick and dirty test inside the module, or just call the function to check by running or debugging the module that it does the right thing. In this way your work is restricted to a single file, and there is no need to switch between the test file and the module file.</p>"},{"location":"chapter-5/#principle-4","title":"Principle 4","text":""},{"location":"chapter-5/#improve-and-extend-the-code","title":"Improve and extend the code","text":"<p>Once the code for the simple problem you started with (see Principle 2) is tested, validated, and understood, you can gradually add complexity, approaching the true problem you need to solve. This may require improvements to the code because the time to solution will probably increase. Possible improvements are:  - better algorithms with lower computational complexity, - better data structures, facilitating good data access patterns, - common sense optimizations, - increasing the flexibility of the code, - restructuring the code, - gradually increase the complexity of the problem.</p> <p>Obviously, continue to test and validate all code changes and extensions.</p>"},{"location":"chapter-5/#principle-5","title":"Principle 5","text":""},{"location":"chapter-5/#optimise-if-necessary","title":"Optimise if necessary","text":"<p>If neccessary, optimise your code. Necessary is when the time to solution is too long. At this point you will probably already have a considerable Python code base. Bearing in mind that premature optimization is the root of all evil, the important question is what needs optimization and what not. The answer is provided by using a profiler. Profiler are tools that tell you how much time your program spends in the different parts of it and how many times tha part is executed.</p> <p>Some profilers provide information on a per function basis, and tell how many times that function was called and the average or cumulative time spent in it. Other profilers provide information on a per line basis, an tell how many times that line was executed and the average or cumulative time spent on it. This article provides the necessary details.</p> <p>You should start profiling on a per function basis and then profile the function(s) consuming most time with a line-by-line profiler. If it turns out that only a few lines of the function are responsible for the runtime consumption, split them off in a new function and try optimising the new function using one of the techniques discussed below. If not, try optimising the whole function with the very same techniques.</p> <p>Tip</p> <p>After optimising a (split-off) function, run the profiler again to check the performance.  </p>"},{"location":"chapter-5/#techniques-for-optimising-performance","title":"Techniques for optimising performance","text":"<p>Below a series of optimization techniques is presented, ordered by development effort needed.</p>"},{"location":"chapter-5/#use-python-modules-built-for-hpc","title":"Use Python modules built for HPC","text":"<p>There exist excellent Python modules that provide very performant high level data structures and operations. Using them can make a dramatic difference. E.g. the difference in performance between Python lists and Numpy arrays can easily be a factor 100. By using them you do not only gain performance. Once you have learned how to use a Python module, such as Numpy, development time is also reduced as you no longer have to code the low-level logic yourself. Here are a few interesting scientific Python modules:</p> <ul> <li>Numpy: n-dimensional arrays, mathematical functions, linear algebra, ...</li> <li>SciPy: fundamental algorithms for scientific computing...</li> <li>sympy: symbolic computation</li> <li>pandas: data analysis</li> </ul>"},{"location":"chapter-5/#numba","title":"Numba","text":"<p>Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python. It takes your Python code, transforms it into C code, compiles it (on the fly) and calls the compiled C code version. Here is how a Numba-optimized function, taking a Numpy array as argument, might look like: </p> <pre><code>import numba \n@numba.jit def sum2d(arr):\n    M, N = arr.shape\n    result = 0.0\n    for i in range(M):\n        for j in range(N):\n            result += arr[i,j]\n    return result\n</code></pre> <p>The <code>@numba.jit</code> decorator instructs the Python interpreter to use the numba just-in-time compiler (jit) to translate the Python code int C. compile it, and use the compiled verson on every call. As pure Python loops are quite expensive and the function has a double Python loop over 'i' and 'j' the performance gain is considerable. As in principle it only involves adding a decorator (and possibly some hinting at data types) using numba can be a really quick win.</p> <p>Note</p> <p>The above function can also defined as <code>np.sum(np.sum(arr))</code>, using numpy function calls exclusively. That might be even faster.</p> <p>Tip</p> <p>Numba is sensitive to data types. Changing the data type of the arguments can improve the situation a lot.  </p>"},{"location":"chapter-5/#developing-your-own-modules-in-cfortran","title":"Developing your own modules in C++/Fortran","text":"<p>If none of the above techniques helps, you might consider to develop your own Python modules compiled from C++ or Fortran code, also known as binary Python extensions. This can be a real game changer. In fact, this is exactly what the HPC Python modules like Numpy and SciPy do, and they do it because that is the way to harness the power of modern CPUs and expose it to Python. Obviously, this requires good knowledge of C++ or Fortran, and good understanding of performance critical characteristics fo modern CPU architecture.  The wip application facilitates building your C++ and Fortran modules. </p>"},{"location":"chapter-5/#principle-6","title":"Principle 6","text":""},{"location":"chapter-5/#parallelise-if-necessary","title":"Parallelise if necessary","text":"<p>If necessary, parallelise your code. Parallelization is necessary is when the time to solution is still too long after paying attention to principle 4 and principle 5, or if the research problem does not fit in the memory of a single machine. In that case it is advisable to optimise (principle 5) anyway to not waste cycles on an expensive supercomputer.</p> <p>To parallelize Python projects these tools come in handy: </p> <ul> <li>dask: multi-core and multi-node parallelization</li> <li>mpi4py: MPI parallelization with python</li> </ul>"},{"location":"chapter-6/","title":"Chapter 6 - Tools for parallellization","text":""},{"location":"chapter-6/#chapter-6-tools-for-parallellization","title":"Chapter 6 - Tools for parallellization","text":"<p>These tools can be used for parallelizing a problem on a HPC cluster (e.g. Vaughan), but also on your  local machine if you install MPI, mpi4py and dask_mpi. To execute a python script <code>script.py</code> in parallel  on Vaughan, you must execute a job script with the command</p> <pre><code>srun python script.py &lt;script parameters&gt;\n</code></pre> <p>This will automatically run the script with the requested resources. For more information about submitting  job scripts see Submitting jobs on Vaughan.</p> <p>To run the python script in parallel on 4 CPUs on your local machine you must run this commmand in a terminal:</p> <pre><code>mpirun -np 4 python script.py\n</code></pre>"},{"location":"chapter-6/#mpi4py","title":"Mpi4py","text":"<p>Mpi4py is a Python package that wraps the popular MPI framework for message passing between processes. It is very  flexible, but typically requires quite a bit of work to set up all the communication correctly.</p> <p>Here are some useful links:</p> <ul> <li>Mpi4py documentation.</li> <li>The mpi4py documents the Python wrappers for MPI functions, not the wrapped functions themselves. The MPI documentation is therefor also useful.</li> <li>A github repo with som simple mpi4py examples.</li> <li>Some hints for setting up a master-slave configuration of cpus:<ul> <li>MPI master slave</li> <li>Get master to do work in a task farm</li> </ul> </li> </ul> <p>Note</p> <p>On your local machine you must first install a MPI library for your OS, before you can <code>pip install mpi4py</code>.</p>"},{"location":"chapter-6/#dask-mpi","title":"Dask-MPI","text":"<p>The Dask-MPI project makes it easy to deploy Dask from within an existing MPI environment, such as one created with  the common MPI command-line launchers mpirun or mpiexec. Such environments are commonly found in high performance  supercomputers, academic research institutions, and other clusters where MPI has already been installed. The dask  documentation is here.</p> <p>Dask futures provide a simple approach to distribute the computation  of a function for a list of arguments over a number of workers and gather the results. The approach reserves one cpu  for the scheduler (rank 0) and one for the client script (rank 1). All remaining ranks are <code>dask</code> workers. Here is  a typical example that computes the square of each item in an iterable using an arbitrary number of dask workers:</p> <pre><code>from mpi4py import MPI\nfrom dask_mpi import initialize\ninitialize()\n# Create a Client object\nfrom distributed import Client\nclient = Client()\n\ndef square(i):\n    result = i*i\n    # a print statement, just to see which worker is computing which result:\n    print(f'rank {MPI.COMM_WORLD.Get_rank()}: {i=}**2 = {result}')\n    # Note that the order of the output of the print statements does NOT \n    # necessarily correspond to the execution order.\n    return result\n\nif __name__ == \"__main__\":\n    # Apply the square method to 0, 1, ..., 999 using the available workers\n    futures = client.map(square, range(1000))\n    # client.map() returns immediately after the work is distributed.\n    # Typically, this is well before the actual work itself is finished.\n    # Gather the results\n    results = client.gather(futures)\n    # client.gather() can only return after all the work is done, obviously.\n    print('-*# finished #*-')\n</code></pre> <p>Note</p> <p>At the time of writing, dask is not installed as an LMOD module on the cluster. So you must install it yourself. Make sure you first source the <code>wetppr-env.sh</code> script mentioned in LMOD modules.</p> <p>To install <code>dask_mpi</code>, run:</p> <pre><code>&gt; . path/to/wetppr-env.sh     # to set $PYTHONUSERBASE\n&gt; python -m pip install --user dask_mpi --upgrade\n&gt; python -m pip install --user dask distributed --upgrade\n</code></pre> <p>Note</p> <p>This installs <code>dask_mpi</code>, <code>dask</code> and <code>dask.distributed</code> into the directory specified by <code>$PYTHONUSERBASE</code>. This environment variable must also be available to the job. If <code>$PYTHONUSERBASE</code> is not set in your <code>~/.bashrc</code> file, you must set it in the job script.</p> <p>Note</p> <p><code>Dask-MPI</code> builds on <code>mpi4py</code>, so, MPI and mpi4py need to be available in your environment.</p>"},{"location":"evaluation/","title":"Evaluation of this course","text":""},{"location":"evaluation/#evaluation-of-this-course","title":"Evaluation of this course","text":"<p>In this course you will be learning by doing. You will be given an assignment, a (parallel) programming task on which you will work for several weeks, under my supervision and with my support. </p> <p>The exam consists of a presentation of your project work (usually in the last week of the course) in which you must</p> <ul> <li>explain the problems you encountered,</li> <li>explain your approach,</li> <li>provide performance measurements for the different versions your code, and for different node counts,</li> <li>explain the performance measurements,</li> <li>tell me what you found difficult during this course.</li> </ul> <p>During the presentation I will ask some questions, mainly because I am curious and eager to learn something, but also to ensure that you understand what you present.</p>"},{"location":"evaluation/#assignment","title":"Assignment","text":"<p>Here is this year's assignment.</p>"},{"location":"evaluation/#use-of-vsc-clusters-for-the-assignment","title":"Use of VSC clusters for the assignment","text":"<p>Students of the course 2000wetppr must use one of the VSC-clusters for the project work. Check out Access to VSC infrastructure and set up of your environment. </p>"},{"location":"evaluation/#guide-lines","title":"Guide lines","text":""},{"location":"evaluation/#learning-by-doing","title":"Learning by doing","text":"<p>The assignment is there because imho programming is something you can only learn by doing. It involves important skills that you should develop while working on the assignment:</p> <ul> <li>Using the background information presented in chapters 1-4</li> <li>Reason about the mathematical formulation of the problem and the algorithm to solve it,</li> <li>Do research on the problem, with respect to solution algorithms and implementation issues.</li> <li>Write and debug code in Python. </li> <li>Learn how slow Python functions can be sped up by converting them to either C++ or Fortran. </li> <li>Run your code on one of the UAntwerp HPC clusters.</li> </ul> <p>Learning is an incremental process. Especially for scientific software development the following is a good approach:</p> <ol> <li>Try, and test (We'll see what testing exactly means). </li> <li>Fail (often, the faster you fail, the faster you learn! ).   </li> <li>Think and do research (Google - or any other good search engine, for that matter - is your best friend), and come up with an improvement. This is the hardest part, it requires intelligence and creativity.</li> <li>Iterate, i.e. restart at 1., until you no more fail and are satisfied with the solution.</li> <li>Document your itinerary. Document your classes, functions, variables, and keep track of the documents that guided you to solving the problems encountered. When you will look at your work three months (only!) after you left it as is, you will wonder what it was all about if you didn't document it. </li> </ol> <p>Although this approach may look as if you are supposed to find the solution to the problem in books or on the World Wide Web, this does not at all exclude creativity. Learning about how other researchers approached a problem, can easily spark new ideas that get you going. The fail fast, fail often principle also </p> <ul> <li>urges you to start as simple and small as possible and </li> <li>make incremental changes. </li> </ul> <p>Don't write a lot of code before you try and test. Typically, and this is corroborated by research, one bug is introduced with every new 10 lines. Finding 10 bugs in 100 lines is a lot more difficult than finding one bug in 10 lines (although sometimes there is more than one bug :( ). </p> <p>Anecdotical evidence</p> <p>A PhD student once asked me for support. He had written a ~10 000 line Fortran program (without tests). When he ran it, the results were not what he expected and he suspected that there was a bug 'somewhere'. He asked if I could help him find the bug and - no kidding - by the end of next week because the program had to go into production by then. I had to disappoint him and told him that he needed a true magician, which I am not. Obviously, the program was flawed in some sense, but other than a bug it might just as well be one or more of the issues below:</p> <ul> <li>The program may contain many bugs, which is very well possible in view of its size.</li> <li>The algorithm for solving the problem is inappropriate.</li> <li>There is an accuracy problem, This can be caused by inadequate  discretisation of time and/or space, an insufficient set of basis functions for expanding the solution, or by the finite precision of floating point numbers. (Floating point numbers are a processor's approximation of the real numbers, but they do have different mathematical properties. E.g. floating point addition is not commutative.)</li> <li>The mathematical formulation itself can  be flawed or misunderstood.</li> <li>It is even possible that the program is correct but that the researcher's expectations are wrong. </li> <li>...</li> </ul> <p>It could easily take weeks, if not months to write tests for the individual components to learn about the behaviour of the program and narrow down to the source of the error. </p> <p>For this reason a sound strategy for scientific software development that makes sure that the code you write has a sense of correctness is indispensable. Had the researcher come to me before he started programming this is the advice he would have been given: </p> <p>Tip</p> <p>Write 5 lines of code and write a test for them. Do not proceed (with the next 5 lines) before the test passes. Just 5, not 10! Your test code is also code and will initially contain bugs as well. As you get more experienced you may increase that number to 6, even 7, ...</p> <p>Admittedly, this advice is slightly biased to the conservative side, but I hope you get the point. You will be surprised how many mistakes you make, being a novice. But as you will discover the source of error soon, your progress will not come to a halt. Instead, you will learn fast and your progress will even speed up. You will be given practical tools to accomplish this. </p> <p>Caveat</p> <p>There is a small disadvantage to the approach we are following. It is biased towards bottom-up design. In bottom-up design your start from the details, gradually aggregating them into larger parts, and, eventually, into the final application. Its opponent is top-down design, in which you start with the a high-level formulation of the problem. This is then broken down in smaller components and gradually refined until all details are covered. With a bit of imagination it is, however, possible to write tests for a top-down approach by focussing  on how the components work together, rather than on what they are doing. Top-down design is important because it forces you to think on the high-level structure of your application. This is paramount to how fast users will adopt your application, because it relates to user-friendly-ness, intuitive understanding, flexibility, ... In general, however, research scientists seem to better at ease with bottom-up thinking.  </p>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#glossary","title":"Glossary","text":"<p>Here is an alphabetical list of terms with links to where they are explained in the text.</p>"},{"location":"glossary/#_1","title":"*","text":"<ul> <li>Local** refers to the physical machine you are working on, i.e. your desktop or laptop. </li> </ul>"},{"location":"glossary/#_2","title":"`","text":"<ul> <li><code>LMOD</code> modules</li> </ul>"},{"location":"glossary/#a","title":"A","text":"<ul> <li>address space</li> <li>anti-pattern</li> <li>array of structures</li> </ul>"},{"location":"glossary/#b","title":"B","text":"<ul> <li>bandwidth limited</li> <li>bandwith saturated</li> </ul>"},{"location":"glossary/#c","title":"C","text":"<ul> <li>Cache coherence</li> <li>cache coherent non-uniform memory architecture</li> <li>cache line</li> <li>cache-oblivious</li> <li>ccNUMA</li> <li>cell-based Verlet list construction</li> <li>code modernization</li> <li>Code optimization</li> <li>Common sense optimizations</li> <li>communication</li> <li>computational complexity</li> <li>computational intensity</li> <li>compute limited</li> <li>compute node</li> </ul>"},{"location":"glossary/#d","title":"D","text":"<ul> <li>Debugging</li> <li>Distributed memory parallelization</li> </ul>"},{"location":"glossary/#e","title":"E","text":"<ul> <li>embarrassingly parallel</li> </ul>"},{"location":"glossary/#h","title":"H","text":"<ul> <li>hardware thread</li> <li>Hybrid memory parallelization</li> </ul>"},{"location":"glossary/#i","title":"I","text":"<ul> <li>instruction pipelining</li> </ul>"},{"location":"glossary/#l","title":"L","text":"<ul> <li>L1 cache</li> <li>L2 cache</li> <li>L3 cache</li> <li>loop fusion</li> </ul>"},{"location":"glossary/#m","title":"M","text":"<ul> <li>main memory</li> <li>memory bandwidth</li> <li>multi-core</li> <li>multi-processor</li> </ul>"},{"location":"glossary/#n","title":"N","text":"<ul> <li>node</li> <li>NUMA</li> </ul>"},{"location":"glossary/#p","title":"P","text":"<ul> <li>parallel program</li> <li>Partitioned Global Address Space</li> <li>pattern</li> <li>peak performance</li> <li>pipeline stalls</li> <li>process</li> <li>profiling</li> </ul>"},{"location":"glossary/#r","title":"R","text":"<ul> <li>registers</li> </ul>"},{"location":"glossary/#s","title":"S","text":"<ul> <li>sequential</li> <li>serial</li> <li>shared memory parallelization</li> <li>short-ranged</li> <li>SIMD vectorisation</li> <li>software thread</li> <li>Spatial locality</li> <li>structure of Arrays</li> </ul>"},{"location":"glossary/#t","title":"T","text":"<ul> <li>Temporal locality</li> <li>thread</li> <li>tiling</li> <li>time to solution</li> </ul>"},{"location":"glossary/#v","title":"V","text":"<ul> <li>Verlet list</li> </ul>"},{"location":"links/","title":"Useful links","text":""},{"location":"links/#useful-links","title":"Useful links","text":""},{"location":"links/#vs-code","title":"VS Code","text":"<ul> <li>VS Code Tutorial \u2013 Become More Productive</li> </ul>"},{"location":"links/#c","title":"C++","text":"<ul> <li>cplusplus.com</li> <li>cppreferencee.com</li> </ul>"},{"location":"links/#fortran","title":"Fortran","text":"<ul> <li>fortran-lang.org</li> </ul>"},{"location":"links/#python","title":"Python","text":"<ul> <li>python.org</li> <li>realpython.com</li> </ul>"},{"location":"links/#parallelization-approaches","title":"Parallelization approaches","text":"<ul> <li>OpenMP: C/C++/Fortran</li> <li>MPI: C/C++/Fortran</li> <li>mpi4py: Python, the documentation of mpi4py is not self-contained. It    relies (silently) on the documentation of MPI.</li> <li>multiprocessing: Python</li> <li>concurrent.futures: Python</li> <li>dask: Python</li> </ul>"},{"location":"links/#hpc","title":"HPC","text":"<ul> <li>Georg Hager's blog</li> <li>SC20 tutorial \u201cNode-Level Performance Engineering\u201d: This is all about    \"When to parallelize, and what to do first\".</li> </ul>"},{"location":"links/#project-management","title":"Project management","text":"<ul> <li> <p>wiptools: managing your Python/C++/Fortran project, helps with</p> </li> <li> <p>creating new projects</p> </li> <li>adding Python sub-modules, Python applications (CLIs), binary extension modules written in C++ and Fortran. </li> <li>automatically extracting documentation from the doc-strings of your files (html or pdf)</li> <li>(unit) testing (pytest)</li> <li>publishing the documentation on readthedocs</li> <li>publishing your code on the Python package index </li> <li>version management and control (on GitHub)</li> </ul>"},{"location":"links/#other-courses","title":"Other courses","text":"<ul> <li>This</li> <li>The Missing Semester of Your CS Education</li> </ul>"},{"location":"more-links/","title":"More links","text":"<p>This page is a compilation of links I found interesting while learning programming and while solving everyday problems in project management and maintenance, and of course scientific computing. As I keep on learning this list evolves continuously :-)</p> <p>How to Learn to Code - 8 Hard Truths</p>"},{"location":"more-links/#software-engineering","title":"Software engineering","text":"<ul> <li>Cognitive Biases In Software Development</li> <li>Lies We Programmers Love to Believe</li> <li>Binary thinking</li> <li>Anecdotical overconfidence</li> <li>Absolutist stances</li> <li>Recency bias</li> <li>Vanity metrics</li> <li>What scientists must know about hardware to write fast code A simplified view - but not over-simplified - on how hardware affects performance. Written with Julia in mind rather than Python, but the principles remain valid.</li> <li>Clean architecture</li> <li>The Grand Unified Theory of Software Architecture</li> <li>Software Engineering's Greatest Hits Very interesting.</li> <li>SOLID principles for software design</li> <li>Software engineering practices</li> <li>A high-velocity style of software development</li> </ul>"},{"location":"more-links/#problem-solving","title":"Problem solving","text":"<ul> <li>The mental game of Python - Raymond Hettinger - pybay 2019</li> <li>You should be reading academic computer science papers</li> <li>7 Functional Programming Techniques Every Developer Should Know</li> <li>recursion</li> <li>structural pattern matching</li> <li>immutability</li> <li>pure functions</li> <li>higher order functions</li> <li>function composition</li> <li>lazy evaluation</li> </ul>"},{"location":"more-links/#python","title":"Python","text":""},{"location":"more-links/#python-for-beginners","title":"Python for Beginners","text":"<ul> <li>Real Python: awesome tutorials on many Python subjects (not everything is free)</li> <li>Python morsels: awesome tutorials on many Python subjects (not everything is free)</li> <li>Harvard CS50\u2019s Introduction to Programming with Python</li> <li>Learn Python: 7 of my favorite resources</li> <li>De Programmeursleerling - Pieter Spronck (in Dutch)</li> <li>Slither into Python</li> <li>Learn Python by building 5 games</li> <li>How do I start learning Python?</li> <li>Learn Python Programming</li> <li>Python cheat sheet</li> <li>Getting started with Python</li> <li>The full Python tutorial</li> <li>Python &amp; PyGame Tutorial \u2013 Code a Duck Hunt Game</li> </ul>"},{"location":"more-links/#python-as-a-language","title":"Python as a language","text":"<ul> <li>What makes Python a great language?</li> <li>Python is known for being a language that\u2019s easy to read, quick to develop in, and applicable to a wide range of scenarios</li> <li>Writing your first Python program</li> <li>How long did it take you to learn Python Wait, don\u2019t answer that. It doesn\u2019t matter. Ned Batchelder</li> <li>PyVideo is a great resource for finding conference talks on specific topics.</li> </ul>"},{"location":"more-links/#python-idioms-readability-and-internals","title":"Python idioms, readability and internals","text":""},{"location":"more-links/#decorators","title":"Decorators","text":"<ul> <li>Practical decorators</li> <li>Useful Python decorators for Data Scientists</li> <li>Decorators, unwrapped How do they work - PyCon 2017</li> <li>Decorators and descriptors decoded - PyCon 2017</li> <li>Gang of 4 inspired decorators</li> <li>pycon 2021 TUTORIAL / Geir Arne Hjelle / Introduction to Decorators: Power UP Your Python Code</li> </ul>"},{"location":"more-links/#regular-expressions","title":"Regular expressions","text":"<ul> <li>How on Earth does ^.? produce primes? interesting, entertaining, and explaining regexes</li> <li>Regular Expressions: Regexes in Python (Part 1)</li> <li>Regular Expressions: Regexes in Python (Part 2)</li> <li>Yes, It's Time to Learn Regular Expressions - PyCon 2017</li> <li>Readable Regular Expressions - PyCon 2017</li> <li>pycon 2021 TUTORIAL / Trey Hunner / Hands-On Regular Expressions in Python</li> </ul>"},{"location":"more-links/#dictionaries","title":"Dictionaries","text":"<ul> <li>The Dictionary Even Mightier - PyCon 2017</li> <li>Modern Python Dictionaries: A confluence of a dozen great ideas - PyCon 2017</li> <li>Getting the most out of Python collections</li> <li>Understanding Attributes, Dicts and Slots in Python</li> </ul>"},{"location":"more-links/#exceptions","title":"Exceptions","text":"<ul> <li>Better Python tracebacks with Rich</li> <li>Write Unbreakable Python</li> <li>pretty-errors: Prettifies Python exception output to make it legible</li> <li>Python KeyError Exceptions and How to Handle Them</li> <li>Passing Exceptions 101 Paradigms in Error Handling - PyCon 2017</li> <li>Exceptions</li> <li>pycon 2021 TALK / Reuven M. Lerner / When is an exception not an exception? Using warnings in Python</li> </ul>"},{"location":"more-links/#logging","title":"Logging","text":"<ul> <li>An Intro to Logging with Python and Loguru</li> </ul>"},{"location":"more-links/#type-annotation","title":"Type annotation","text":"<ul> <li>Master Python typing (type hints) with interactive online exercises!</li> <li>Type-checked Python in the real world - PyCon 2018 mypy</li> <li>Applying mypy to real world projects</li> <li>Types at the Edges in Python</li> <li>Type hints for busy programmers</li> <li>Exhaustiveness (enum) Checking with Mypy</li> <li>Type annotations</li> <li>pycon 2021 TALK / Alexander Hultn\u00e9r / Intro to Pydantic, run-time type checking for your dataclasses</li> <li>pycon 2021 TALK / Luciano Ramalho / Protocol: the keystone of type hints</li> <li>pycon 2021 TALK / Maggie Moss / Gradual Typing in Practice</li> </ul>"},{"location":"more-links/#iteration","title":"Iteration","text":"<ul> <li>Looping Like a Pro in Python - PyCon 2017</li> <li>Tour of Python Itertools</li> <li>A tour of Python's itertools library</li> <li>Python Dictionary Iteration: Advanced Tips &amp; Tricks</li> <li>Generators, Iterables, Iterators in Python: When and Where</li> <li>Modify Iterables While Iterating in Python</li> </ul>"},{"location":"more-links/#strings","title":"Strings","text":"<ul> <li>73 Examples to Help You Master Python's f-strings</li> <li>Python f-strings Are More Powerful Than You Might Think</li> <li>The unreasonable effectiveness of f-strings and re.VERBOSE how to construct readable and documented regular expressions.</li> <li>Pragmatic Unicode - Ned Batchelder - PyCon 2012</li> </ul>"},{"location":"more-links/#context-manager","title":"Context manager","text":"<ul> <li>The Curious Case of Python's Context Manager</li> </ul>"},{"location":"more-links/#code-execution","title":"Code execution","text":"<ul> <li>How to run a python script</li> <li>The many ways to pass code to Python from the terminal</li> <li>pycon 2021 TALK / Graham Bleaney, the_storm/ Unexpected Execution: Wild Ways Code Execution can Occur in Python</li> <li>Python's many command-line utilities by Trey Hunner</li> </ul>"},{"location":"more-links/#internals","title":"Internals","text":"<ul> <li>Boolean short-circuiting</li> <li>Cpython source code guide</li> <li>Know thy self - Methods and method binding - PyCon 2017</li> <li>Namespaces and Scope in Python</li> <li>Unpacking in Python: Beyond Parallel Assignment</li> <li>Constant folding in Python (constant expressions)</li> <li>Python behind the scenes</li> <li>Python behind the scenes #7: how Python attributes work</li> <li>Unravelling the import statement</li> <li>Episode 40: How Python Manages Memory and Creating Arrays With np.linspace</li> <li>Syntactic sugar</li> <li>Unravelling Python classes</li> <li>Python range</li> <li>Python Lambda functions</li> <li>Strict Python function parameters</li> <li>Python bytecode explained</li> <li>This Is Why Python Data Classes Are Awesome</li> <li>understanding class and instance variables</li> <li>Blog on Python internals with many interesting topics</li> <li>Python List sort(): An In-Depth Guide to Sorting Lists</li> <li>pycon 2021 TALK / Sebastiaan Zeeff / The magic of \"self\": How Python inserts \"self\" into methods</li> </ul>"},{"location":"more-links/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Python Pitfalls - Expecting The Unexpected</li> <li>Weird Python \"Features\" That Might Catch You By Surprise</li> <li>Python quirks</li> <li>How to write Python code people actually want to use</li> <li>The Elements of Python Style</li> <li>Elegant Solutions For Everyday Python Problems - PyCon 2018</li> <li>Readability Counts - PyCon 2017 Trey Hunner</li> <li>Transforming Code into Beautiful, Idiomatic Python Raymond Hettinger</li> <li>Elegant Solutions For Everyday Python Problems Nina Zakharenko</li> <li>Craft Your Python Like Poetry Trey Hunner on line length and wrapping lines</li> <li>Python module of the week</li> <li>Python Tips and Tricks, You Haven't Already Seen - part 1</li> <li>Python Tips and Tricks, You Haven't Already Seen - part 2</li> <li>30 Python Best Practices, Tips, And Tricks</li> <li>pythonic things</li> <li>71 Python Code Snippets for Everyday Problems</li> <li>Clean Code Concepts Adapted for Python</li> <li>The place of the 'is' syntax in Python</li> <li>5 Things You're Doing Wrong When Programming in Python</li> <li>10 Python Tips and Tricks For Writing Better Code</li> <li>Unpacking in Python: Beyond Parallel Assignment</li> <li>When Python Practices Go Wrong About the use of exec() and eval(). A presentation, so, the logic isn`t always obvious, but definitely an interesting topic. Here's the corresponding video When Python Practices Go Wrong - Brandon Rhodes - code::dive 2019</li> <li>Demystifying Python\u2019s Descriptor Protocol</li> <li>Why You Should Use More Enums In Python</li> <li>Novice to Advanced RegEx in Less-than 30 Minutes + Python</li> <li>10 Awesome Pythonic One-Liners Explained</li> <li>Stop writing classes</li> <li>New Features in Python 3.9 You Should Know About</li> <li>Python 101 \u2013 Working with Strings</li> <li>A Guide to Python Lambda Functions</li> <li>Pythonic code review</li> <li>Python args and kwargs: Demystified</li> <li>Python Code style and pythonic idioms</li> <li>Learn something new about Python every day in less than 1 minute</li> <li>The pass Statement: How to Do Nothing in Python</li> <li>The Correct Way to Overload Functions in Python</li> <li>Singleton is a bad idea</li> <li>Organize Python code like a PRO</li> <li>Coding 102: Writing code other people can read</li> <li>sockets for dummies</li> </ul>"},{"location":"more-links/#useful-packages","title":"Useful packages","text":""},{"location":"more-links/#installing-packages","title":"Installing packages","text":"<ul> <li>A quick-and-dirty guide on how to install packages for Python</li> <li>uv - pip killer or yet another package manager?: uv is the \"pip but blazingly fast\u2122\ufe0f because it's written in rust\" and is developed by the same folks that built ruff. It is designed as a drop-in replacement for pip and pip-tools for package management. uv supports everything you'd expect from a modern Python packaging tool: editable installs, Git dependencies, URL dependencies, local dependencies, constraint files, source distributions, custom indexes, and more, all designed around drop-in compatibility with your existing tools. uv's virtual environments are standards-compliant and work interchangeably with other tools \u2014 there's no lock-in or customization required. It supports Linux, Windows, and macOS, and has been tested at-scale against the public PyPI index.</li> </ul>"},{"location":"more-links/#single-packagepurpose-links","title":"Single package/purpose links","text":"<ul> <li>CPython API for asynchronous functions</li> <li>The Right Way to Run Shell Commands From Python: If you find yourself orchestrating lots of other processes in Python, then you should at least take a look at <code>sh</code> library. (plus a lot of very useful comments on standard library ways to run OS tasks)</li> <li>Whenever: Typed and DST-safe datetimes for Python, written in Rust</li> <li>tqdm: Show progress in your Python apps</li> <li>Pathlib</li> <li>safer: a safer file writer</li> <li>Python 101 \u2013 Creating Multiple Processes</li> <li>sproc: subprocesses for subhumans</li> <li>Python and PDF: A Review of Existing Tools</li> <li>Pickle\u2019s nine flaws</li> <li>pycon 2023 Talks - Glyph: How To Keep A Secret API keys, passwords, auth tokens, cryptographic secrets\u2026 in the era of cloud-based development, we've all got a bunch of them. But where do you put them? How do you keep them safe? And how can you access them conveniently from your Python code, both in development and production, without putting them at risk?</li> <li>A cross-platform Python module for copy and paste clipboard functions</li> <li>rich: rich text and beautiful formatting in the terminal</li> <li>Awesome pattern matching (apm) for Python</li> <li>Scheduling All Kinds of Recurring Jobs with Python</li> <li>The Python pickle Module: How to Persist Objects in Python</li> <li>bidict</li> <li>Python asyncio</li> <li>stringzilla</li> <li>module itertools overview</li> <li>PDF-Extract-Kit</li> <li>ghostty embed a terminal emulator in your package, First impressions with ghostty</li> </ul>"},{"location":"more-links/#package-overview-links","title":"Package overview links","text":"<ul> <li>A curated list of awesome Python frameworks, libraries, software and resources</li> <li>Python's Missing Batteries: Essential Libraries You're Missing Out On</li> <li>The 22 Most-Used Python Packages in the World</li> <li>Five Amazing Python Libraries you should be using!</li> <li>The most underrated python packages</li> <li>Python Packages: Five Real Python Favorites</li> </ul>"},{"location":"more-links/#interactive-python","title":"Interactive Python","text":""},{"location":"more-links/#notebooks","title":"Notebooks","text":"<ul> <li>Satyrn: A modern Jupyter client for Mac</li> <li>Jupyter Notebooks in the IDE</li> <li>Jupyter everywhere</li> <li>8 surprising ways how to use Jupyter Notebook</li> <li>IPython and Jupyter in Depth: High productivity, interactive Python Matthias Bussonier</li> </ul>"},{"location":"more-links/#python-for-hpc","title":"Python for HPC","text":""},{"location":"more-links/#concepts-and-ideas","title":"Concepts and ideas","text":"<ul> <li>Performance Python: Seven Strategies for Optimizing Your Numerical Code</li> <li>Speeding up your code</li> <li>Does it ever make sense to use more concurrent processes than processor cores? You can have as many threads as you want as long as they're doing nothing.</li> <li>Putting the \u201cYou\u201d in CPU Curious exactly what happens when you run a program on your computer? Read this article to learn how multiprocessing works, what system calls really are, how computers manage memory with hardware interrupts, and how Linux loads executables.</li> <li>PyO3: From Python to Rust and Back Again: improving Python's performance by integrating Rust code. The conversation explores the challenges and ongoing developments in making Python and Rust work seamlessly together, highlighting the complexities involved in bridging these two languages for enhanced performance.</li> <li>When NumPy is too slow</li> </ul>"},{"location":"more-links/#numpy-arrays","title":"Numpy arrays","text":"<ul> <li>pycon 2023 Talks - Jodie Burchell: Vectorize using linear algebra and NumPy to make your Python code fast</li> <li>An overview of the Sparse Array Ecosystem for Python</li> </ul>"},{"location":"more-links/#jax","title":"JAX","text":"<ul> <li>JAX, a high-performance numerical computation library with support for automatic differentiation.</li> <li>The PyTorch developer's guide to JAX fundamentals</li> </ul>"},{"location":"more-links/#numba","title":"Numba","text":"<ul> <li>Numba</li> <li>Pycon 2023 Tutorial - Cheuk Ting Ho: Power up your work with compiling and profiling</li> <li>Understanding CPUs can help speed up Numba and NumPy code</li> <li>Bridging the CUDA C++ Ecosystem and Python Developers with Numbast</li> </ul>"},{"location":"more-links/#python-extensions-in-other-languages","title":"Python extensions in other languages","text":"<ul> <li>High performance Python 1</li> <li>High performance Python 2</li> <li>High performance Python 3</li> <li>Python Bindings: Calling C or C++ From Python</li> <li>PyCon 22 Talk - Henry Fredrick Schreiner III: Building a binary extension</li> <li>Wrapping C++ with Cython: intro</li> <li>Implementing C++ Virtual Functions in Cython</li> <li>How vectorization speeds up your Python code Quite a few interesting ideas: - self-instrumenting a python program for performance measurements - using pypy</li> <li>Cython, Rust, and more: choosing a language for Python extensions</li> <li>Calling Rust from Python</li> <li>Introduction to Coding In Rust for Pythonistas ArjanCodes</li> <li>nanobind, a fastder pybind11</li> </ul>"},{"location":"more-links/#parallelization-approaches-mimicking-or-wrapping-openmp-and-mpi","title":"Parallelization approaches mimicking or wrapping OpenMP and MPI:","text":"<ul> <li>Pymp \u2013 OpenMP-like Python Programming A really interesting concept, not as efficient as OpenMP itself (which incurs quite a bit of overhead itself), and, of course, limited to a single node. As the number of cores per node keeps increasing, pymp may be a good solution for problems that can do with a single node.</li> <li>High performance Python 4 Mpi4py, doing mpi from Python.</li> </ul>"},{"location":"more-links/#other-parallel-processing-approaches","title":"Other parallel processing approaches:","text":"<ul> <li>Why your multiprocessing Pool is stuck: solving deadlocking issues</li> <li>Sequential Execution, Multiprocessing, and Multithreading IO-Bound Tasks in Python</li> <li>Common Issues Using Celery (And Other Task Queues)</li> <li>The Parallelism Blues: when faster code is slower</li> <li>Dask</li> <li>pycon 2021 TUTORIAL / James Bourbeau, Julia Signell / Hacking Dask: Diving Into Dask's Internals</li> <li>Visualize multi-threaded Python programs with an open source tool</li> </ul>"},{"location":"more-links/#the-gil","title":"The GIL","text":"<ul> <li>pycon 2023 Talks - Alireza Farhidzadeh: Getting Around the GIL: Parallelizing Python for Better Performance</li> <li>Tracing the Python GIL</li> <li>Python behind the scenes #13: the GIL and its effects on Python multithreading</li> <li>Instrumenting Python GIL with eBPF</li> </ul>"},{"location":"more-links/#gpu-computing-in-python","title":"GPU computing in Python","text":"<ul> <li>Accelerating Python on GPUs with nvc++ and Cython</li> <li>What Every Developer Should Know About GPU Computing</li> <li>How GPU computing works</li> </ul>"},{"location":"more-links/#profiling-and-performance-analysis","title":"Profiling and performance analysis","text":"<ul> <li>Profiling python</li> <li>Python profiling with blackfire</li> <li>Python 3.9 StatsProfile</li> <li>Profiling Python Code</li> <li>Disassemble Your Python Code</li> <li>Counting FLOPS and other CPU counters in Python</li> <li>A Comprehensive Guide to Profiling Python Programs</li> <li>Yet Another Python Profiler, but this time thread&amp;coroutine&amp;greenlet aware</li> <li>0x.Tools: X-Ray vision for Linux systems: a set of open-source utilities for analyzing application performance on Linux.</li> </ul>"},{"location":"more-links/#pop-performance-optimisation-and-productivity-coe","title":"POP - Performance Optimisation and Productivity CoE","text":"<ul> <li>POP-COE Learning Material / Documentation</li> <li>POP-COE Open-source toolsets (developed by POP partners and collaborators)</li> </ul>"},{"location":"more-links/#timing","title":"Timing","text":"<ul> <li>Python timer functions</li> <li>How to Benchmark (Python) Code</li> </ul>"},{"location":"more-links/#memory-profiling","title":"Memory profiling","text":"<ul> <li>Memory profiler for Python</li> <li>pycon 2021 TALK / Emery Berger / Scalene: A high-performance, high-precision CPU+GPU+memory profiler for Python</li> <li>Optimizing Memory Usage in Python Applications</li> <li>Pycon 2023 Talk - Pablo Galindo Salgado: How memory profilers work</li> </ul>"},{"location":"more-links/#resource-monitoring","title":"Resource monitoring","text":"<ul> <li>Remora</li> <li>REMORA: REsource MOnitoring for Remote Applications</li> <li>My favorite Linux top command options</li> </ul>"},{"location":"more-links/#lumi-and-amd-systems-in-general","title":"LUMI (and AMD systems in general)","text":"<ul> <li> <p>LUMI training materials</p> </li> <li> <p>Introduction to ROCm Profiler -AMD Profiling workshop - Day 1- Pt1</p> </li> <li>Introduction to OmniTrace - AMD Profiling workshop - Day 1 - Pt2</li> <li>Introduction to Omniperf - AMD Profiling workshop - Day 2- Pt1</li> <li>Introduction to Roofline 1 - AMD Profiling workshop - Day 2 - Pt2</li> </ul> <p>(The Pawsey Supercomputing Research Centre is located in Perth, Western Australia)</p>"},{"location":"more-links/#design-patterns","title":"Design patterns","text":"<ul> <li>Design Patterns in Python for the Untrained Eye - PyCon 2019</li> <li>Python patters</li> <li>Refactoring and Design patterns</li> <li>Pyton anti-patterns</li> <li>Coding problems</li> </ul>"},{"location":"more-links/#testing","title":"Testing","text":""},{"location":"more-links/#pytest","title":"PyTest","text":"<ul> <li>Getting Started Testing: pytest edition</li> <li>Pytest Fixtures: A Complete Guide to Pytest Fixtures</li> <li>A Gentle Introduction to Testing with PyTest</li> <li>Visual Testing with PyCharm and pytest - PyCon 2018</li> <li>8 great pytest plugins</li> <li>Pytest Features, That You Need in Your (Testing) Life</li> <li>Effective Python Testing With Pytest</li> <li>Testing Python Applications with Pytest </li> <li>pycon 2021 TUTORIAL / Moshe Z / Python Unit Testing with Pytest and Mock</li> </ul>"},{"location":"more-links/#pytest-plugins","title":"PyTest plugins","text":"<ul> <li>A pytest plugin designed for analyzing resource usage</li> <li>15 amazing pytest plugins and more (an episode on an interesting blog).</li> </ul>"},{"location":"more-links/#other-packages","title":"Other packages","text":"<ul> <li>unittest\u2019s new context methods in Python 3.11</li> <li>tox nox and invoke Break the Cycle: Three excellent Python tools to automate repetitive tasks</li> <li>Hypothesis</li> <li>Escape from auto-manual testing with Hypothesis!</li> <li>Python Testing 201 with pytest</li> <li>ward - A modern Python test framework</li> <li>How to write doctests in Python</li> </ul>"},{"location":"more-links/#mocking","title":"Mocking","text":"<ul> <li>How to mock in Python? \u2013 (almost) definitive guide</li> <li>Why your mock doesn't work</li> </ul>"},{"location":"more-links/#property-based-testing","title":"Property-based testing","text":"<ul> <li>Property-Based Testing with hypothesis, and associated use cases</li> <li>pycon 2021 TUTORIAL / Zac Hatfield-Dodds / Introduction to Property-Based Testing</li> <li>Property-based tests for the Python standard library (and builtins)</li> <li>"},{"location":"more-links/#general-ideas","title":"General ideas","text":"<ul> <li>Beyond Unit Tests: Taking Your Testing to the Next Level - PyCon 2018</li> <li>\"WHAT IS THIS MESS?\" - Writing tests for pre-existing code bases - PyCon 2018</li> <li>An Introduction To Test Driven Development</li> <li>How To Write Tests For Python</li> <li>How I\u2019m testing in 2020</li> <li>Building Good Tests</li> <li>The Clean Architecture in Python - How to write testable and flexible code</li> <li>Document your tests</li> <li>Why you should document your tests</li> <li>ARRANGE-ACT-ASSERT: A PATTERN FOR WRITING GOOD TESTS</li> <li>There's no one right way to test your code</li> <li>Learning Python Test Automation These days, there\u2019s a wealth of great content on Python testing. Here\u2019s a brief reference to help you get started.</li> <li>Pycon 2023 Talk - Shai Geva: 10 Ways To Shoot Yourself In The Foot With Tests</li> <li>\"It's A Bug Hunt\" - Armor Plate Your Unit Tests in Cpp - Dave Steffen - CppCon 2022</li> </ul>"},{"location":"more-links/#debugging","title":"Debugging","text":""},{"location":"more-links/#gdb","title":"GDB","text":"<ul> <li>Learn to debug code with the GNU Debugger</li> <li>GDBGUI - A browser-based frontend to gdb</li> <li>GDB Tutorial - a walkthrough with examples</li> <li>Add custom windows to GDB: Programming the TUI in Python</li> <li>All about gdb, TUI and python inside gdb:</li> <li>CppCon 2015: Greg Law \"Give me 15 minutes &amp; I'll change your   view of GDB</li> <li>CppCon 2016: Greg Law \u201cGDB - A Lot More Than You   Knew</li> <li>Getting the Most Out of GDB - Mark Williamson &amp; Greg Law - C++ on Sea 2022</li> <li>CppCon 2018: Greg Law \u201cDebugging Linux C++\u201d</li> <li>Cool New Stuff in Gdb 9 and Gdb 10 - Greg Law - CppCon   2021</li> <li>Cool New Stuff in GDB 9, 10 and 11 - Greg Law - ACCU   2022 - </li> </ul>"},{"location":"more-links/#pdb","title":"PDB","text":"<ul> <li>pdb - The Python debugger</li> <li>Python debugging with pdb</li> <li>Python 101 \u2013 Debugging Your Code with pdb</li> </ul>"},{"location":"more-links/#debugging-python-binary-extensions","title":"Debugging Python binary extensions","text":"<ul> <li>Debugging Python and C(++) extensions with gdb and pdb</li> <li>When C extensions crash: easier debugging for your Python application</li> <li>Debugging Python C extensions with GDB</li> <li>Debugging a Mixed Python and C Language Stack</li> </ul>"},{"location":"more-links/#better-interfaces","title":"Better interfaces","text":"<ul> <li>pudb for Visual Debugging</li> <li>Cyberbrain: Python debugging, redefined</li> </ul>"},{"location":"more-links/#diy-debugging","title":"DIY debugging","text":"<ul> <li>tutorial on sys.settrace</li> <li>Liran Haimovitch - Understanding Python\u2019s Debugging Internals - PyCon 2019</li> <li>bdb - debugger framework python base debugger (standard library)</li> <li>Python Traceback (Error Message) Printing Variables</li> <li>Introspection in Python</li> </ul>"},{"location":"more-links/#pystack","title":"PyStack","text":"<ul> <li>PyStack: The endgame Python stack debugger</li> <li>Debugging Crashes and Deadlocks in Python using PyStack</li> </ul>"},{"location":"more-links/#logging_1","title":"Logging","text":"<ul> <li>Python logging tutorial</li> <li>Writing custom profilers for Python</li> <li>Do not log</li> <li>Understanding Python's logging library</li> <li>Logging like a pro</li> </ul>"},{"location":"more-links/#clis-and-scripting","title":"CLIs and scripting","text":""},{"location":"more-links/#general-ideas_1","title":"General ideas","text":"<ul> <li>Command Line Interface Guidelines</li> <li>build a command line text editor with Python and curses</li> </ul>"},{"location":"more-links/#click-and-typer","title":"Click and typer","text":"<ul> <li>Click</li> <li>Build AWESOME CLIs With Click in Python (ArjanCodes)</li> <li>Things I\u2019ve learned about building CLI tools in Pythonx</li> <li>QUICK: A real quick GUI generator for click</li> <li>typer: Python library for building CLI applications Built on top of click</li> </ul>"},{"location":"more-links/#bash","title":"bash","text":"<ul> <li>When laziness is efficient: Make the most of your command line</li> <li>Messing with the python shell</li> <li>Converting shell scripts to python scripts</li> <li>a Python shell environment that combines the expressiveness of shell pipelines with the power of python iterators</li> <li>iterm2 plugins written in python</li> </ul>"},{"location":"more-links/#other-tools","title":"Other tools","text":"<ul> <li>Building a CLI for Firmware Projects using Invoke</li> <li>Questionary is a Python library for effortlessly building pretty command line interfaces</li> </ul>"},{"location":"more-links/#gui","title":"GUI","text":""},{"location":"more-links/#qt","title":"Qt","text":"<ul> <li>Use PyQt's QThread to Prevent Freezing GUIs</li> <li>Learn Python GUI Development for Desktop \u2013 PySide6 and Qt Tutorial</li> </ul>"},{"location":"more-links/#tkinter","title":"Tkinter","text":"<ul> <li>CustomTkinter UI-Library</li> <li>Create a modern user interface with the Tkinter Python library</li> <li>Python Tkinter GUI Design Using ttkbootstrap</li> </ul>"},{"location":"more-links/#scientific-python","title":"Scientific Python","text":"<ul> <li>Array Oriented Programming with Python NumPy</li> <li>Numeric and Scientific Python Packages built on Numpy</li> <li>Symbolic Maths in Python</li> <li>How to use HDF5 files in Python</li> <li>A Gentle Introduction to Serialization for Python (pickle, hdf5)</li> <li>A free course on Numpy</li> <li>Generating Stl Models with Python (CAD)</li> <li>SciPy Tutorial (2022): For Physicists, Engineers, and Mathematicians</li> <li>NumPy Tutorial (2022): For Physicists, Engineers, and Mathematicians</li> <li>SymPy Tutorial (2022): For Physicists, Engineers, and Mathematicians</li> </ul>"},{"location":"more-links/#visualization","title":"Visualization","text":""},{"location":"more-links/#matplotlib","title":"Matplotlib","text":"<ul> <li>The animation tools of 3blue1brown</li> <li>Manim: A community maintained Python library for creating mathematical animations (based on the animation tools from 3blue1brown)</li> <li>matplotlib</li> <li>Effectively using matplotlib</li> <li>Matplotlib Tutorial (2022): For Physicists, Engineers, and Mathematicians</li> <li>Scientific Visualization using Python and Matplotlib</li> <li>\"Cyberpunk style\" for matplotlib plots</li> <li>widgets in matplotlib</li> </ul>"},{"location":"more-links/#other-packages_1","title":"Other packages","text":"<ul> <li>Pycon 2023 Talk - Tadeh Hakopian: The Lost Art of Diagrams: Making Complex Ideas Easy to See with Python</li> <li>ModernGL : a python wrapper over OpenGL 3.3+</li> <li>Magnum: Lightweight and modular C++11/C++14 graphics middleware for games and data visualization</li> <li>Grammar of graphics for Pyhon (using plotnine and pandas)</li> <li>Annotated area charts with plotnine</li> <li>plotly Express</li> <li>How to build beautiful plots with Python and Seaborn</li> <li>HiPlot is a lightweight interactive visualization tool to help discover correlations and patterns in high-dimensional data</li> <li>Taichi: a programming language designed for high-performance computer graphics</li> <li>Plotnine: Grammar of Graphics for Python</li> <li>pycon 2021 TUTORIAL / Husni Almoubayyed / Effective Data Visualization (with Seaborn) </li> </ul>"},{"location":"more-links/#artificial-intelligence-machine-learning","title":"Artificial intelligence, machine learning","text":"<ul> <li>How to get from high school math to cutting-edge ML/AI: a detailed 4-stage roadmap with links to the best learning resources that I\u2019m aware of</li> <li>Deep Learning Course for Beginners</li> <li>Scikit-learn, wrapping your head around machine learning - PyCon 2019</li> <li>Applied Deep Learning for NLP Using PyTorch</li> <li>Thinking like a Panda: Everything you need to know to use pandas the right way</li> <li>Top 10 Python Packages for Machine Learning</li> <li>A series how to turn machine learning models into production-ready software solutions</li> <li>Neural Networks Explained from Scratch using Python</li> <li>Machine learning made easy withe Python</li> <li>NN template</li> <li>Object localization using PyTorch : part 1</li> <li>Object localization using PyTorch : part 2</li> <li>\u00b5Transfer: A technique for hyperparameter tuning of enormous neural networks</li> <li>Practical Quantization in PyTorch</li> <li>Natural language processing demistified</li> <li>Neural Networks: Zero to Hero</li> <li>This guide is designated to anybody with basic programming knowledge or a computer science background interested in becoming a Research Scientist with \ud83c\udfaf on Deep Learning and NLP</li> <li>Harvard CS50\u2019s Artificial Intelligence with Python \u2013 Full University Course</li> </ul>"},{"location":"more-links/#data-science","title":"Data science","text":"<ul> <li>pyGWalker: A Python Library for Exploratory Data Analysis with Visualization</li> <li>streamz: Build Pipelines to Manage Continuous Streams of Data</li> <li>nfstream - A flexible network data analysis framework</li> <li>Data Science Best Practices with pandas - PyCon 2019</li> <li>Xorbits: scalable Python data science, familiar &amp; fast</li> <li>The great Python dataframe showdown, part 1: Demystifying Apache Arrow</li> <li>Python for Data Analysis, 3d Edition</li> <li>Pydantic: Pydantic provides the essential structure and validation for seamless data management.</li> </ul>"},{"location":"more-links/#pandas","title":"Pandas","text":"<ul> <li>From Pandas to Polars Upgrading Your Data Workflow \u2014 Matt Harrison (PyBay 2024)</li> <li>awesome polars</li> <li>pandas 2.0 and the Arrow revolution (part I)</li> <li>Speeding up your pandas code</li> <li>A free course on Python Pandas</li> <li>Tutorial: Indexing DataFrames in Pandas</li> <li>Efficient pandas Dataframes</li> <li>Speed up your slow pandas python code by 2500x...</li> <li>How to iterate over DataFrame rows (and should you?)</li> <li>Effective Pandas (Matt Harrison)</li> </ul>"},{"location":"more-links/#python-packaging","title":"Python packaging","text":"<ul> <li>packaging with setuptools (nov 2021)</li> <li>Inside the Cheeseshop: How Python Packaging Works - PyCon 2018 historical overview with thorough explanation</li> <li>Share Your Code! Python Packaging Without Complication - PyCon 2017</li> <li>A Python alternative to Docker</li> <li>The Python Packaging Ecosystem</li> <li>Python Packaging Is Good Now</li> <li>Conda: Myths and Misconceptions</li> <li>The private PyPI server powered by flexible backends</li> <li>Packaging without setup.py</li> <li>PDM - Python Development Master</li> <li>Python Packaging Made Better: An Intro to Python Wheels</li> <li>Options for packaging your Python code: Wheels, Conda, Docker, and more</li> <li>What the heck is pyproject.toml?</li> <li>4 Things Tutorials Don't Tell You About PyPI</li> <li>How to improve Python packaging, or why fourteen tools are at least twelve too many</li> <li>Trusted Publishing; how to publish to PyPI with Github Actions</li> </ul>"},{"location":"more-links/#tools","title":"Tools","text":"<ul> <li>Compile shell scripts to machine code.</li> <li>Beyond Hypermodern: Python is easy now</li> <li>pycon 2023 Talks - Al Sweigart: An Overview of the Python Code Tool Landscape 2023</li> <li>Software Development Checklist for Python Applications</li> <li>Faster Python Programs - Measure, don't Guess - PyCon 2019</li> <li>Python Tooling Makes a Project Tick</li> <li>Life Is Better Painted Black, or: How to Stop Worrying and Embrace Auto-Formatting</li> <li>Using GitHub, Travis CI, and Python to Introduce Collaborative Software Development - PyCon 2018</li> <li>What's in your pip toolbox - PyCon 2017</li> <li>How can I get tox and poetry to work together to support testing multiple versions of a Python dependency?</li> <li>Understanding Best Practice Python Tooling by Comparing Popular Project Templates</li> <li>My unpopular meaning about Black code formatter</li> <li>Python static analysis tools</li> <li>Leverage Sublime project folders to eashttps://martinheinz.dev/blog/34e your work</li> <li>Deep dive into how pyenv actually works by leveraging the shim design pattern</li> <li>Explore binaries using this full-featured Linux tool</li> <li>How to write a configuration file in python</li> <li>How to automatically set up a development machine with Ansible</li> <li>direnv -- unclutter your .profile &lt;https://github.com/direnv/direnv?utm_source=tldrnewsletter. direnv is an extension for your shell. It augments existing shells with a new feature that can load and unload environment variables depending on the current directory.</li> <li>A list of new(ish) command line tools</li> <li>A web-based math entry system (MathLex)</li> <li>Deep CMake for library authors CppCon 2019, Interesting CMake stuff by craig scott, the author of </li> <li>Professional CMake - a practical guide</li> <li>Developer essentials: How to search code using grep</li> </ul>"},{"location":"more-links/#version-control","title":"Version control","text":""},{"location":"more-links/#git","title":"git","text":"<ul> <li>Git Productivity Toolkit</li> <li>Writing commit messages</li> <li>lazygit: A simple terminal UI for git commands</li> <li>Introduction to Git In 16 Minutes</li> <li>9 useful tricks of git branch</li> <li>gitutor</li> <li>Git Commands to Live By - The cheat sheet that goes beyond Git basics</li> <li>Things You Want to Do in Git and How to Do Them</li> <li>Helpful git commands for beginners</li> <li>understanding git: commits are snapshots not diffs</li> <li>Getting The Most Out Of Git</li> <li>Git is my buddy: Effective Git as a solo developer</li> <li>A practical guide to using the git stash command</li> <li>gitlab</li> <li>git flight rules</li> <li>HUBFS \u00b7 File System for GitHub</li> <li>Git for professionals: Tools &amp; Concepts for Mastering Version Control with Git</li> <li>Create changelog from git history</li> <li>10 Git tips we can't live without</li> <li>idiot proof git</li> <li>how to use git squash, git rebase, and git cherry-pick</li> <li>Rewriting your git history, removing files permanently - cheatsheet &amp; guide</li> </ul>"},{"location":"more-links/#other-version-control-systems","title":"Other version control systems","text":"<ul> <li>Pijul, a sound and fast distributed version control system based on a mathematical theory of asynchronous work.</li> <li>jj init - What if we actually could replace Git? Jujutsu might give us a real shot.</li> </ul>"},{"location":"more-links/#development-environmentworkflow","title":"Development environment/workflow","text":"<ul> <li>How to set up a perfect Python project</li> <li>How to Set Up a Python Project For Automation and Collaboration</li> <li>Hypermodern Python</li> <li> <p>Thoughts on where tools fit into a workflow</p> </li> <li> <p>wiptools</p> </li> <li> <p>create-python-package a wiptools 'light'</p> </li> <li> <p>pyenv+poetry+pipx</p> </li> <li>Improving Python Dependency Management With pipx and Poetry</li> <li>Managing Python Environments</li> <li> <p>poetry</p> </li> <li> <p>pipupgrade</p> </li> </ul>"},{"location":"more-links/#ide","title":"(I)DE","text":"<ul> <li>The AI code editor Built to make you extraordinarily productive, Cursor is the best way to code with AI.</li> <li>I tried 10 code editors</li> <li>VS Code</li> <li>Visual Studio Code Extensions for Backend Development</li> <li>PyCharm</li> <li>Using Sublime Text for python</li> <li>C++ Coding with Neovim - Prateek Raman - CppCon 2022</li> <li>waveterm</li> <li>Zasper</li> </ul>"},{"location":"more-links/#cicd","title":"CI/CD","text":"<ul> <li>GitHub Actions: the full course</li> <li>Blazing fast CI with GitHub Actions, Poetry, Black and Pytest</li> <li>A beginner\u2019s guide to CI/CD and automation on GitHub</li> <li>GitHub Features Every C++ Developer Should Know - Michael Price - CppCon 2022</li> </ul>"},{"location":"more-links/#documentation","title":"Documentation","text":""},{"location":"more-links/#general-ideas_2","title":"General ideas","text":"<ul> <li>Rules for Writing Software Tutorials</li> <li>Write the Docs is a global community of people who care about documentation</li> <li>How documentation works, and how to make it work for your project - PyCon 2017</li> <li>How to document Python code with Sphinx  interesting section about tox</li> <li>Scott Meyers' advise on writing</li> <li>Write documentation as code</li> <li>pycon 2021 TALK / Meredydd Luff / Writing Good Documentation for Developers</li> <li>pycon 2021 TALK / Paul Everitt / Static Sites with Sphinx and Markdown</li> <li>Schrijfwijzer UAntwerpen</li> <li>best practices for writing code comments</li> </ul>"},{"location":"more-links/#figures","title":"Figures","text":"<ul> <li>Penrose: create beautiful diagrams just by typing notation in plain text</li> </ul>"},{"location":"more-links/#mkdocs-and-markdown","title":"Mkdocs and Markdown","text":"<p>This tutorial by Real Python really got me going in an afternoon... The extension with autorefs is also very useful. Note that the command <code>mkdocs gh-deploy</code> makes publication on readthedocs superfluous!</p>"},{"location":"more-links/#installation","title":"Installation","text":"<p>We need the following packages to build documentation:</p> <pre><code>&gt; python -m pip install mkdocs\n&gt; python -m pip install \"mkdocstrings[python]\"\n&gt; python -m pip install mkdocs-material\n&gt; python -m pip install mkdocs-autorefs\n</code></pre>"},{"location":"more-links/#markdown-faq","title":"Markdown FAQ","text":"<ol> <li>For ~~all~~ most of your formatting questions: The Markdown guide</li> <li>How to do comments</li> <li>Tune Your Configuration: Explore advanced configuration for MkDocs, such as adding support for search and multiple languages. Or install and include additional plugins. A good option is autorefs, which allows you to add relative links in your docstrings that work in your rendered documentation.  </li> <li>LateX in markdown</li> <li>Converting reStucturedText to Markdown</li> </ol>"},{"location":"more-links/#other-markdown-applications","title":"Other Markdown applications","text":"<ul> <li>Presentations</li> <li>a Markdown viewer/browser for your terminal: https://github.com/Textualize/frogmouth</li> <li>Markdown Code Runner: https://github.com/basnijholt/markdown-code-runner</li> </ul>"},{"location":"more-links/#latex","title":"latex","text":"<ul> <li>This almost class-less CSS library turns your HTML document into a website that looks like a LATEX document</li> </ul>"},{"location":"more-links/#sphinx-and-rest","title":"Sphinx and ReST","text":"<p>(imho less practical than <code>mkdocs</code>) - Writing Documentation with Sphinx and reStructuredText - Practical Sphinx - PyCon 2018  - A \u201cHow to\u201d Guide for Sphinx + ReadTheDocs - sphinx-autodoc-typehints</p>"},{"location":"more-links/#containers","title":"Containers","text":"<ul> <li>Building Python Data Science Container using Docker</li> <li>HPC-Admin Update on Containers in HPC</li> </ul>"},{"location":"more-links/#low-level-programming-languages","title":"Low-level programming languages","text":""},{"location":"more-links/#fortran","title":"Fortran","text":"<ul> <li>https://www.fortran90.org</li> </ul>"},{"location":"more-links/#c","title":"C++","text":"<ul> <li>http://www.cplusplus.com</li> <li>http://cppreference.com</li> <li>two factions of C++</li> <li>A friendly guide to the syntax of C++ method pointers</li> <li>How Many Strings Does C++ Have?</li> <li> <p>Johnny's software lab very thorough site on C++ performance issues.</p> </li> <li> <p>std::simd: How to Express Inherent Parallelism Efficiently Via Data-parallel Types - Matthias Kretz - CPPCON 2023</p> </li> <li>SIMD Libraries in C++ - Jeff Garland - CppNow 2023</li> <li>Fast C++ by using SIMD Types with Generic Lambdas and Filters - Andrew Drakeford - CppCon 2022</li> <li>The Au C++ Units Library: Handling Physical Units Safely, Quickly, &amp; Broadly - Chip Hogg - CppCon 2023</li> <li>Taking Static Type-Safety to the Next Level - Physical Units for Matrices - Daniel Withopf CppCon 22</li> <li>Exploration of Strongly-typed Units in C++: A Case Study from Digital Audio - Roth Michaels - CppCon 2023</li> <li>std::linalg: Linear Algebra Coming to Standard C++ - Mark Hoemmen - CppCon 2023</li> <li>HPX - A C++ Library for Parallelism and Concurrency - Hartmut Kaiser - CppCon 2022</li> <li>An Introduction to Multithreading in C++20 - Anthony Williams - CppCon 2022</li> <li>C++ Algorithmic Complexity, Data Locality, Parallelism, and Compiler Optimizations, seasoned with Some Concurrency - a deep dive into Performance: on the importance of cache-friendly code.</li> <li>C++ Performance Portability - A Decade of Lessons Learned - Christian Trott - CppCon 2022 (from the developers of kokkos)</li> <li> <p>Scale Transforming CUDA code for AMD gpus, an alternative to HIP.</p> </li> <li> <p>Nobody Can Program Correctly - Lessons From 20 Years of Debugging C++ Code - Sebastian Theophil CppCon 2022</p> </li> <li> <p>Back to Basics: Debugging in C++ - Mike Shah - CppCon 2022</p> </li> <li> <p>C++23: An Overview of Almost All New and Updated Features - Marc Gregoire - CppCon 2023</p> </li> <li>What\u2019s New in C++23 - Sy Brand - CppCon 2022</li> <li> <p>What\u2019s New in C++26 - Alexander Fokin CppZurich 2024</p> </li> <li> <p>Using Modern C++ to Eliminate Virtual Functions - Jonathan Gopel - CppCon 2022</p> </li> <li>Lightning Talk: MP: Template Meta-Programming in C++ - Kris Jusiak - CppCon 2022</li> <li>Back to Basics: The C++ Core Guidelines - Rainer Grimm - CppCon 2022</li> <li>Lightning Talk: Best Practices Every C++ Programmer Needs to Follow - Oz Syed - CppCon 2022</li> <li>import CMake, CMake and C++20 Modules - Bill Hoffman - CppCon 2022</li> <li>C++ Lambda Idioms - Timur Doumler - CppCon 2022</li> <li>Back to Basics: Templates in C++ - Nicolai Josuttis - CppCon 2022</li> <li>Principia Mathematica - The Foundations of Arithmetic in C++ - Lisa Lippincott - CppCon 2022</li> <li>A Faster Serialization Library Based on Compile-time Reflection and C++ 20 - Yu Qi - CppCon 2022</li> <li>Exceptionally Bad: The Misuse of Exceptions in C++ &amp; How to Do Better - Peter Muldoon - CppCon 2023</li> <li>Fast, High-Quality Pseudo-Random Numbers for Non-Cryptographers in C++ - Roth Michaels - CppCon 2022</li> </ul>"},{"location":"more-links/#compilers","title":"Compilers","text":"<ul> <li>CppCon 2017: Matt Godbolt \u201cWhat Has My Compiler Done for Me Lately? Unbolting the Compiler's Lid\u201d</li> <li>A Complete Guide to LLVM for Programming Language Creators</li> <li>Common Misconceptions about Compilers</li> </ul>"},{"location":"more-links/#code-modernization","title":"Code modernization","text":"<ul> <li>Improving performance with SIMD intrinsics in three use cases</li> <li>LLVM Optimization Remarks - Ofek Shilon - CppCon 2022</li> <li>The Most Important Optimizations to Apply in Your C++ Programs - Jan Bielak - CppCon 2022`</li> <li>CppCon 2016: Jason Turner \u201cPractical Performance Practices\"</li> <li>CppCon 2018: Jonathan Boccara \u201c105 STL Algorithms in Less Than an Hour\u201d</li> <li>Introduction to OpenMP - Tim Mattson (Intel) Youtube channel</li> </ul>"},{"location":"more-links/#other-performance-oriented-programming-languages","title":"Other performance-oriented programming languages","text":"<ul> <li> <p>Julia can be considered between low-level and high-level, as it combines the performance of low-level languages with the  scripting capabilities of Python.</p> </li> <li> <p>mojo: The expressiveness of Python, with the performance of C</p> </li> <li>Python vs Mojo ArjanCodes</li> </ul>"},{"location":"more-links/#os-stuff","title":"OS stuff","text":""},{"location":"more-links/#windows","title":"Windows","text":"<ul> <li>Using WSL to Build a Python Development Environment on Windows This is promising: maybe we finally have a an environment on Windows with a minimal difference from Linux an MacOSX.</li> </ul>"},{"location":"more-links/#linux-macos","title":"Linux, Macos","text":"<ul> <li>2020: The Year of the Linux Desktop - Moving from Macbook to Linux</li> <li>How to Set Environment Variables in Linux and Mac: The Missing Manual</li> <li>a file browser for your terminal</li> </ul>"},{"location":"more-links/#physics-simulation","title":"Physics simulation","text":"<ul> <li>Simulate Elastic Objects in Any Representation with NVIDIA Kaolin Library</li> <li> <p>Genesis is a physics platform designed for general purpose Robotics/Embodied AI/Physical AI applications. It is simultaneously multiple things:</p> </li> <li> <p>A universal physics engine re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.</p> </li> <li>A lightweight, ultra-fast, pythonic, and user-friendly robotics simulation platform.</li> <li>A powerful and fast photo-realistic rendering system.</li> <li>A generative data engine that transforms user-prompted natural language description into various modalities of data.</li> </ul>"},{"location":"more-links/#accelerators","title":"Accelerators","text":""},{"location":"more-links/#nvidia-gpus","title":"Nvidia GPUs","text":"<ul> <li>How GPU Computing Works</li> <li>How CUDA programming works</li> <li>How to write a CUDA program</li> </ul>"},{"location":"more-links/#amd-gpus","title":"AMD GPUs","text":""},{"location":"over-de-auteur/","title":"Over de auteur","text":""},{"location":"over-de-auteur/#engelbert-tijskens","title":"[Engel]bert Tijskens","text":"<ul> <li>Lic. Aard- en delfstofkunde, Master in Physics of Microelectronics and Material Sciences, Doctor in de Natuurwetenschappen - ik heb dus eigenlijk geen academische opleiding genoten in programmeren of wetenschappelijk rekenen, maar wel veel opleidingen bijgewoond, en vooral veel gelezen tijdens mijn voortdurende zoektocht naar betere manieren om de opdrachten waar ik voor stond uit te voeren.  </li> <li>ik werk sinds 2012 voor CalcUA, de UA kernfaciliteit voor supercomputing, en voor het VSC, het Vlaams Supercomputer Centrum. Ik verzorg er opleiding en ondersteuning van onderzoekers rond wetenschappelijk programmeren voor HPC-omgevingen en performantie-analyse. k ben gepassioneerd door Python, C++, Fortran en libraries en frameworks waarmee hoog-performante en parallelle applicaties kunnen gebouwd worden.</li> <li>Sinds 2014 geef ik het vak \"Parallel programmeren\". Ik geef graag les en wil mijn ervaring van 30 jaar wetenschappelijk programmeren delen met jonge onderzoekers.</li> <li>Voor 2012 leidde ik de DEM Research Group aan de KU Leuven. DEM staat voor Discrete Element Modelling. Je kan het vergelijken met Molecular Dynamics, maar dan in de macroscopische wereld met atomen die een vorm hebben, korrels dus, of grains in het Engels. Daarom wordt het ook Granular Dynamics genoemd. Korrelstromen komen in heel wat industri\u00eble processen voor en het modelleren ervan is interessant, om inzicht te verwerven in korrelige processen en om er goede procesinstallaties voor te ontwerpen. Dit is erg uitdagend omdat de fysica van korrelige processen zo complex is. In tegenstelling tot MD zijn interacties tussen korrels dissipatief en worden de contactkrachten bepaald door materiaaleigenschappen en oppervlakte-eigenschappen en zijn er zowel normale als tangenti\u00eble contactkrachten (wrijving). Bovendien zijn de korrels - afhankelijk van het materiaal - soms vervormbaar, of zelfs breekbaar. Omdat korrelige processen vaak over heel veel deeltjes gaan, zijn performantie en parallellisatie essentieel. De simulatiesoftware waar we toen aan werkten, wordt nu gecommercialiseerd door Mpacts. </li> </ul>"},{"location":"over-de-auteur/#enkele-voorbeelden","title":"Enkele voorbeelden","text":"<p>Kijk op Mpacts case studies voor meer voorbeelden. </p>"},{"location":"over-de-auteur/#cnh-maaidorser-ontwikkeld-met-mpacts","title":"CNH maaidorser - ontwikkeld met Mpacts","text":"<p>Hier zijn verscheidene korrelige processen aan de orde:</p> <ul> <li>de strohalmen maaien en binnen trekken in de machine,</li> <li>de graantjes losmaken van de aar en ze scheiden van het stro,</li> <li>stro afvoeren naar achter,</li> <li>het kaf van het koren scheiden (kaf weg blazen),</li> <li>het graan transporteren naar de verzamelbak bovenaan de machine,</li> <li>het graan transporteren van de verzamelbak naar de aanhangwagen achter de tractor,</li> </ul>"},{"location":"over-de-auteur/#spherische-korrels-die-op-een-trampoline-vallen","title":"Spherische korrels die op een trampoline vallen","text":""},{"location":"over-de-auteur/#mpacts-ijsbreker","title":"Mpacts: ijsbreker","text":"<p>Mpacts simulatie van een ijsbreker die door een ijslaag breekt.</p> <p></p>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#overview","title":"Overview","text":""},{"location":"overview/#goals","title":"Goals","text":"<ul> <li>What is parallel programming?</li> <li>Why parallel programming?</li> <li>Performance is important</li> <li>How to parallelize a program?</li> <li>Tools</li> <li>Principles and best practices</li> <li>Strategy for scientific software development</li> </ul>"},{"location":"overview/#background-knowledge","title":"Background knowledge","text":"<ul> <li>The working of a modern processor</li> <li>CPU architecture and hierarchical memory architecture<ul> <li>A short introduction: Memory location matters for performance</li> <li>A very good talk about this topic, you need to see this: Scott Meyers on Cpu Caches and Why You Care </li> </ul> </li> <li>Accelerators (GPU), increasingly important topic, but we cannot treat everything in this course. </li> <li>The architecture of a supercomputer</li> <li>Nodes and cores, NUMA domains</li> <li>Interconnect</li> </ul>"},{"location":"programming-quotes/","title":"QUOTES","text":""},{"location":"programming-quotes/#quotes","title":"QUOTES","text":"<p>\"The code you write makes you a programmer. The code you delete makes you a good one. The code you don't have to write makes you a great one.\" - Mario Fusco</p> <p>\u201cIt's hard enough to find an error in your code when you're looking for it; it's even harder when you've assumed your code is error-free.\u201d - Steve McConnell</p> <p>\"the most dangerous thought you can have as a creative person, is that you know what you are doing.\" Bret Victor - The Future of Programming.</p> <p>\"Every great developer you know got there by solving problems they were unqualified to solve until they actually did it.\" - Patrick McKenzie</p> <p>It\u00b4s better to wait for a productive programmer to become available than it is to wait for the first available programmer to become productive. - Steve McConnell</p> <p>Debugging is twice as hard as writing the code in the first place.Therfore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it. - Rajanand</p> <p>Always code as if the guy who ends up maintaning your code will be a violent psychopath who knows where you live. - Rick Osborne</p> <p>The trouble with programmers is that you can never tell what a programmer is doing until it's too late. - Seymour Cray</p> <p>First, solve the problem. Then write the code. - John Johnson</p> <p>Code is like humor. When you have to explain it, it\u2019s bad. \u2013 Cory House</p> <p>Simplicity is the soul of efficiency. \u2013 Austin Freeman</p> <p>Most of you are familiar with the virtues of a programmer. There are three, of course: laziness, impatience, and hubris.</p> <p>C makes it easy to shoot yourself in the foot; C++ makes it harder, but when you do, it blows your whole leg off.</p> <p>A system administrator has two problems: 1. Dumb users. 2. Smart users.</p> <p>\"Never trust a programmer in a suit.\" - Anonymous</p> <p>\"Fools ignore complexity. Pragmatists suffer it. Some can avoid it. Geniuses remove it.\" - Alan J. Perlis</p> <p>\"Theory and practice sometimes clash. And when that happens, theory loses. Every single time.\" - Linus Torvalds</p> <p>\"Code never lies, comments sometimes do.\" - Ron Jeffries</p>"},{"location":"vsc-infrastructure/","title":"VSC infrastructure","text":""},{"location":"vsc-infrastructure/#vsc-infrastructure","title":"VSC infrastructure","text":"<p>This document describes how to set up your environment for the project work of the 2000wetppr course on the Tier-2 cluster of the University of Antwerp, which is also a VSC cluster. With minor modifications this can be applied to all VSC clusters. In general, the text is also applicable to other HPC clusters, but the modifictions needed may be a bit more substantial.</p> <p>Note for students</p> <p>These topics are logically ordered. Make sure that you carry out all the tasks in the order described.</p> <p>In this course we often use the terms local and remote. Local refers to the physical machine you are working on, i.e. your desktop or laptop. Remote, on the other hand refers to a machine which is, typically, at some other place, and which you are accessing through your local machine and a network connection with the remote machine.</p>"},{"location":"vsc-infrastructure/#preparing-for-the-vsc-infrastructure-in-2000wetppr","title":"Preparing for the VSC infrastructure in 2000WETPPR","text":""},{"location":"vsc-infrastructure/#applying-for-a-guest-account","title":"Applying for a guest account","text":"<p>Note</p> <p>This section is only for students of the course 2000wetppr.</p> <p>Students of the course 2000wetppr must apply for a guest account to access the university's HPC clusters unless they already have a VSC account. The project work (see Evaluation) requires access to one of the university's HPC clusters.</p> <p>To apply for a guest account, create a SSH public/private key pair (see below) and send it by e-mail to franky.backeljauw@uantwerpen.be with engelbert.tijskens@uantwerpen.be in cc. A guest account will subsequently be created for you.</p>"},{"location":"vsc-infrastructure/#applying-for-a-vsc-account","title":"Applying for a VSC account","text":"<p>Note</p> <p>This section is only for researchers of Flemish institutes.</p> <p>Researchers of Flemish research institutes can apply for a VSC account to get access to the VSC Tier-2 and Tier-1 supercomputers. See Getting access to VSC clusters. An ssh public/private key pair is also required.</p>"},{"location":"vsc-infrastructure/#creating-an-ssh-publicprivate-key-pair","title":"Creating an ssh public/private key pair","text":"<p>An ssh public/private key pair in necessary for both a guest account (students) and a VSC account (researchers).</p> <p>A ssh public/private key pair is a way for secure access to a system through the Secure Shell protocol. They are basically two small files with matching numbers. You may think of the public key as a lock. Everyone may see the lock but no one can open the lock without its key, which is the private part of the key pair. The public key (the lock) will be placed on a system you need access to, in this case the Tier-2 supercomputer of our university (currently, that is Vaughan). To access to the supercomputer (i.e., to open the lock) from, say, your laptop, you need the private key to be stored on your laptop (or a USB memory stick) and pass it to the SSH protocol, which will verify that the private key and the public key match. If case they do, the SSH protocol will open the lock and grant you access to the machine.</p> <p>To create a ssh public/private key pair proceed as follows. Open a 'terminal':</p> <p>On Windows</p> <p>The latest builds of Windows 10 and Windows 11 include a built-in SSH server and client that are based on OpenSSH. You can use the <code>cmd</code> prompt, powershell,or WSL (Windows subsystem for Linux) as a terminal. For older Windows versions, we recommend installing mobaxterm to generate a ssh public/private key pair.</p> <p>On Linux</p> <p>Most Linux distributions have a <code>terminal</code> application.</p> <p>MacOSX</p> <p>MacOSX comes with a build in <code>Terminal.app</code>. <code>iTerm2</code> is a replacement for <code>Terminal.app</code> with many interesting extra features.</p> <p>Type the following command at the prompt:</p> <pre><code>&gt; ssh-keygen -t rsa -b 4096\n</code></pre> <p>You will then be prompted for a file location of the public and private key. You may accept the default location by entering. The default file location will look a bit different, depending on your OS. If the files already exist you can choose to overwrite them or to cancel the operation. You might want to change the filename of the key to a more meaningful name, e.g. <code>access_vaughan_rsa</code>. Don't use blanks in the filename. Use hyphens (<code>-</code>) or underscores (<code>_</code>) instead.</p> <pre><code>Enter file in which to save the key (C:\\Users\\your_username/.ssh/id rsa) :\nC:\\Users\\your_username/.ssh/id rsa already exists.\nOverwrite (y/n)? y\n</code></pre> <p>You will then be prompted for a passphrase (twice). A passphrase provides an extra level of protection in case somebody would steal your private key. Press <code>enter</code> for an empty passphrase. (Passphrases are a little annoying when using VSCode(see below) for remote development.)</p> <pre><code>Enter passphrase (empty for no passphrase):\nEnter same passphrase again:\n</code></pre> <p>Finally, you will be notified of where the keys are stored:  </p> <pre><code>Your identification has been saved in C:\\Users\\your_username/.ssh/id rsa.\nYour public key has been saved in C:\\Users\\your_username/.ssh/id rsa.pub.\n</code></pre> <p>For students of 2000wetppr</p> <p>To obtain a guest account, students must send their public key (and only the public key, the private key is, well, um, private) to <code>franky.backeljauw@uantwerpen.be</code> with <code>engelbert.tijskens@uantwerpen.be</code> in cc. The public key is the one with the <code>.pub</code> extension.</p>"},{"location":"vsc-infrastructure/#accessing-vaughan","title":"Accessing Vaughan","text":""},{"location":"vsc-infrastructure/#terminal-based-access","title":"Terminal based access","text":"<p>Vaughan is (at the time of writing) the University of Antwerp's Tier-2 HPC cluster. For terminal based access you open a <code>terminal</code> (see above) and execute the command:</p> <pre><code>&gt; ssh -i path/to/my/private-ssh-key your-user-id@login1-vaughan.hpc.uantwerpen.be\nLast login: Mon Feb 27 12:40:32 2023 from 143.129.75.140\n--------------------------------------------------------------------\nWelcome to VAUGHAN !\n...\n</code></pre> <p>If the key is in subdirectory <code>.ssh</code> of you home directory, the <code>-i path/to/my/private-ssh-key</code> can be omitted.</p> <p>After the command is finished, you can use the terminal as if you were working on the login node. The current working directory will be a location in your file system on the cluster, rather than on your local machine.</p> <p>Vaughan has two login nodes. <code>login1-vaughan.hpc.uantwerpen.be</code> and <code>login2-vaughan.hpc.uantwerpen.be</code>. You can also use <code>login-vaughan.hpc.uantwerpen.be</code>. Then the system will choose the login node with the highest availability.</p> <p><code>Ssh</code> comes with a <code>.ssh/config</code> file that allows you to store the arguments of frequently used ssh commands. E.g.</p> <pre><code># file ~/.ssh/config\nHost vn1\n  HostName login1-vaughan.hpc.uantwerpen.be\n  User vsc20170\n  IdentityFile /full/path/to/my/private-ssh-key\n  IdentitiesOnly yes\n  ForwardX11 yes\n  ForwardX11Trusted yes\n  ServerAliveInterval 60\n</code></pre> <p>which allows to abbreviate the above <code>ssh</code> command as <code>ssh vn1</code>. The <code>config</code> file can contain several <code>Host</code> entries.</p> <p>Editing files in terminal based access is performed using terminal editors, e.g. <code>vim</code> or <code>nano</code>. Although <code>vim</code> is very powerful, not everyone is comfortable using it.</p>"},{"location":"vsc-infrastructure/#ide-based-access","title":"IDE based access","text":"<p>Many developers (including me) find code development using terminal based access rather cumbersome. IDEs (Integrated Development Environment) provide a more user-friendly GUI based experience. Visual Studio Code provides a very reasonable user experience for both local aand remote development, providing a project directory tree, an editor pane, syntax highlighting, a debugging pane, a terminal, ... It is very well suited for our project work. So, install Visual Studio Code on your local machine. (It is available for Windows, Linux, and MacOSX).</p> <p>Here are some useful VSCode extensions that you should install. Click the <code>Extensions</code> icon in the activity bar on the left. You can search the Marketplace for interesting extensions.</p> <p></p> <p>Necessary extensions</p> <ul> <li>Remote Development</li> </ul> <p>Highly recommended extensions</p> <ul> <li>Python extension for Visual Studio Code</li> <li>Python extension pack</li> </ul> <p>Recommended extensions for C++</p> <ul> <li>C/C++</li> <li>Better C++ syntax</li> <li>CMake</li> <li>CMake tools</li> </ul> <p>Recommended extensions for Fortran</p> <ul> <li>Modern Fortran</li> </ul> <p>There is a helpfull tutorial on Using VSCode for Remote Development, but before getting your hands dirty, please complete the steps below first.</p>"},{"location":"vsc-infrastructure/#setting-up-a-git-account-required-for-micc2-projects","title":"Setting up a git account (required for micc2 projects)","text":"<p>See signing up for a new GitHub account</p> <p>The code that you write must be regularly committed to a remote GitHub repository. This has many advantages:</p> <ul> <li>First, it serves as a backup. Every single commit can be retrieved at all times. So, you can't lose your code, even not the older versions.</li> <li>Everyone with access to the repository can access the code. If you keep the repository public, that means everyone with access to the internet. If you make it private, only the people you invite can access.</li> <li>It is important that you give me access. If you have problems, I can clone your repository and debug it to see what is going wrong,</li> <li>If you cooperate with another student on the project you can exchange updates easily. You can make use of git branches to avoid bothering other people with your code changes before they are correct.  </li> </ul> <p>The presentation of the project must be added to your GitHub repository before you present it. I will keep a copy of your project repo as a proof of your work.</p>"},{"location":"vsc-infrastructure/#setting-up-your-remote-environment","title":"Setting up your remote environment","text":""},{"location":"vsc-infrastructure/#avoiding-file-quota-exceeded-caused-by-vscode-remote-development","title":"Avoiding <code>file quota exceeded</code> caused by vscode remote development","text":"<p>Vscode remote installs some machinery on the remote machine you are using. As vscode can easily create a lot of files remotely, this can easily cause <code>file quota exceeded</code> on your <code>user</code> file system (home directory). This can be easily fixed. </p> <p>Before starting remote development on the cluster, execute these commands in a normal terminal:</p> <pre><code>&gt; cd $VSC_HOME\n&gt; mkdir $VSC_DATA/.vscode-server\n&gt; ln -s $VSC_DATA/.vscode-server\n</code></pre> <p>This ensures that effective location for storing the vscode machinery on the remote side is  on the <code>data</code> file system. The default location is the <code>user</code> file system, which has limited file quota. As vscode often creates many files, this may cause <code>file quota exceeded</code> on the <code>user</code> file system and vscode will consequently fail to connect. The above commands avoid this issue. If you happen to forget to execute these commands before opening vscode on the cluster, you can still fix the problem by moving the <code>.vscode-server</code> directory and soft-linking it with these commands:  </p> <pre><code>&gt; cd $VSC_HOME\n&gt; mv .vscode-server $VSC_DATA/\n&gt; ln -s $VSC_DATA/.vscode-server\n</code></pre> <p>Warning</p> <p>Do not move the <code>.vscode-server</code> directory into the <code>scratch</code> file system. </p>"},{"location":"vsc-infrastructure/#lmod-modules","title":"LMOD modules","text":"<p>A HPC cluster provides you with many installed software packages. However, none of them are immediately available. To make a package available, you must <code>load</code> the corresponding software module (this is a different module than the Python modules, also known as <code>LMOD</code> modules). Here is a list of <code>LMOD</code> modules you may need for the project work:</p> <ul> <li><code>Python</code>,the default python distribution (= Intel Python 3.8.3, at the time of writing), also provides [<code>numpy</code>]   (https://numpy.org), <code>f2py</code>, <code>scipy</code>,   <code>sympy</code>, <code>pandas</code>,   <code>mpi4py</code>, <code>h5py</code>,   <code>pytest</code> as well as the C/C++/Fortran   compilers with which the Python distribution was build.</li> <li><code>numba</code></li> </ul> <p>Tip</p> <p>To see the list of installed Python packages, load the LMOD module for the Python distribution of your choice, and execute <code>pip list -v</code>. This will show you also the location where the package is installed. Pre-installed packages, the ones that are made available by loading LMOD modules, will show up under <code>/apps/antwerpen</code>, while the packages you installed yourself with <code>pip install --user</code> will show up under <code>${PYTHONUSERBASE}</code>, c.q. <code>/scratch/antwerpen/201/vsc20170/.local</code>.  </p> <p>The following LMOD modules are needed by micc2 (see below):</p> <ul> <li><code>buildtools</code>,</li> <li><code>git</code>,</li> <li><code>gh</code>.</li> </ul> <p>Note</p> <p>Every time you start a remote terminal session, you must load these modules.</p> <p>This is conveniently done by writing down all the <code>load</code> commands in a file (and add the file to your <code>git</code> repository:</p> <pre><code># File wetppr-env.sh\n# Prepare environment for Python/C++/Fortran development\n# You must 'source' this file\nmodule load Python\nmodule load numba\nmodule load buildtools\nmodule load git\nmodule load gh\n# list all loaded modules\nmodule list\n\n# allow to install python packages locally\nexport PYTHONUSERBASE=/data/antwerpen/gst/guestXXX/.local # replace guestXXX with your guest ID\nmkdir -p ${PYTHONUSERBASE}\nexport PATH=\"$PATH:${PYTHONUSERBASE}/bin\"\n</code></pre> <p>Every time you start a new remote terminal session, you must execute the command:</p> <pre><code>&gt; source path/to/wetppr-env.sh\n</code></pre> <p>to load all modules and to modify the environment variables <code>PYTHONUSERBASE</code> and <code>PATH</code>.</p> <p>Warning</p> <p>Initially, it will appear useful to source the script automatically when you login. However, soon you will discover that such scripts depend on the project you work on, and that it is better to have it somewhere in your project directory.</p>"},{"location":"vsc-infrastructure/#micc2","title":"Micc2","text":"<p>Micc2 is a Python package that simplifies your project management considerably. If you haven't already done so, source the environment script:</p> <pre><code>&gt; source path/to/wetppr-env.sh\n</code></pre> <p>and install it in a (remote) terminal as:</p> <pre><code>&gt; pip install --user et-micc2\n...\n</code></pre> <p>Note</p> <p>The <code>--user</code> flag instructs <code>pip</code> to install the package in the directory defined by the environment variable <code>${PYTHONUSERBASE}</code>. The default install path of <code>pip</code> is a system location for which you do not have write permissions. Omitting <code>--user</code> would raise a <code>PermissionError</code>.</p> <p>Micc2 requires a little setup before it is fully functional.</p> <p>Warning</p> <p>You need a GitHub account before you can set up <code>micc2</code>.</p> <p>To set up <code>micc2</code> , enter</p> <pre><code>&gt; micc2 setup\n</code></pre> <p>and supply the data the application asks for.</p> <p>Warning</p> <p>Make sure that you get a personal access token (pat) to allow creating remote repositories at GitHub! Check this.</p>"},{"location":"vsc-infrastructure/#pybind11","title":"Pybind11","text":"<p>You will also need <code>nanobind</code> if you want to use <code>wip</code> for building binary extension modules for Python from C++.</p> <pre><code>&gt; pip install --user nanobind\n</code></pre>"},{"location":"vsc-infrastructure/#submitting-jobs-on-vaughan","title":"Submitting jobs on Vaughan","text":"<p>Most modern supercomputers - Vaughan included - use Slurm for resource management and scheduling. An extensive description about using Vaughan can be found here.</p> <p>Unlike your personal computer, which you use mainly interactively, a supercomputer is mostly used in batch mode. Rather executing a command to perform the computation that you want, you send a request to execute that command to the scheduler. In that request you must specify the resources you need, setup the environment, including necessary LMOD modules, and the command you want to execute.</p> <p>Here is a typical 'hello world' job script <code>mpi4py_hello_world.slurm</code> (You can find it in the  <code>wetppr/scripts/vaughan_examples</code> directory of the wetppr github repo):</p> <pre><code>#!/bin/bash                               # 1 Shebang\n#SBATCH --ntasks=64 --cpus-per-task=1     # 2 SLURM job script parameters\n#SBATCH --time=00:05:00                   # 2\n#SBATCH --mail-type=BEGIN,END,FAIL        # 2\n#SBATCH -\u2013mail-user=&lt;your e-mail address&gt; # 2\n#SBATCH --job-name mpi4py_hello_world     # 2\n#SBATCH -o %x.%j.stdout                   # 2\n#SBATCH -e %x.%j.stderr                   # 2\n\nmodule --force purge                      # 3 Setup execution environment\nmodule load calcua/2020a                  # 3\nmodule load Python                        # 3\nmodule list                               # 3\n\nsrun python mpi4py_hello_world.py         # 4 Job command(s)\n</code></pre> <p>The job is submitted for execution by executing this command in a terminal running a session on a login node:</p> <pre><code>&gt; cd path/to/wetppr/scripts/vaughan_examples\n&gt; sbatch mpi4py_hello_world.slurm\n</code></pre> <p>(Note that we first cd into the directory containing the job script.) If all goes well, <code>sbatch</code> responds with something like</p> <pre><code>Submitted batch job 709521\n</code></pre> <p>Where <code>709521</code> is the job id. </p> <p>The job is now in the job queue. You can check the status of all your submitted jobs with the <code>squeue</code> command:</p> <pre><code>&gt; squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            709521      zen2 mpi4py_h vsc20170 PD       0:00      1 (Priority)\n</code></pre> <p>The <code>ST</code> column shows the status of your job. <code>PD</code> means 'pending', the job is waiting for resource allocation. It will eventually run. Once running, it will show <code>R</code> as a status code and the <code>TIME</code> column will show the wall time of the job. Once completed the status will be 'CD' and after some minutes, it will disappear from the output of the <code>squeue</code> command. The directory <code>wetppr/scripts/vaughan_examples</code> now contains two extra files:</p> <pre><code>&gt; ls -l\ntotal 12\n-rw-rw-r-- 1 vsc20170 vsc20170   0 May  4 12:04 mpi4py_hello_world.709521.stderr\n-rw-rw-r-- 1 vsc20170 vsc20170 576 May  4 12:04 mpi4py_hello_world.709521.stdout\n-rw-rw-r-- 1 vsc20170 vsc20170 126 May  4 11:38 mpi4py_hello_world.py\n-rw-rw-r-- 1 vsc20170 vsc20170 346 May  4 12:01 mpi4py_hello_world.slurm\n...\n</code></pre> <p>File <code>mpi4py_hello_world.709521.stderr</code> contains the output written by the job to stderr. If there are no errors, it is generally empty, as indicated here by the 0 file size. File <code>mpi4py_hello_world.709521.stdout</code> contains the output written by the job to stdout. Here it is:</p> <pre><code>Currently Loaded Modules:\n  1) calcua/2020a\n  2) GCCcore/9.3.0\n  3) binutils/2.34-GCCcore-9.3.0\n  4) intel/2020a\n  5) baselibs/2020a-GCCcore-9.3.0\n  6) Tcl/8.6.10-intel-2020a\n  7) X11/2020a-GCCcore-9.3.0\n  8) Tk/8.6.10-intel-2020a\n  9) SQLite/3.31.1-intel-2020a\n 10) HDF5/1.10.6-intel-2020a-MPI\n 11) METIS/5.1.0-intel-2020a-i32-fp64\n 12) SuiteSparse/5.7.1-intel-2020a-METIS-5.1.0\n 13) Python/3.8.3-intel-2020a\nHello from rank=7/64\nHello from rank=5/64\n...\nHello from rank=25/64\n</code></pre> <p>The lines following <code>Currently Loaded Modules:</code> represent the output of the <code>module list</code> command. The subsequent lines <code>rank=&lt;rank&gt;/64</code> represent the output of the <code>print</code> statement in the <code>mpi4py_hello_world.py</code> script. Each rank produces one printed line in random order (this is because of OS jitter). </p> <p>The job script has four sections:</p> <ul> <li>the shebang</li> <li>SLURM job script parameters </li> <li>setup of the job's execution environment</li> <li>the actual job commands </li> </ul>"},{"location":"vsc-infrastructure/#shebang","title":"Shebang","text":"<p>The first line, starting with <code>#!</code> is the shebang). It is mandatory and sets the system <code>bash</code> as the interpreter of the job script:  </p> <pre><code>#!/bin/bash\n</code></pre>"},{"location":"vsc-infrastructure/#slurm-job-script-parameters","title":"SLURM job script parameters","text":"<p>After the shebang the Slurm job script parameters are specified. They all start with <code>#SBATCH</code>. The second line selects the number of tasks (processes) and cpus per process to be used for your job:</p> <pre><code>#SBATCH --ntasks=64 --cpus-per-task=1\n</code></pre> <p>Here, 64 processes are requested, each process using one cpu for each process. Since the compute nodes of Vaughan have 64 cores, this corresponds to requesting a single entire node. Typically, processes communicate via MPI and the cpus of a process via OpenMP.  So, each core runs a process communicating with the other processes via MPI. </p> <p>Furthermore, we need to specify how long we suspect the job to run:</p> <pre><code>#SBATCH --time=00:05:00\n</code></pre> <p>Here, we request 5 minutes. If the job is not finished after five minutes, it will be aborted. The maximum wall time for a job is 72 hours. Longer jobs must be split into pieces shorter than 72 hours. Jobs of 1 hour or less have high priority than longer jobs.</p> <p>Larger jobs typically do not start immediately. First, the requested resources must be available. Moreover, your job must be the next in the queue, which is formed on a fair share basis. To know when your job starts and ends, you can ask the scheduler to send you an e-mail. Here, we request that an e-mail is sent when the job starts, ends and also when it fails for some reason.</p> <pre><code>#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH -\u2013mail-user=&lt;your e-mail address&gt;\n</code></pre> <p>If you leave out the <code>--mail-user</code>, the e-mail address </p> <p>Finally, it is convenient to specify a name for the job and its output files. </p> <pre><code>#SBATCH --job-name mpi4py_hello_world\n#SBATCH -o %x.%j.stdout\n#SBATCH -e %x.%j.stderr\n</code></pre> <p>As the job script is executing the Python script <code>mpi4py_hello_world.py</code> and was therefor named <code>mpi4py_hello_world.slurm</code>, the job name is correspondingly set to <code>mpi4py_hello_world</code>. The output to <code>stdout</code> and <code>stderr</code> is then redirected to <code>mpi4py_hello_world.&lt;jobid&gt;.stdout</code> and <code>mpi4py_hello_world.&lt;jobid&gt;.stderr</code>, resp. This ensures that an alphabetical listing of files will group all files corresponding to and produced by this job script. </p>"},{"location":"vsc-infrastructure/#setup-of-the-jobs-execution-environment","title":"Setup of the job's execution environment","text":"<p>We prefer to start from a clean environment:</p> <pre><code># module --force purge\n</code></pre> <p>Then we load a toolchain:</p> <pre><code># module load calcua/2020a\n</code></pre> <p>(More recent toolchains are upcoming).</p> <p>Next, we load the LMOD modules we need:</p> <pre><code># module load Python/3.8.3-intel-2020a\n</code></pre> <p>This LMOD module comes with a bunch of pre-installed python modules, including numpy, mpi4py, scipy, ...</p> <p>It is also useful to print which modules we have loaded, to leave a trace of the environment in which the job is executed for later reference.:</p> <pre><code># module list\n</code></pre>"},{"location":"vsc-infrastructure/#job-execution-commands","title":"Job execution commands","text":"<p>The job script ends with a list of (<code>bash</code>) commands that compose the job: </p> <pre><code>srun python mpi4py_hello_world.py\n</code></pre> <p>The <code>srun</code> command calls <code>mpirun</code> with the resources requested in the Job script parameters. Consequentially, 64 MPI processes will be started, one on each core of a compute node. Their ranks will be numbered 0..63.</p>"},{"location":"vsc-infrastructure/#recommended-steps-from-here","title":"Recommended steps from here","text":"<ul> <li>A helpful tutorial on Using VSCode for Remote Development</li> <li>The wip documentation</li> </ul>"}]}